---
title: "MoE 论文阅读：Sparsely-Gated MoE"
date: 2021-10-27T11:15:57+08:00
draft: false
mathjax: true

excerpt_separator: <!--more-->
---
本文主要是一点 Mixture of Experts (MoE) 网络结构的介绍。<!--more-->

## 背景

深度学习的发展趋势就是，数据越来越多，模型也越来越大，这导致的直接后果就是对计算能力的要求也越来越高。针对这个问题，学术上尝试了一种条件计算的方法，就是根据输入样本自身的一些数据特性来决定启用一部分的网络进行计算，其它部分则不使用，这样的好处就是，当模型的容量变大时，计算量不会等比例的变大。

在条件计算中，怎样选择哪部分网络进行计算则需要进一步研究，目前的做法包括一些基于强化学习的、后向传播的，虽然理论上这些方法是有优势的，但是实际使用时对模型容量、训练时间、模型质量的提高都帮助不大。已有的算法一般都具有下面几个明显的缺点：

* GPU 对上述实现中分支选取这种条件语句性能差，至少比计算性能差很多
* 大的 Batch size 对模型的效果至关重要，但是条件计算中，本质上这些分支看到的有效 batch size 是变小了的。比如，一次关闭50%的网络分支，那么一次迭代，这些分支的有效 batch size 其实只有 bs / 2，因为迭代两次才会平均遇到一个完整的 bs，所以平均下来就只有 bs / 2 的 batch size 大小了
* 网络通信瓶颈。

## 结构

MoE 层由两部分构成：(1) n 个专家网络 $E_1, ... E_n$，每一个专家网络的参数都是相互独立的；(2) 门控网络 $G$，输出的是一个 n 维稀疏向量。用 $G(x)_i$ 和 $E_i(x)$ 分别表示门控网络的输出以及第 i 个专家网络的输出。则 MoE 的实现可以用下式进行表示：

$$y = \sum_i^{n=1} G(x)_i E_i(x)$$

通过限制 $G(x)$ 为稀疏向量来降低计算量，也就是不会将输入 $x$ 送入到 $G(x)_i=0$ 的那些专家网络。另一方面是，当 n 比较大的时候，选择专家网络的这个分支就非常庞大，这时候可以通过引入分层的 专家网络分支 结构进行优化，第一层分支可以选择 m 组，第二层每一组内有 n 个专家网络，所以就可以通过引入分层来讲专家网络的个数增加 n 倍。

图 - 1 展示了一个 MoE 的实现示例，示意图种设置的每次激活的分支个数是2。

![图 - 1 MoE 实现示例](/imgs/moe/moe0.png)

这里 $G(x)_i$ 其实也可以认为是一种 Attention，结合下文的 Gating 网络部分的内容，这个数值既作为 select 的策略，又作为 attention score 进行使用。

### Gating 网络

一种是非稀疏的 gating 网络，将输入 $x$ 乘上一个可优化的权重矩阵 $W_g$，然后送入 softmax 网络，则门控网络的计算公式如下：

$$G_{sigmoid}(x) = Softmax(x \cdot W_g)$$

另一种就是本文提出来的实现稀疏门控的MoE 网络，具体实现是在上一种非稀疏实现方式上的做的一个小的微调，即仅选取 Top k 个最大的 $G(x)_i$ 对应的专家网络进行计算。在这个过程中，为了保证所有 专家网络之间的负载均衡，又额外加入了一个可训练的噪声信号，噪声信号也会通过一个可学习的权重矩阵来调整每个专家网络对应的数值：

$$G(x) = Softmax(KeepTopK(H(x), k))$$

$$H(x)_i = (x \cdot W_g)_i + StandardNormal() \cdot Softplus((x \cdot W _{noise})_i)$$

其中，$KeepTopK(v, k)_i$ 当 $v_i$ 在 $v$ 的所有元素中位于最大 k 内，则取 $v_i$，剩下的则取 $-\infty$，这样经过 Softmax 计算之后才会变为0。

### Shrinking Batch Size 问题

即有效的 batch size 变小问题，比如 当前分支的激活概率是 $p$，则当前分支有效的 batchsize 其实只有正常 batch size 的 p 大小，即 $p * bs$，然而我们也知道 bs 越大，模型效果才越好。针对这个问题，作者提出了以下种方法：

1. 混合 DP & MP 的实现
    简单来说，就是将模型的 MoE 部分以 MP 的方式实现，这样不同分组的 Experts 网络可以分配到不同的 GPU 上，组的大小最小是1，最大可以调整；然后模型的其它结果是 DP 的实现，即不同 gpu 上是同一套网络参数。
2. 使用accumulation的做法
    这种做法是将训练的前几次迭代计算的梯度进行累加求平均，然后更新模型参数。
3. 使用基于 Recurrent MoE 的网络来训练，增大 batch size
   这种做法是将 LSTM / GRU 等都换成 MoE 的结构，这样就降低了整个模型的参数量（同时计算的参数量），就可以增加 batch size了，但是这种做法与第2种做法会冲突。

### 负载均衡

MoE 网络中，一个非常重要的点是要注意实现负载均衡，即需要保证所有的专家网络遇到的 样本数量（计算量）大致相同。前面提到的 Noisy Top-K Gating 是一种方法，但主要还是需要模型的 Loss 来约束模型自己主动进行负载均衡。论文里提出了两个新的 Loss，一个是为了保证专家网络之间的重要性没有大的差别，如果没有这个约束，则训练过程中就很容易出现这种情况：个别专家网络被使用非常频繁，然后就越来越频繁，其它专家网络基本没有学习到啥有用的参数，这跟增加模型容量的目的也是不相容的。

做着定义了一个专家网络重要度的度量，并基于这个度量来实现重要度的均衡。重要度指标定义如下：

$$Importance(X) = \sum_{x \in X} G(x)$$

然后基于这个指标定义的 Loss 为：

$$L_{importance}(X) = w_{importance} \cdot CV(Importance(X))^2$$

首先是重要度指标的定义，也就是所有样本在当前专家网络分支上概率的和（注意这里是所有的样本），$G(x)$ 可以认为是该专家网络开启的概率；然后是 Loss 的定义，CV 表示方差，所以 Loss 的效果就是当所有专家网络的重要性一样的时候才会变为0。

另一个 Loss 是直接针对专家网络之间的负载均衡的，也是先定义了一个专家网络的闲忙度量，然后基于这个度量定义了一个新的 Loss。

闲忙度量定义是那些实际经过当前专家网络分支的样本的概率和：

$$P(x, i) = Pr((x \cdot W_g) _i + StandardNormal() \cdot Softplus((x \cdot W _{noise})_i) > kth excluding(H(x), k, i))$$

其中 $kth excluding(H(x), k, i)$ 表示的是v里不包含第 $i$ 的元素剩下的最大k个元素。上式经过调整变量之后，就可以调整为一个 Normal 分布的 CDF 函数，关于 CDF 函数的定义见 wiki。

$$P(x, i) = \Phi (\frac{x \cdot W_g) _i - kth excluding(H(x), k, i))}{Softplus((x \cdot W _{noise})_i)})$$

然后定义每个专家网络对应的闲忙指标：

$$Load(X)_i = \sum _{x\in X}P(x, i)$$

最后基于这个指标基于 CV 函数定义了负载均衡 Loss:

$$L_{load}(X) = w_{load} \cdot CV(Load(X))^2$$

上面提到的 $w_{load}$ 与 $w_{importance}$ 都是超参，用于控制该 Loss 在总的 Loss 中的贡献大小。

另，M6 论文里提出了另一种负载均衡的实现，大致思路与上面是一样的，可以参考以下。

### 通信

结合上面 MP & DP 混合实现方式，通信是一个大问题，因为所有 GPU 上的输出到需要汇聚到当前 GPU 的专家网络的输入中，所以通信量非常之大。直接的做法是采用压缩、fp16等方式进行优化。

## 结果

这一个还是直接看论文吧，就不贴图了。

