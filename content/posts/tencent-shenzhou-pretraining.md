---
title: "Tencent Shenzhou Pretraining"
date: 2021-10-12T10:56:28+08:00
draft: true
---

记录一下新闻稿里QQ浏览器用到的一些技术，原稿在：[百亿参数、中文NLU能力首次超越人类，QQ浏览器大模型「神舟」登顶CLUE](https://mp.weixin.qq.com/s/PODShmOo0tg9cmchNhzvtw)

「神舟」自然语言预训练模型是由腾讯 QQ 浏览器实验室于 2021 年自研的成果。通过联合腾讯 QQ 浏览器搜索和内容算法团队，在 6 月登顶 CLUE 的摩天预训练模型基础上进一步进行了大量创新：引入跨层衰减的 Attention 残差链接算法、并将 instance-wise 的自蒸馏技术引入到预训练模型的训练中，以及自回归的 MLM 训练策略等。同时，在此基础上通过二次预训练的方式进行知识增强，进一步提高预训练模型效果。

大规模深度学习模型的效果在各方面获得了成功，但是训练一个百亿的双向自编码模型一直是一个挑战。「神舟」模型通过 ZeRO 分割方案，将百亿模型分割到 N 张卡上，并结合 FP16 训练、梯度检查进一步降低显存使用。底层通信将 TCP 改为 GPUDirect RDMA 通信，大大提高了通信效率，并进一步通过梯度聚集算法减少通信量。

结合结合腾讯 pcg venus 机器学习平台引入大量模型优化和加速算法，「神舟」在之前十亿级别参数量的「小」模型摩天（Motian）基础上，构建了百亿参数的训练能力，结合算法的大量优化，最终得到了这个在中文语言理解表现上业界表现最佳的模型。

* 预训练数据清洗：「神舟」借鉴了摩天模型的大量基础训练数据，包括企鹅号、小说、各类百科、新闻、社区问答等内容。并在此基础上额外引入了大量互联网网页数据，经过精准清洗优化，在数据量保障的前提下同时避免低质数据导致的模型漂移。
* ALBEF / R-Drop 中的算法实现自蒸馏，实验表明 Instance-wise 自蒸馏表现更优，但是更占用显存
* 引入知识，包括基于搜索构建的知识图谱数据、百科语料，尝试了三种知识性任务：远监督分类关系、同类实体替换预测、三元组-文本Mask预测；实验表明三种方法都可以提高效果。
* 优化避免参数遗忘。通过知识性任务能够驱动预训练模型学习到相关的参数，但也很容易造成原参数遗忘和模型通用能力的下降。常见的解决参数遗忘的做法是针对输入的知识性语料，训练 MLM 任务和知识性任务。这种做法虽然减缓了参数遗忘，但由于知识性语料较为单一和规整，引入 MLM 也无法避免模型在通用场景中效果变差。针对这个问题，团队引入了双路语料输入的机制，将通用预训练语料和知识性任务语料组合为双路输入，共享模型 Encoder 参数，进行联合训练。这样做既保证了 MLM 任务的语料输入的多样性，又减少了知识性任务都是较规整的百科语料对模型的影响。实验结果表明，采用双路输入比只在百科语料中做联合学习在多个下游任务中平均有超过 0.5% 的提升。在引入双路输入后，上面提到的 3 个知识性任务均能给预训练模型在下游任务中带来提升。其中，远监督关系分类、三元组 - 文本 Mask 预测任务能在阅读理解类任务的 EM 指标上上平均提升 0.7%；在自然语言推理类任务上，则有 0.15% 到 0.3% 不等的提升。
* 


