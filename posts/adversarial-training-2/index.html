<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Adversarial Training 2 &middot; Triloon</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		<link rel="stylesheet" href="/css/custom.css">
		
		<link rel="icon" href="/favicon.ico"/>
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="" rel="alternate" type="application/rss+xml" title="Triloon" />

		<script src="/js/darkmode.js"></script>
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h2 class="nav-title">Triloon</h2>
					
				</a>
				<ul>
    
    
        <li>
            <a href="/about/about">
                
                <span>About</span>
                
            </a>
        </li>
    
        <li>
            <a href="/posts/">
                
                <span>Posts</span>
                
            </a>
        </li>
    
</ul>
			</div>
		</nav>

        <div id="darkModeToggle" onclick="toggleDarkMode()">
  &#9680; 
</div>

        

<main>
	


        <div class="post">
		<div class="post-info">
    <span>Written by</span>
        triloon
        <br>
        <span>on&nbsp;</span><time datetime="2021-10-13 14:30:10 &#43;0800 CST">October 13, 2021</time>
</div>

		<h1 class="post-title">Adversarial Training 2</h1>
<div class="post-line"></div>

		

		<p>对PGD算法的改进，包括FreeAT, FreeLB, SMART等。</p>
<p>博客<a href="/content/posts/adversarial-training.md">Adversarial Trainig</a>里面的PGD模型在对抗样本训练中可以起到很好的效果，但是缺点在于求解最优扰动时需要进行迭代，比如迭代K次，则意味着训练耗时增加K + 1倍。本文提到的几个算法主要就是针对这个问题进行优化，包括 FreeAT, FreeLB, SMART 等算法。</p>
<h2 id="freeat">FreeAT</h2>
<p>首先在PGD算法中，为了得到$\max_{\delta \in \mathcal{S}} L(\theta, x + \delta, y)$，需要对 adversarial samples 进行迭代生成，$x^{t+1} = \prod_{x + \mathcal{S}} (x^t + \alpha \mathrm{sign} \nabla_x L(\theta, x, y))$。FreeAT主要是为了降低后面迭代求解最优Adversarial Samples的过程。</p>
<p>对于一个 PGD-K 算法而言，每一个batch的训练中，需要计算K次前向-后向，每次前向-后向会更新一下最新的扰动数值，这个扰动数值也会累加，然后使用第K次的adversarial examples计算$L_{adv}$以及 第 K + 1 次前向计算 $L$，最终反向传播一次$L + L_{adv}$。</p>
<p>而对于FreeAT算法，在K次前向-后向计算计算最优扰动项的时候，也会使用$L$进行一次前向-后向传播，并且总的 Epoch 数降低为$ N / K$，N 为正常训练时需要的 epoch 个数。与PGD-K的区别是，每次计算扰动项的时候，模型的参数也会更新，同时降低总的训练epoch数，保证总的iteration数不变。在训练下一个 Batch 的时候，上一个Batch最新的扰动数值作为当前Batch的初始扰动数值。</p>
<p>为了避免参数遗忘，这里的 $FreeAT-K$ 中的K不能太大。</p>
<p>最终，FreeAT 的算法如下。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freeat0.png" alt="图 - 1 FreeAT算法伪代码">
    <figcaption>图 - 1 FreeAT算法伪代码</figcaption>
    </center>
</figure></p>
<p>模型的超参数$m$（也就是上文的$K$）是重要的超参数，图-2展示了在 CIFAR-100数据集上的影响。可见随着 $m$ 的增加，精度会下降，但当$m&lt;10$的时候对精度影响都比较小，而且别PGD-7的效果都好，可以作为备选范围。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freeat1.png" alt="图 - 2 FreeAT算法中m的影响">
    <figcaption>图 - 2 FreeAT算法中m的影响</figcaption>
    </center>
</figure></p>
<p>在ResNet-50 + ImageNet 的配置下，当$m=4$的时候效果最好，这一切都是不像PGD那样增加训练计算量的前提下实现的。</p>
<h3 id="gradient-masking">Gradient Masking</h3>
<p>Gradient Masking 的意思是指模型的输出对输入的梯度趋近于零，所以当输入发生微小变化时，不会影响模型的输出，从而实现鲁棒性。但是这种方法并没有真正的提高模型的鲁棒性，因为考虑到对抗样本的 transferability，换成另一个模型，这个模型对输入的梯度不接近于零，导致用这个模型生成的对抗样本导致目前正在训练的模型还是会预测错误。如下图所示，</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/gradientmasking0.png" alt="图 - 3 Gradient Masking导致的后果">
    <figcaption>图 - 3 Gradient Masking导致的后果</figcaption>
    </center>
</figure></p>
<p>其中，(a) 中的模型对与输入 $x$ 附近的梯度已经为0了，所以此时对 $x$ 的扰动有一定的鲁棒性，但是当使用另一个模型 (b) 对 $x$ 的梯度来生成对抗样本 $x^*$ 时，基于transferability，这个对抗样本对模型 (a) 仍然是有效的。所以当发生 Gradient Masking 时，模型并非是真正的鲁棒。</p>
<p>Label smoothing 一定程度上也可以提高对抗样本效果，因为知识蒸馏之类的Loss可以让学习到的模型更平滑，也就是对输入更不敏感。</p>
<p>更多的可以参考：<a href="https://arxiv.org/pdf/1611.03814.pdf">SoK: Towards the Science of Security and Privacy in Machine Learning</a></p>
<h2 id="freelb">FreeLB</h2>
<p>首先，FreeAT那种在每次更新扰动项$\delta$的时候都会更新模型的权重（梯度下降），这会导致<code>stale gradient</code>的发生，也就是对于第 $t$ 步，扰动的更新不是最大化模型在 $t$ 时刻的参数$\theta_t$，而是基于下式$\nabla_{\delta} L(f_{\theta_{t-1}}(x + \delta_{t-1}), y)$计算得来的(这个梯度计算公式还是以PGD那里的为准)。</p>
<p>FreeLB算法其实对计算量并没有减少，主要是提出了另一种梯度更新过程。在PGD-K的K次前向-后向计算用于构造adversarial examples时，FreeLB会累加每次后向传播中模型参数的，最后使用这个累加的梯度更新模型的参数。总的来说，前向-后向次数由K + 1次变为 K 次。另一个好处是，这个累加的梯度可以包含更多扰动的信息，可以认为每次模型的梯度更新都使用了更大的Batch的样本计算得到，即$x+\delta_0, \ldots, x + \delta_{K-1}$，而PGD算法只能最小化$x+\delta_{k-1}$位置的扰动损失，理论认为这会比PGD得到更好的泛化性能。</p>
<p>上述改动等价于在两个高维球里求解最优的扰动：</p>
<p>$$\mathcal{I}_ t=\mathcal{B}<em>{x+\delta_0}(\alpha t) \cap \mathcal{B}</em>{x}(\epsilon)$$</p>
<p>其中$\mathcal{B}_x(\epsilon)$表示半径为$\epsilon$的球。而通过梯度累加移动平均，则等价于优化下面的损失函数。</p>
<p>$$\min_\theta \mathbb{E}<em>{(z, y) \sim \mathcal{D}} \left[  \frac{1}{K} \sum</em>{t=0}^{K-1} \max_{\delta_t \in \mathcal{I}<em>t} L(f</em>\theta(x + \delta_t), y) \right]$$</p>
<p>使用FreeLB算法需要特别注意的地方在于，在包含Dropout的模型中，需要保证 K 次前向-后向计算时 Dropout 的Mask保持一致，否则的话，得到的扰动就不是针对某一模型的最优扰动了。所以，使用时需要保证在一个Step内，Dropout用到的 Mask 保持不变。</p>
<p>最终，对应的FreeLB算法伪代码如下。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freelb0.png" alt="图 - 4 FreeLB算法伪代码">
    <figcaption>图 - 4 FreeLB算法伪代码</figcaption>
    </center>
</figure></p>
<p>需要说明的是，论文里提到的 PGD 与原文中的公式定义不太一致。</p>
<p>$$\delta_{t+1} = \prod_{\parallel \delta \parallel_F \le \epsilon} (\delta_t + \alpha g(\delta_t) / \parallel g(\delta_t) \parallel_F)$$</p>
<p>其中，$g(\alpha_t) = \nabla_{\delta} L(f_\theta(x + \delta_t), y)$，这里定义的是对扰动的梯度，而不是对输入$x$的梯度。这一点FreeLB对应的代码里是对应论文里的公式的，需要找PGD的官方实现进行验证。</p>
<h2 id="smart">SMART</h2>
<p><a href="https://arxiv.org/pdf/1911.03437.pdf">SMoothness-inducing Adversarial Regularization</a></p>
<p>SMART论文主要提出了两个创新点。</p>
<ul>
<li>
<p>Smoothness-inducing adversarial regularization</p>
<p>这个正则项主要是为了提高模型的鲁棒性。</p>
</li>
<li>
<p>Bregman proximal point optimization</p>
<p>这里是为了提高模型的效果，类似<code>Mean Teacher</code>。</p>
</li>
</ul>
<h3 id="smoothness-inducing-adversarial-regularization">Smoothness-inducing adversarial regularization</h3>
<p>提出优化下面的损失函数，</p>
<p>$$\min_\theta \mathcal{F}(\theta) = \mathcal{L}(\theta) + \lambda_s \mathcal{R}_s (\theta)$$</p>
<p>其中，$\mathcal{L}(\theta)$为正常损失函数，是基于数据对$(x_i, y_i)$的有监督损失函数。</p>
<p>$$\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n \ell(f(x_i; \theta), y_i)$$</p>
<p>$\mathcal{R}_s(\theta)$为smoothness-inducing adversarial regularizer项：</p>
<p>$$\mathcal{R}_ s(\theta) = \frac{1}{n} \sum_{i=1}^n \max_{\parallel \tilde{x}_i - x_i \parallel_p \le \epsilon} \ell_s(f(\tilde{x}_i; \theta), f(x_i; \theta))$$</p>
<p>最新的SMART论文里，基于TRADES论文中的损失函数定义$\mathcal{R}_s$，适用于模型输出端是概率分布的情况。</p>
<p>$$\elll_s(P, Q) = \mathcal{D}<em>{KL} (P \parallel Q) + \mathcal{D}</em>{KL} (Q \parallel P)$$</p>
<p>当模型的输出是一个Scalar，也就是做类似回归任务时，有：</p>
<p>$$\ell_s(p, q) = \parallel p - q \parallel^2$$</p>
<h3 id="bregman-proximal-point-optimization">Bregman Proximal Point Optimization</h3>
<p>这一步主要是为了防止模型的参数在每个Step中更新过大。Transformer模型中的小的 lr 本身也是一种正则化，也就是让模型的参数不会变化非常大，提高模型的泛化性能、鲁棒性等。</p>
<p>Vanilla Bregman Proximal Point (VBPP) 算法定义了模型参数更新过程：</p>
<p>$$\theta_{t+1} = \mathrm{ArgMin}<em>{\theta} \mathcal{F}(\theta) + \mu \mathcal{D}</em>{Breg}(\theta, \theta_t)$$</p>
<p>其中，$\mu &gt; 0$，$\mathcal{D}_{Breg}$用于阻止模型的参数更新过大：</p>
<p>$$\mathcal{D} _{Breg}(\theta, \theta_t) = \frac{1}{n}\sum _{i=1}^n \ell_s (f(x_i; \theta), f(x_i; \theta_t))$$</p>
<p>实际使用中，可以借助Momentum Update的方式来更新参考的模型权重：</p>
<p>$$\tilde{\theta}<em>t = (1 - \beta)\theta_t + \beta \tilde{\theta}</em>{t-1}$$</p>
<p>然后计算$\mathcal{D}_{Breg}(\theta, \tilde{\theta}_t)$。这个过程与EMA非常相似，在自监督学习中经常被使用。</p>
<h3 id="伪代码">伪代码</h3>
<p>SMART算法实现的伪代码如图-5。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/smart0.png" alt="图 - 5 SMART算法伪代码">
    <figcaption>图 - 5 SMART算法伪代码</figcaption>
    </center>
</figure></p>
<h2 id="其它">其它</h2>
<ul>
<li>
<p>YOPO: <a href="https://arxiv.org/pdf/1905.00877.pdf">You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</a></p>
<p>通过分析Pontryagin’s Maximum Principle，观察到每次对抗样本的更新只与模型的前几层的梯度有关，基于这个观察，作者提出了 YOPO 算法。</p>
<p>关于 PMP &amp; Hamiltonian 看不懂，略过。</p>
<blockquote>
<p>Momentum should be accumulated between mini-batches other than different adversarial examples from one mini-batch, otherwise overfitting will become a serious problem.</p>
</blockquote>
</li>
<li>
<p>ALUM: <a href="https://arxiv.org/pdf/2004.08994.pdf">Adversarial training for large neural LangUage Models</a></p>
<p>这篇文章就提到了，对抗训练一方面是为了提高鲁棒性，另一方面是为了提高泛化性。本文提出的模型叫做 MT-DNN，上面的 SMART 算法与这个算法也可以结合起来。本文发现对抗学习也可以提高预训练阶段的效果，。</p>
<p>发现，virtual adversarial training 比 conventional adversarial trainging 效果更好，尤其是存在 noisy label 的时候。 BERT 预训练的 MLM 就是属于 noisy label 的情况，因为被 mask 的 word 实际上可以有很多的选择。所以 SMART 中的 $\lambda$ 在预训练阶段会比较大，比如 = 10，微调阶段为 = 1。</p>
</li>
<li>
<p>CIFS: <a href="https://arxiv.org/abs/2102.05311">CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection</a></p>
<p>对每个channel进行channel-wise 的扰动！</p>
</li>
</ul>
<h2 id="代码实现">代码实现</h2>
<p>先来看下 FreeLB 的代码实现，参考：<a href="https://github.com/zhuchen03/FreeLB">FreeLB - github</a>。在这个<a href="https://github.com/zhuchen03/FreeLB/blob/master/huggingface-transformers/launch/run_glue.sh">run_glue.sh</a> 脚本中，不同的下游任务对 $\delta$ 的初始化也不同，比如全零，或者<code>uniform()</code>等。<a href="https://github.com/mahyarnajibi/FreeAdversarialTraining">FreeAT - github</a>的实现代码其实与此类似。</p>
<p>代码里的亮点在于：(1) 首先获取输入文本的 Embedding 表示 (2) 初始化 $\delta$ (3) 迭代求解最优的对抗样本。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">88
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">89
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">90
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">91
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">92
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">93
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">94
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">for</span> step, batch <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(dataloader):
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">isinstance</span>(model, torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>DataParallel):
</span></span><span style="display:flex;"><span>        embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>module<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_init_mag <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        input_mask <span style="color:#000;font-weight:bold">=</span> inputs[<span style="color:#d14">&#39;attention_mask&#39;</span>]<span style="color:#000;font-weight:bold">.</span>to(embeds_init)
</span></span><span style="display:flex;"><span>        input_lengths <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(input_mask, <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#998;font-style:italic"># check the shape of the mask here..</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;l2&#34;</span>:
</span></span><span style="display:flex;"><span>            delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)<span style="color:#000;font-weight:bold">.</span>uniform_(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>,<span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> input_mask<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>            dims <span style="color:#000;font-weight:bold">=</span> input_lengths <span style="color:#000;font-weight:bold">*</span> embeds_init<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>            mag <span style="color:#000;font-weight:bold">=</span> args<span style="color:#000;font-weight:bold">.</span>adv_init_mag <span style="color:#000;font-weight:bold">/</span> torch<span style="color:#000;font-weight:bold">.</span>sqrt(dims)
</span></span><span style="display:flex;"><span>            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">*</span> mag<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>))<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">elif</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;linf&#34;</span>:
</span></span><span style="display:flex;"><span>            delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)<span style="color:#000;font-weight:bold">.</span>uniform_(<span style="color:#000;font-weight:bold">-</span>args<span style="color:#000;font-weight:bold">.</span>adv_init_mag,
</span></span><span style="display:flex;"><span>                                                            args<span style="color:#000;font-weight:bold">.</span>adv_init_mag) <span style="color:#000;font-weight:bold">*</span> input_mask<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>        delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#998;font-style:italic"># the main loop</span>
</span></span><span style="display:flex;"><span>    dp_masks <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">None</span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">for</span> astep <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(args<span style="color:#000;font-weight:bold">.</span>adv_steps):
</span></span><span style="display:flex;"><span>        <span style="color:#998;font-style:italic"># (0) forward</span>
</span></span><span style="display:flex;"><span>        delta<span style="color:#000;font-weight:bold">.</span>requires_grad_()
</span></span><span style="display:flex;"><span>        inputs[<span style="color:#d14">&#39;inputs_embeds&#39;</span>] <span style="color:#000;font-weight:bold">=</span> delta <span style="color:#000;font-weight:bold">+</span> embeds_init
</span></span><span style="display:flex;"><span>        inputs[<span style="color:#d14">&#39;dp_masks&#39;</span>] <span style="color:#000;font-weight:bold">=</span> dp_masks
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        outputs, dp_masks <span style="color:#000;font-weight:bold">=</span> model(<span style="color:#000;font-weight:bold">**</span>inputs)
</span></span><span style="display:flex;"><span>        loss <span style="color:#000;font-weight:bold">=</span> outputs[<span style="color:#099">0</span>]  <span style="color:#998;font-style:italic"># model outputs are always tuple in transformers (see doc)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#998;font-style:italic"># (1) backward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>n_gpu <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#000;font-weight:bold">=</span> loss<span style="color:#000;font-weight:bold">.</span>mean()  <span style="color:#998;font-style:italic"># mean() to average on multi-gpu parallel training</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#000;font-weight:bold">=</span> loss <span style="color:#000;font-weight:bold">/</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#000;font-weight:bold">=</span> loss <span style="color:#000;font-weight:bold">/</span> args<span style="color:#000;font-weight:bold">.</span>adv_steps        <span style="color:#998;font-style:italic"># 求解梯度的平均</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        tr_loss <span style="color:#000;font-weight:bold">+=</span> loss<span style="color:#000;font-weight:bold">.</span>item()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>fp16:
</span></span><span style="display:flex;"><span>            <span style="color:#000;font-weight:bold">with</span> amp<span style="color:#000;font-weight:bold">.</span>scale_loss(loss, optimizer) <span style="color:#000;font-weight:bold">as</span> scaled_loss:
</span></span><span style="display:flex;"><span>                scaled_loss<span style="color:#000;font-weight:bold">.</span>backward()
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            loss<span style="color:#000;font-weight:bold">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> astep <span style="color:#000;font-weight:bold">==</span> args<span style="color:#000;font-weight:bold">.</span>adv_steps <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#998;font-style:italic"># further updates on delta</span>
</span></span><span style="display:flex;"><span>            <span style="color:#000;font-weight:bold">break</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#998;font-style:italic"># (2) get gradient on delta</span>
</span></span><span style="display:flex;"><span>        delta_grad <span style="color:#000;font-weight:bold">=</span> delta<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>clone()<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#998;font-style:italic"># (3) update and clip</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;l2&#34;</span>:
</span></span><span style="display:flex;"><span>            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta_grad<span style="color:#000;font-weight:bold">.</span>view(delta_grad<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>), dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(denorm, <span style="color:#0086b3">min</span><span style="color:#000;font-weight:bold">=</span><span style="color:#099">1e-8</span>)
</span></span><span style="display:flex;"><span>            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">+</span> args<span style="color:#000;font-weight:bold">.</span>adv_lr <span style="color:#000;font-weight:bold">*</span> delta_grad <span style="color:#000;font-weight:bold">/</span> denorm)<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>            <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:   <span style="color:#998;font-style:italic"># 通常为0 或者 1e-7</span>
</span></span><span style="display:flex;"><span>                delta_norm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta<span style="color:#000;font-weight:bold">.</span>view(delta<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>float(), p<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>, dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>                exceed_mask <span style="color:#000;font-weight:bold">=</span> (delta_norm <span style="color:#000;font-weight:bold">&gt;</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm)<span style="color:#000;font-weight:bold">.</span>to(embeds_init)
</span></span><span style="display:flex;"><span>                reweights <span style="color:#000;font-weight:bold">=</span> (args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">/</span> delta_norm <span style="color:#000;font-weight:bold">*</span> exceed_mask \
</span></span><span style="display:flex;"><span>                                <span style="color:#000;font-weight:bold">+</span> (<span style="color:#099">1</span><span style="color:#000;font-weight:bold">-</span>exceed_mask))<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>                delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">*</span> reweights)<span style="color:#000;font-weight:bold">.</span>detach()            <span style="color:#998;font-style:italic"># 进入下一次循环</span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">elif</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;linf&#34;</span>:
</span></span><span style="display:flex;"><span>            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta_grad<span style="color:#000;font-weight:bold">.</span>view(delta_grad<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>), dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, p<span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">float</span>(<span style="color:#d14">&#34;inf&#34;</span>))<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(denorm, <span style="color:#0086b3">min</span><span style="color:#000;font-weight:bold">=</span><span style="color:#099">1e-8</span>)
</span></span><span style="display:flex;"><span>            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">+</span> args<span style="color:#000;font-weight:bold">.</span>adv_lr <span style="color:#000;font-weight:bold">*</span> delta_grad <span style="color:#000;font-weight:bold">/</span> denorm)<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>            <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
</span></span><span style="display:flex;"><span>                delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(delta, <span style="color:#000;font-weight:bold">-</span>args<span style="color:#000;font-weight:bold">.</span>adv_max_norm, args<span style="color:#000;font-weight:bold">.</span>adv_max_norm)<span style="color:#000;font-weight:bold">.</span>detach()
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#0086b3">print</span>(<span style="color:#d14">&#34;Norm type </span><span style="color:#d14">{}</span><span style="color:#d14"> not specified.&#34;</span><span style="color:#000;font-weight:bold">.</span>format(args<span style="color:#000;font-weight:bold">.</span>norm_type))
</span></span><span style="display:flex;"><span>            exit()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">isinstance</span>(model, torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>DataParallel):
</span></span><span style="display:flex;"><span>            embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>module<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#998;font-style:italic"># ============================ End (2) ==================</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> (step <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">%</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>fp16:
</span></span><span style="display:flex;"><span>            torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>utils<span style="color:#000;font-weight:bold">.</span>clip_grad_norm_(amp<span style="color:#000;font-weight:bold">.</span>master_params(optimizer), args<span style="color:#000;font-weight:bold">.</span>max_grad_norm)
</span></span><span style="display:flex;"><span>        <span style="color:#000;font-weight:bold">else</span>:
</span></span><span style="display:flex;"><span>            torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>utils<span style="color:#000;font-weight:bold">.</span>clip_grad_norm_(model<span style="color:#000;font-weight:bold">.</span>parameters(), args<span style="color:#000;font-weight:bold">.</span>max_grad_norm)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#000;font-weight:bold">.</span>step()
</span></span><span style="display:flex;"><span>        scheduler<span style="color:#000;font-weight:bold">.</span>step()  <span style="color:#998;font-style:italic"># Update learning rate schedule</span>
</span></span><span style="display:flex;"><span>        model<span style="color:#000;font-weight:bold">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        global_step <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对应的 SMART 代码在<a href="https://github.com/namisan/mt-dnn">mt-dnn</a>。</p>

		
	</div>

	<div class="pagination">
		<a href="/posts/adversarial-training/" class="left arrow">&#8592;</a>
		<a href="/posts/swin-transformer/" class="right arrow">&#8594;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2022-07-25 12:50:02.734602768 &#43;0800 CST m=&#43;0.141796831">2022</time> triloon. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
