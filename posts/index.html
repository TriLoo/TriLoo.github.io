<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Posts &middot; Triloon</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
        <link rel="stylesheet" href="/css/custom.css">
		
		<link rel="icon" href="favicon.ico" />
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Triloon" />
	</head>

    <body>
        
		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h1 class="nav-title">Triloon</h1>
					
				</a>
				<ul>
    
    
        <li>
            <a href="/about/about">
                
                <span>About</span>
                
            </a>
        </li>
    
        <li>
            <a href="/posts/">
                
                <span>Posts</span>
                
            </a>
        </li>
    
</ul>
			</div>
		</nav>

        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">

        

<main>
	<div class="catalogue">
		
			<a href="https://triloon.space/posts/torch-allgather/" class="catalogue-item">
    <div>
        <time datetime="2022-01-16 18:43:13 &#43;0800 CST" class="catalogue-time">January 16, 2022</time>
        <h2 class="catalogue-title">Torch all_gather 的梯度问题</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>pytorch all_gather 计算结果是叶子节点，也就是不会继续向后传递梯度了。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/swin-transformer-v2/" class="catalogue-item">
    <div>
        <time datetime="2021-11-23 12:07:18 &#43;0800 CST" class="catalogue-time">November 23, 2021</time>
        <h2 class="catalogue-title">Swin Transformer V2</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>本文围绕如何有效的增加模型参数量、弥补不同任务输入图像尺寸不同时的Windows大小不同导致的相对位置编码变化问题这两个任务提出了解决方案，总的来说，方案简单有效，值得学习。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/dali-intro/" class="catalogue-item">
    <div>
        <time datetime="2021-11-22 17:20:46 &#43;0800 CST" class="catalogue-time">November 22, 2021</time>
        <h2 class="catalogue-title">DALI简单实用案例</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>DALI(NVIDIA Data Loading Library)库是NVIDIA提供的用于加速数据加载过程的代码库，支持在GPU上完成一些数据处理，从而提高加载速度；另一方面是方便多种源数据文件格式的加载，包括MXNet RecordIO / TFRrecord / LMDB 或者 文件目录等形式的数据集加载；第三就是支持多种数据格式，包括图片、视频、音频等。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/mlm-related-2/" class="catalogue-item">
    <div>
        <time datetime="2021-10-27 22:08:15 &#43;0800 CST" class="catalogue-time">October 27, 2021</time>
        <h2 class="catalogue-title">常见掩码生成方式 2</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>这是接着上一篇掩码生成方式写的，主要仅包含SpanBERT &amp; MacBERT的原理与实现。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/tokenizers/" class="catalogue-item">
    <div>
        <time datetime="2021-10-22 19:51:11 &#43;0800 CST" class="catalogue-time">October 22, 2021</time>
        <h2 class="catalogue-title">分词算法基础</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>常见的几种分词算法小结，包括BERT用到WordPiece以及Albert用到的Byte-Pair-Encoding。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/mlm-related/" class="catalogue-item">
    <div>
        <time datetime="2021-10-19 19:51:11 &#43;0800 CST" class="catalogue-time">October 19, 2021</time>
        <h2 class="catalogue-title">常见掩码生成方式</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>主要是几种常见的MLM的改进以及对应的代码实现，包括WWM, SpanBERT, ERNIE这三种。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/swin-transformer/" class="catalogue-item">
    <div>
        <time datetime="2021-10-15 22:07:45 &#43;0800 CST" class="catalogue-time">October 15, 2021</time>
        <h2 class="catalogue-title">ICCV2021 Best Paper - Swin Transformer</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>重读论文之后，发现还真是一个非常精巧的模型。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/adversarial-training-2/" class="catalogue-item">
    <div>
        <time datetime="2021-10-13 14:30:10 &#43;0800 CST" class="catalogue-time">October 13, 2021</time>
        <h2 class="catalogue-title">Adversarial Training 2</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>对PGD算法的改进，包括FreeAT, FreeLB, SMART等。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/adversarial-training/" class="catalogue-item">
    <div>
        <time datetime="2021-10-05 11:21:33 &#43;0800 CST" class="catalogue-time">October 5, 2021</time>
        <h2 class="catalogue-title">Adversarial Training</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>几个基础的常见的对抗样本的生成算法，包括FGM/FGSM, PGD等。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/torch-usage/" class="catalogue-item">
    <div>
        <time datetime="2021-10-01 16:06:21 &#43;0800 CST" class="catalogue-time">October 1, 2021</time>
        <h2 class="catalogue-title">Torch的一些使用方法记录</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>记录一些Torch使用过程中会用到的小知识点。</p>
        </p>
    </div>
</a>

		
	</div>
	
	<div class="pagination">
		
		
			<a href="/posts/page/2/" class="right arrow">&#8594;</a>
		
        
            <p>pytorch all_gather 计算结果是叶子节点，也就是不会继续向后传递梯度了。</p>
<h2 id="背景">背景</h2>
<ul>
<li>
<p>背景一：使用 all_gather 来获取其它 GPU 上的参数</p>
<p>最早接触使用Pytorch的<code>all_gather</code>来获取其它GPU上的数据在当前进程中使用的代码应该是 MoCo 论文中的实现：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#3c5d5d;font-weight:bold">@torch.no_grad</span>()
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">concat_all_gather</span>(tensor):
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Performs all_gather operation on the provided tensors.
</span><span style="color:#d14">    *** Warning ***: torch.distributed.all_gather has no gradient.
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    tensors_gather <span style="color:#000;font-weight:bold">=</span> [torch<span style="color:#000;font-weight:bold">.</span>ones_like(tensor)
        <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(torch<span style="color:#000;font-weight:bold">.</span>distributed<span style="color:#000;font-weight:bold">.</span>get_world_size())]
    torch<span style="color:#000;font-weight:bold">.</span>distributed<span style="color:#000;font-weight:bold">.</span>all_gather(tensors_gather, tensor, async_op<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
  
    output <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>cat(tensors_gather, dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    <span style="color:#000;font-weight:bold">return</span> output
</code></pre></td></tr></table>
</div>
</div><p>一方面是，<code>concat_all_gather</code>函数使用了<code>no_grad()</code>修饰器；另一方面，即使不用 <code>no_grad</code> 修饰，这里的结果（也就是<code>output</code>）的梯度也不会传递给输入参数<code>tensor</code>。</p>
</li>
<li>
<p>背景二：对 all_gather 的结果进行梯度后向传播</p>
<p>代码中使用了普通的 triplet loss 计算 Loss，然后进行梯度更新，triplet loss 函数的中的 anchor 来自于离线计算好的数据，因此不会进行梯度后向传播（requires_grad = False），而 pos, neg 则来自于上述<code>concat_all_gather()</code>函数的输出。最开始的时候，<code>autograd.backward(loss)</code> 的计算会报错，提示计算 loss 的几个参数都不需要计算梯度，去掉<code>torch.no_grad()</code>之后，错误仍然存在。</p>
<p>另一个现象是，当 anchor 也来自于模型计算（可以梯度后向传播时），使用<code>concat_all_gather()</code>的结果计算 triplet loss 会比只使用当前 GPU 上输出作为 pos / neg 时速度快上一倍以上，这就非常违反直觉了，因为 <code>all_gather</code> 的通信开销应该导致速度更慢才对。</p>
<p>因此， 考虑<code>concat_all_gather()</code>函数体中导致梯度传播中断的计算。主要是两个地方，一个是<code>ones_like()</code>这里，默认创建的 tensor 具有<code>requires_grad=False</code>参数，因此将代码替换为：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    tensors_gather <span style="color:#000;font-weight:bold">=</span> [torch<span style="color:#000;font-weight:bold">.</span>ones_like(tensor, requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)
        <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(torch<span style="color:#000;font-weight:bold">.</span>distributed<span style="color:#000;font-weight:bold">.</span>get_world_size())]
</code></pre></td></tr></table>
</div>
</div><p>然而错误、或者训练速度异常仍然存在，因此，错误也就只可能出在 <code>all_gather()</code> 计算上了。</p>
</li>
</ul>
<p>搜索引擎了一下，发现下面相关帖子:</p>
<p><a href="https://discuss.pytorch.org/t/will-dist-all-gather-break-the-auto-gradient-graph/47350">Will “dist.all_gather” break the auto gradient graph?</a></p>
<h2 id="让all_gather支持梯度传播">让all_gather支持梯度传播</h2>
<p>上面的问题总结出来就是，torch.dist 中自带的 <code>all_gather</code> 函数会阻断梯度的后向传播。针对这个问题，帖子中也给出了一个新的实现代码，并且配合新的<code>concat_all_gather</code>的实现代码如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">torch</span>
<span style="color:#000;font-weight:bold">import</span> <span style="color:#555">torch.distributed</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">dist</span>

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">AllGather</span>(torch<span style="color:#000;font-weight:bold">.</span>autograd<span style="color:#000;font-weight:bold">.</span>Function):
    <span style="color:#d14">&#34;&#34;&#34; 
</span><span style="color:#d14">    all_gather with gradient back-propagation
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(ctx, tensor_list, tensor):
        dist<span style="color:#000;font-weight:bold">.</span>all_gather(tensor_list, tensor)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#0086b3">tuple</span>(tensor_list)

    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">backward</span>(ctx, <span style="color:#000;font-weight:bold">*</span>grad_list):
        grad_list <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">list</span>(grad_list)
        rank <span style="color:#000;font-weight:bold">=</span> dist<span style="color:#000;font-weight:bold">.</span>get_rank()

        dist_ops <span style="color:#000;font-weight:bold">=</span> [
            dist<span style="color:#000;font-weight:bold">.</span>reduce(grad_list[i], i, async_op<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(dist<span style="color:#000;font-weight:bold">.</span>get_world_size())
        ]

        <span style="color:#000;font-weight:bold">for</span> op <span style="color:#000;font-weight:bold">in</span> dist_ops:
            op<span style="color:#000;font-weight:bold">.</span>wait()

        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">None</span>, grad_list[rank] 


all_gather <span style="color:#000;font-weight:bold">=</span> AllGather<span style="color:#000;font-weight:bold">.</span>apply

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">concat_all_gather</span>(tensor):
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Performs all_gather operation on the provided tensors.
</span><span style="color:#d14">    *** Warning ***: torch.distributed.all_gather has no gradient.
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    tensors_gather <span style="color:#000;font-weight:bold">=</span> [torch<span style="color:#000;font-weight:bold">.</span>ones_like(tensor)
        <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(torch<span style="color:#000;font-weight:bold">.</span>distributed<span style="color:#000;font-weight:bold">.</span>get_world_size())]
    all_gather(tensors_gather, tensor)

    output <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>cat(tensors_gather, dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    <span style="color:#000;font-weight:bold">return</span> output
</code></pre></td></tr></table>
</div>
</div><p>后记，看MoCo代码中<code>concat_all_gather</code>的注释，原来答案就在纸面上，擦。</p>
        
            <p>本文围绕如何有效的增加模型参数量、弥补不同任务输入图像尺寸不同时的Windows大小不同导致的相对位置编码变化问题这两个任务提出了解决方案，总的来说，方案简单有效，值得学习。</p>
<h2 id="背景">背景</h2>
<p>首先，NLP里面用到的 Transformer 模型的参数量借助 MoE 等技术已经到达万亿规模，虽然可能存在没有充分利用模型容量的问题，但是整体来看，随着模型容量（参数量）的提高，在语言任务上的效果是越好的。</p>
<p>然而在CV领域并非这样，随着模型参数量的增加，模型训练也更加困难，并且其实现阶段图像还是一个比较低效率的信息载体（没法像文本那样传递更抽象的信息），通常存在大量的像素冗余，而这些冗余又是传递信息所必需的，这导致图像数据的收集以及挖掘抽象信息从而高效利用数据都存在问题；另一方面，CV多样的任务对输入图像的尺寸也有不同的要求，如果只是全局分类，那么图像尺寸只需要 224 * 224 就可以做，但是如果涉及到分割、目标检测等任务则需要非常细粒度的信息，也就需要保证输入图像的分辨率足够大，一般来说模型预训练数据量大，为了保证训练效率，预训练一般采用比较低的分辨率，这也就导致与下游这些任务对分辨率需求上存在 Gap，也导致模型的效果在这些任务上下降。</p>
<p>针对模型的 Scaling 问题，本文主要分析了为什么随着模型参数量的增加，会存在模型训练不稳定的问题，作者发现主要是因为残差结构的存在，导致约往后，模型的输出的数值的量级也越大，结果就是后面层的激活值输出相对于浅层的激活值相差达到10e4这个量级。然后针对分辨率的变化，作者提出用 Log-spaced Continuous Position Bias 来Scaling 相对位置编码参数。</p>
<p>关于CV图片数据的利用，可以参考kaiming的 MAE论文，也就是人为的掩盖75%的像素来让模型高效利用数据。但是不论是CV中模型参数量的扩展还是数据的利用都应该会有更大突破的想法出现。</p>
<p>下面是本文的主要三个技术的一些细节。</p>
<h2 id="方案">方案</h2>
<p>针对模型参数 Scaling 问题，作者发现是因为模型后面层的激活输出值量级太大了，如图一所示，可以看出（以B-Pre）为例，Block 22 的最大值达到了Block 1 最大值的 10e4 倍。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer-v2/swinv2-0.png" alt="图 1 不同层的输出值的量级增加非常快">
    <figcaption>图 1 不同层的输出值的量级增加非常快</figcaption>
    </center>
</figure></p>
<h3 id="post-normalization--scaled-cosine-attention">Post Normalization &amp; Scaled Cosine Attention</h3>
<p>看 ViT 的实现代码可以发现，ViT 采用了 Pre-Normalization 实现方式，所以这里采用 Post-Normalization 的方式，注意，这里只对 Attention / MLP 层的输出计算 Layer Norm；Scaled Cosine Attention 其实就是使用 Scaled Cosine 计算代替原来的 Scaled Dot-Product Attention 的计算。这里虽然都有 Scaled，但是前者的 Scale $\tau$ 是学习得到的参数，并且大于0.01，但是后者是一个固定的数$\sqrt{d_k}$，也就是每个 Head 的维度。</p>
<p>Scaled Cosine Attention 的数学表达式是：</p>
<p>$$Sim(q_i, k_j) = \cos (q_i, k_j) / \tau + B_{ij}$$</p>
<p>其中，$B_{ij}$就是相对位置编码参数，本文中也就是下面提到的 Log-spaced Continuous Position Bias来计算的。</p>
<p>上述两个改动与V1版本的对比示意图如图2所示。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer-v2/swinv2-1.png" alt="图 2 Post Norm &amp;amp; Scaled Cosine Attention 示意图">
    <figcaption>图 2 Post Norm &amp;amp; Scaled Cosine Attention 示意图</figcaption>
    </center>
</figure></p>
<p>图3展示了上述两个改动的 ablation 实验，发现两个改动都对效果有帮助，当然重要的还是可以将模型容量进行扩充。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer-v2/swinv2-3.png" alt="图 3 Post Norm &amp;amp; Scaled Cosine Attention 效果分析">
    <figcaption>图 3 Post Norm &amp;amp; Scaled Cosine Attention 效果分析</figcaption>
    </center>
</figure></p>
<h3 id="log-space-continuous-position-bias">Log-space Continuous Position Bias</h3>
<p>这一部分的目的是实现 Scaling Up Window Resolution。首先来看下什么是 Continuous Position Bias，这是相对于参数化的相对位置编码而言的，后者是直接学习相对位置编码的 Embedding；而 Continuous Position Bias 的方案是采用一个小的 Meta 网络来映射相对位置：</p>
<p>$$B(\Delta x, \Delta y) = \mathcal{G}(\Delta x, \Delta y)$$</p>
<p>其中$\mathcal{G}$可以是一个中间使用 ReLU 激活函数的2层 MLP。</p>
<p>为了避免因为 windows 大小变化太大导致需要外推出很多之前没用过的相对位置信息，作者提出用将线性空间的星队距离映射到 log 空间中，然后输入到上述Meta网络中生成相对位置编码。映射到 log 空间的过程如下：</p>
<p>$$\hat{\Delta x} = sign (x) \cdot \log (1 + | \Delta x |)$$</p>
<p>$$\hat{\Delta y} = sign (y) \cdot \log (1 + | \Delta y |)$$</p>
<p>其中，$\Delta x, \Delta y, \hat{\Delta x}, \hat{\Delta y}$分别表示线性空间、log空间的相对位置量，与Swin Transformer使用的参数化的位置编码相比，log域的效果最好。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer-v2/swinv2-2.png" alt="图 4 3种相对位置编码效果对比">
    <figcaption>图 4 3种相对位置编码效果对比</figcaption>
    </center>
</figure></p>
<p>表格里每个位置表示不适用 / 使用微调训练的效果，可以看出，使用 log 域的相对位置编码，即使不进行微调训练也可以在一定程度上保持模型效果，而且还可能效果更好（这主要是因为windows变大了）！</p>
<h3 id="省显存">省显存</h3>
<p>作者用到了下面三个措施来降低显存使用。</p>
<ul>
<li>ZeRO Stage 1，即将AdamW优化器的一些参数分配到不同的 GPU 上（类似模型并行），这个做法可以显著降低显存开销，重要的是对训练速度影响非常小</li>
<li>Activation Checkpoint，这样大概会降低训练速度30%的样子</li>
<li>Sequential Self-Attention Computation，讲Batch内样本的 Self Attention 的计算串行化，对训练速度非常小</li>
</ul>
<p>此外，增加模型参数量的方法主要还是增加 channel 宽度、增加 Stage 3 的层数，如6 -&gt; 18 -&gt; 42等。</p>
<h2 id="实现">实现</h2>
<p>有待补充。</p>
        
            <p>DALI(NVIDIA Data Loading Library)库是NVIDIA提供的用于加速数据加载过程的代码库，支持在GPU上完成一些数据处理，从而提高加载速度；另一方面是方便多种源数据文件格式的加载，包括MXNet RecordIO / TFRrecord / LMDB 或者 文件目录等形式的数据集加载；第三就是支持多种数据格式，包括图片、视频、音频等。</p>
<h2 id="总览">总览</h2>
<p>使用DALI，大体分为三个步骤：</p>
<ul>
<li>定义预处理的 Graph</li>
<li>编译Graph，用于Engine执行</li>
<li>（可选）使用迭代器进行封装，进行数据遍历</li>
</ul>
<p>前两个步骤都是基于<code>Pipeline</code>类来完成，定义Graph对应的是定义新的 Pipeline 对象，下面会提到三种方式；编译的过程通过调用<code>Pipeline.build()</code>函数来实现。第三个步骤是可选的，DALI提供一些Iterator类，接受一个Pipeline作为参数来，然后进行迭代，其实通过<code>Pipeline.run()</code>也可以得到新的元素。</p>
<h2 id="一些概念">一些概念</h2>
<h3 id="pipeline">Pipeline</h3>
<p>Pipeline就是定义数据预处理的对象，包含预处理的计算图定义、执行引擎。用户通过定义新的 Pipeline 来实现新的数据预处理过程，定义新的 Pipeline 的方式有三种：</p>
<ul>
<li>使用修饰器<code>pipeline_def()</code>进行定义</li>
<li>定义<code>Pipeline</code>类，并借助<code>Pipeline.set_outputs()</code>函数来指定Pipeline的输出变量列表</li>
<li>通过继承<code>Pipeline</code>类，并重定义派生类的<code>define_graph()</code>函数来定义新的计算图(这是传统的实现方法)</li>
</ul>
<p>上面三种方式都可以实现将预初期定义为<strong>当前</strong> Pipeline 的计算图，也就是说，<code>define_graph()</code>函数定的计算天然的属于当前的 Pipeline；使用第一种中的修饰器也是；对于第二种，则可以使用<code>with</code>语句实现，如下例：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pipe <span style="color:#000;font-weight:bold">=</span> dali<span style="color:#000;font-weight:bold">.</span>Pipeline(batch_size<span style="color:#000;font-weight:bold">=</span>N, num_threads<span style="color:#000;font-weight:bold">=</span><span style="color:#099">3</span>, device_id<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
<span style="color:#000;font-weight:bold">with</span> pipe:
    src <span style="color:#000;font-weight:bold">=</span> dali<span style="color:#000;font-weight:bold">.</span>ops<span style="color:#000;font-weight:bold">.</span>ExternalSource(my_source, num_outputs<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>)
    a, b <span style="color:#000;font-weight:bold">=</span> src()
    pipe<span style="color:#000;font-weight:bold">.</span>set_outputs(a, b)
</code></pre></td></tr></table>
</div>
</div><p>其中用到的<code>Pipeline</code>类的重要参数包括：<code>batch_size</code>、<code>num_threads</code>、<code>device_id</code>等，<code>device_id=None</code>表示不实用GPU，其他的包括一些性能方面考虑的参数：<code>set_affinity、max_streams、bytes_per_sample</code>等。对应上面代码，使用第一种的等价实现代码如下：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#3c5d5d;font-weight:bold">@dali.pipeline_def</span>(batch_size<span style="color:#000;font-weight:bold">=</span>N, num_threads<span style="color:#000;font-weight:bold">=</span><span style="color:#099">3</span>, device_id<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">my_pipe</span>(my_source):
    <span style="color:#000;font-weight:bold">return</span> dali<span style="color:#000;font-weight:bold">.</span>fn<span style="color:#000;font-weight:bold">.</span>external_source(my_source, num_outputs<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>)

pipe <span style="color:#000;font-weight:bold">=</span> my_pipe(my_source)
</code></pre></div><p>Pipeline 中的计算图（Graph）包含两类节点:</p>
<ul>
<li>Operators</li>
<li>Data Nodes: 支持像Python一样的Indexing，并且可以数学计算</li>
</ul>
<p>Pipeline 按照 Stage 来划分计算流程，包括三种 Stage:</p>
<ul>
<li>cpu: CPU输入、CPU输出，通过调用<code>.gpu()</code>来将数据拷贝到 GPU 上</li>
<li>gpu: GPU输入、GPU输出</li>
<li>mixed： CPU输入、GPU输出</li>
</ul>
<p>注意，位于GPU上的数据在Graph中是不能被传回CPU的，只能后面使用``。大多数的DALI操作都会接收命名参数，支持的命名参数的种类有两个：</p>
<ul>
<li>Python constants</li>
<li>Argument Input，这是特指的名字，必须是 CPU 操作的输出的变量，变量位于 CPU 上</li>
</ul>
<p>对应于由Graph编译得到 Engine，通过<code>build()</code>成员函数来实现。通过调用<code>run()</code>函数可以获得新的迭代元素进行处理，这一步得到的数据类型是下面提到的<code>TensorList</code>格式。</p>
<h3 id="tensorlist">TensorList</h3>
<p><code>Pipeline.run(), Pipeline.outputs(), Pipeline.share_ouptuts()</code>等函数返回的数据的类型是<code>TensorList</code>，表示一个batch的Tensor；<code>Pipeline.releast_outputs()</code>让当前的 Tensor 不在可用，也就是说 DALI 将可以使用对应的存储资源来存放其他的变量，而且当前 iteration 的 Tensor 在下一个 iteration 也会变得不可用，只能保存到其他数据里（如 Torch 的 Tensor / MXNet 的 NDArray）才可以另作他用。</p>
<p>TensorList 包含两个具体的子类，<code>TensorListCPU / TensorListGPU</code>，顾名思义不赘述。重要的成员函数包括<code>at(), as_array()</code>等，前者类似获取列表对应位置索引处的数据，后者用于将 Tensor 转换为numpy array。<code>TensorListGPU</code>的<code>as_cpu()</code>来将数据拷贝到  <code>cpu</code>端。<code>layout()</code>函数获取当前TensorList的数据存储格式，如HWC / CHW等。</p>
<p>与DataNode的区别。DataNode是TensorList的一个表示符号，并不真正的包含数据，只用于Graph定义阶段，也就是链接两个 Operator。既然 TensorList 在Graph定义阶段使用DataNode表征的，所以用于Graph定义的数学计算操作也是对DataNode进行处理的，包括三角函数、指数/对数、开方、N次方、floor、ceil、clamp、加减乘除等操作。注意这些数学表达式中至少有一个参数是DALI Operator的输出才行，其他的输入可以是<code>nvidia.dali.types.Constant</code>或者常规的 Python 常量数据；此外，所有的计算操作都是 element-wise 的方式进行的，支持广播。</p>
<p>多说一些 DataNode。DataNode支持indexing / slicing，并且支持负数的索引或者表示范围等。</p>
<h3 id="reader">Reader</h3>
<p>DALI的一个优势就是对各种数据的文件类型进行了抽象与统一，也就是使用特定的函数/类完成数据的加载以后，后续的所有的处理操作都可以复用。Reader的意思就是从磁盘文件进行数据解析、加载的函数，<strong>一般是Graph定义的第一个步骤</strong>。几个常见的Reader是：</p>
<ul>
<li>nvidia.dali.fn.readers.mxnet()，读入<code>RecordIO</code>格式的数据，输入<code>.rec / .idx</code>文件路径</li>
<li>nvidia.dali.fn.readers.tfrecord()，读入<code>TFRecord</code>格式数据；这里需要先调用<code>tfrecord2idx</code>脚本来生成<code>.idx</code>文件，然后作为参数使用。<code>tfrecord2idx</code>脚本的实现可以帮助理解一下tfrecord文件的布局，整体思路与 RecordIO 的类似，但是应该是使用了 Protobuf 进行了编码，所以解析过程不够直观</li>
<li>nvidia.dali.fn.readers.caffe()，读入<code>LMDB</code>格式数据</li>
<li>nvidia.dali.fn.readers.file()解码文件目录等</li>
</ul>
<p>这些Reader的创建函数都会接收两个参数，<code>shard_id</code>以及<code>num_shards</code>，后者表示将原始数据分成多少份、前者表示当前是第几部分，可以用于多进程训练时数据的分配。此外，在定义过程中一般使用<code>name</code>参数来指定Reader的名称，可用于后面的Iterator等。</p>
<p>Reader函数都接收<code>random_shuffle</code>参数，用于表明是否对数据集进行随机打乱。这里所说的随机打乱并非进行 Global 层次的打乱，而是在参数<code>initialize_fill</code>参数指定的 buffer 里进行打乱，也就是Local 层次的随机打乱；当然如果有多个文档输入的时候，会现在文档层次进行打乱。</p>
<p>另一个比较特殊的是<code>dali.ops.ExternalSource</code>可调用类或<code>dali.fn.external_source()</code>函数的使用，在上面 Pipeline 的代码实例里也提到了，可以接受一个 Python 实现的 Iterator 来产生数据，用于封装在 DALI 中使用，主要的参数就是<code>num_outputs</code>这个了，用于表明这个 Iterator 有几个输出。</p>
<h3 id="iterator">Iterator</h3>
<p>上面提到，Reader作为数据来源，是 Graph 定义的第一个步骤；然后就是其他操作的定义以及 Graph 的编译；第三个步骤是可选的，主要是用一个新的 Iterator 进行封装，然后迭代产生<strong>特定于DL框架</strong>的数据格式，比如 MXNet 的 NDArray、Torch的Tensor等。以MXNet为例进行说明，DALI提供了两个主要的类，一个用于生成简单分类任务的数据，输出只有两个变量：Image / Label；另一个是更加通用的迭代器，可以生成多个多个输出。</p>
<ul>
<li>
<p>nvidia.dali.plugin.mxnet.DALIClassificationIterator()</p>
<p>该类用于分类任务，只输出两个变量，分别是 data 与 label，类型是MXNet中的 DataBatch of NDArrays。</p>
</li>
<li>
<p>nvidia.dali.plugin.mxnet.DALIGenericIterator()</p>
<p>更加通用的 Iterator，可以输出任意数量的MXNet&rsquo;s DataBatch of NDArrays格式的数据。</p>
</li>
</ul>
<p>注意，这两种 Iterator 返回的 DataBatch 数据的所有权仍然属于DALI，并且只在当前的 Iteration 里有效，如果想在其他 Iteration 中使用，需要将它拷贝到其他 NDArray 里保存才行。</p>
<p>至于针对 Pytorch 提供的 Iterator 也是这两个，功能与MXNet类似。</p>
<h3 id="其他一些操作">其他一些操作</h3>
<p>其他的包括数据解码函数：</p>
<ul>
<li>nvidia.dali.fn.decoders.audio()</li>
<li>nvidia.dali.fn.decoders.image()</li>
<li>nvidia.dali.fn.decoders.image_crop()</li>
<li>nvidia.dali.fn.decoders.image_random_crop()，比先使用<code>image()</code>函数进行解码然后使用<code>crop()</code>的方式会更高效一些，即使用<code>libjpect-turbo / nvJPEG</code>等库提供的 ROI 解码函数，也就是只解码特定区域的图像数据</li>
<li>nvidia.dali.fn.decoders.image_slice()</li>
<li></li>
</ul>
<p>生成随机数的函数：</p>
<ul>
<li>nvidia.dali.fn.random.coin_flip()</li>
<li>nvidia.dali.fn.random.normal()</li>
<li>nvidia.dali.fn.random.uniform()</li>
</ul>
<p>数据简单变换函数：</p>
<ul>
<li>nvidia.dali.fn.transforms.combine()</li>
<li>nvidia.dali.fn.transforms.crop()</li>
<li>nvidia.dali.fn.transforms.rotation(0)</li>
<li>nvidia.dali.fn.transforms.scale()</li>
<li>nvidia.dali.fn.transforms.shear()</li>
<li>nvidia.dali.fn.transforms.translation()</li>
</ul>
<h2 id="支持的图像增广计算">支持的图像增广计算</h2>
<p>DALI提供了也还算多的图像增广系列函数，包括：</p>
<ul>
<li>对比度调整</li>
<li>颜色空间转换</li>
<li>HSV，通过设置参数可以实现随机灰度化的功能</li>
<li>插值算法</li>
<li>Resize操作</li>
<li>Warp Affine</li>
<li>3D Transforms操作等</li>
</ul>
<p>其他通用的操作提供了<code>fn.normalize()</code>、<code>nvidia.dali.fn.crop_mirror_normalize()</code>等函数来实现Normalize。像后者一样，DALI还提供一些融合多个功能的Operator，这个函数实现的是随机裁剪、翻转、Normalize，而且这个函数支持改变输出数据的Layout，比如输入是 HWC，可以指定输出的Layout格式是CHW，如果只想实现Layout的转换，则可以将Crop / Mirror / Normalize对应的参数设置为不起作用即可，至于如何设置可以参考API文档，保持默认值即可。</p>
<p>可以发现，DALI目前还没有RandAug / AutoAug / CutMix / Mixup 等复杂操作的实现的，文档里也提供了相应的自己实现新 Op 的说明，可以说还是非常的yin性化的。</p>
<h2 id="一些具体的例子">一些具体的例子</h2>
<p>这个例子以实现 Torch 框架下模型使用 RecordIO / TFRecord 等源数据文件格式进行训练的方式进行说明。</p>
<p>首先是定义数据预处理的 Graph，这部分可以分为两部分，第一部分是针对两种文件格式的数据加载函数、另一部分是对加载的数据进行处理的部分，后者可以公用。</p>
<p>公用的数据处理方式：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">common_pipeline</span>(jpegs, labels):
    images <span style="color:#000;font-weight:bold">=</span> fn<span style="color:#000;font-weight:bold">.</span>decoders<span style="color:#000;font-weight:bold">.</span>image(jpegs, device<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;mixed&#39;</span>)
    images <span style="color:#000;font-weight:bold">=</span> fn<span style="color:#000;font-weight:bold">.</span>resize(
        images,
        resize_shorter<span style="color:#000;font-weight:bold">=</span>fn<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>uniform(<span style="color:#0086b3">range</span><span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">256</span>, <span style="color:#099">480</span>)),
        interp_type<span style="color:#000;font-weight:bold">=</span>types<span style="color:#000;font-weight:bold">.</span>INTERP_LINEAR)
    images <span style="color:#000;font-weight:bold">=</span> fn<span style="color:#000;font-weight:bold">.</span>crop_mirror_normalize(
        images,
        crop_pos_x<span style="color:#000;font-weight:bold">=</span>fn<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>uniform(<span style="color:#0086b3">range</span><span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">0.0</span>, <span style="color:#099">1.0</span>)),
        crop_pos_y<span style="color:#000;font-weight:bold">=</span>fn<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>uniform(<span style="color:#0086b3">range</span><span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">0.0</span>, <span style="color:#099">1.0</span>)),
        dtype<span style="color:#000;font-weight:bold">=</span>types<span style="color:#000;font-weight:bold">.</span>FLOAT,
        crop<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">227</span>, <span style="color:#099">227</span>),
        mean<span style="color:#000;font-weight:bold">=</span>[<span style="color:#099">128.</span>, <span style="color:#099">128.</span>, <span style="color:#099">128.</span>],
        std<span style="color:#000;font-weight:bold">=</span>[<span style="color:#099">1.</span>, <span style="color:#099">1.</span>, <span style="color:#099">1.</span>])
    <span style="color:#000;font-weight:bold">return</span> images, labels
</code></pre></div><p>读取RecordIO数据：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#3c5d5d;font-weight:bold">@pipeline_def</span>
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">mxnet_reader_pipeline</span>(num_gpus):
    jpegs, labels <span style="color:#000;font-weight:bold">=</span> fn<span style="color:#000;font-weight:bold">.</span>readers<span style="color:#000;font-weight:bold">.</span>mxnet(
        path<span style="color:#000;font-weight:bold">=</span>[db_folder<span style="color:#000;font-weight:bold">+</span><span style="color:#d14">&#34;train.rec&#34;</span>],
        index_path<span style="color:#000;font-weight:bold">=</span>[db_folder<span style="color:#000;font-weight:bold">+</span><span style="color:#d14">&#34;train.idx&#34;</span>],
        random_shuffle<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>,
        shard_id<span style="color:#000;font-weight:bold">=</span>Pipeline<span style="color:#000;font-weight:bold">.</span>current()<span style="color:#000;font-weight:bold">.</span>device_id,
        num_shards<span style="color:#000;font-weight:bold">=</span>num_gpus,
        name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;Reader&#39;</span>)

    <span style="color:#000;font-weight:bold">return</span> common_pipeline(jpegs, labels)
</code></pre></div><p>读取TFrecord数据：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">nvidia.dali.tfrecord</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">tfrec</span>

<span style="color:#3c5d5d;font-weight:bold">@pipeline_def</span>
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">tfrecord_reader_pipeline</span>(num_gpus):
    inputs <span style="color:#000;font-weight:bold">=</span> fn<span style="color:#000;font-weight:bold">.</span>readers<span style="color:#000;font-weight:bold">.</span>tfrecord(
        path <span style="color:#000;font-weight:bold">=</span> tfrecord,
        index_path <span style="color:#000;font-weight:bold">=</span> tfrecord_idx,
        features <span style="color:#000;font-weight:bold">=</span> {
            <span style="color:#d14">&#34;image/encoded&#34;</span> : tfrec<span style="color:#000;font-weight:bold">.</span>FixedLenFeature((), tfrec<span style="color:#000;font-weight:bold">.</span>string, <span style="color:#d14">&#34;&#34;</span>),
            <span style="color:#d14">&#34;image/class/label&#34;</span>: tfrec<span style="color:#000;font-weight:bold">.</span>FixedLenFeature([<span style="color:#099">1</span>], tfrec<span style="color:#000;font-weight:bold">.</span>int64,  <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)},
        random_shuffle<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>,
        shard_id<span style="color:#000;font-weight:bold">=</span>Pipeline<span style="color:#000;font-weight:bold">.</span>current()<span style="color:#000;font-weight:bold">.</span>device_id,
        num_shards<span style="color:#000;font-weight:bold">=</span>num_gpus,
        name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;Reader&#39;</span>)

    <span style="color:#000;font-weight:bold">return</span> common_pipeline(inputs[<span style="color:#d14">&#34;image/encoded&#34;</span>], inputs[<span style="color:#d14">&#34;image/class/label&#34;</span>])
</code></pre></div><p>最后是数据迭代：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">numpy</span> <span style="color:#000;font-weight:bold">as</span> <span style="color:#555">np</span>
<span style="color:#000;font-weight:bold">from</span> <span style="color:#555">nvidia.dali.plugin.pytorch</span> <span style="color:#000;font-weight:bold">import</span> DALIGenericIterator


pipe_types <span style="color:#000;font-weight:bold">=</span> [
    [mxnet_reader_pipeline, (<span style="color:#099">0</span>, <span style="color:#099">999</span>)],
    [tfrecord_reader_pipeline, (<span style="color:#099">1</span>, <span style="color:#099">1000</span>)]]

<span style="color:#000;font-weight:bold">for</span> pipe_t <span style="color:#000;font-weight:bold">in</span> pipe_types:
    pipe_name, label_range <span style="color:#000;font-weight:bold">=</span> pipe_t
    <span style="color:#000;font-weight:bold">print</span> (<span style="color:#d14">&#34;RUN: &#34;</span>  <span style="color:#000;font-weight:bold">+</span> pipe_name<span style="color:#000;font-weight:bold">.</span>__name__)
    pipes <span style="color:#000;font-weight:bold">=</span> [pipe_name(
        batch_size<span style="color:#000;font-weight:bold">=</span>BATCH_SIZE, num_threads<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>, device_id<span style="color:#000;font-weight:bold">=</span>device_id, num_gpus<span style="color:#000;font-weight:bold">=</span>N) <span style="color:#000;font-weight:bold">for</span> device_id <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(N)]
    dali_iter <span style="color:#000;font-weight:bold">=</span> DALIGenericIterator(pipes, [<span style="color:#d14">&#39;data&#39;</span>, <span style="color:#d14">&#39;label&#39;</span>], reader_name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;Reader&#39;</span>)

    <span style="color:#000;font-weight:bold">for</span> i, data <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(dali_iter):
        <span style="color:#998;font-style:italic"># Testing correctness of labels</span>
        <span style="color:#000;font-weight:bold">for</span> d <span style="color:#000;font-weight:bold">in</span> data:
            label <span style="color:#000;font-weight:bold">=</span> d[<span style="color:#d14">&#34;label&#34;</span>]
            image <span style="color:#000;font-weight:bold">=</span> d[<span style="color:#d14">&#34;data&#34;</span>]
            <span style="color:#998;font-style:italic">## labels need to be integers</span>
            <span style="color:#000;font-weight:bold">assert</span>(np<span style="color:#000;font-weight:bold">.</span>equal(np<span style="color:#000;font-weight:bold">.</span>mod(label, <span style="color:#099">1</span>), <span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>all())
            <span style="color:#998;font-style:italic">## labels need to be in range pipe_name[2]</span>
            <span style="color:#000;font-weight:bold">assert</span>((label <span style="color:#000;font-weight:bold">&gt;=</span> label_range[<span style="color:#099">0</span>])<span style="color:#000;font-weight:bold">.</span>all())
            <span style="color:#000;font-weight:bold">assert</span>((label <span style="color:#000;font-weight:bold">&lt;=</span> label_range[<span style="color:#099">1</span>])<span style="color:#000;font-weight:bold">.</span>all())
    <span style="color:#000;font-weight:bold">print</span>(<span style="color:#d14">&#34;OK : &#34;</span> <span style="color:#000;font-weight:bold">+</span> pipe_name<span style="color:#000;font-weight:bold">.</span>__name__)
</code></pre></div><p>至于更多的细节可以参考DALI的官方文档。</p>
        
            <p>这是接着上一篇掩码生成方式写的，主要仅包含SpanBERT &amp; MacBERT的原理与实现。</p>
<h2 id="spanbert">SpanBERT</h2>
<p>SpanBERT的官方实现代码比想象中的复杂，并且还没有比较好的说明文档，所以这里单独作为一篇笔记详细分析一下。SpanBERT这篇论文主要的创新点有三个，（1）引入了新的掩膜方式，即随机掩膜掉连续长度的token，（2）引入了掩膜边界预测损失，即Span Boundary Object (SBO)，（3）去掉了BERT中使用的 NSP 任务</p>
<h3 id="spanbert创新点">SpanBERT创新点</h3>
<p>对于第一点，需要确定连续掩膜的起始位置 + 掩膜长度。这里起始位置是uniform随机选择的；掩膜长度是按照$l \sim \textrm{Geo}(p=0.2)$ 来确定的，其中Geo对应的是<a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric Distribution</a>。</p>
<p>Gemoetric Distribution分布可以从两个角度理解，试验k次，成功的次数，其中p为每次实验成功的概率，或者实验k次，成功前连续有k-1次失败的概率，前者k的取值是$[1, 2, \ldots ]$，后者的取值是$[0, 1, \ldots]$。对应的数学公式如下。</p>
<p>$$\textrm{Geo}(p) = (1-p)^{k-1}p$$</p>
<p>这里 k 也就是掩膜的长度了，也就是对应的掩膜长度对应1、2、3 &hellip; 等的概率了。并且，p越接近于1，概率下降越快，也就更倾向于更短的掩膜；还有一点是，这里的长度值得是 whole word 的个数，而不是wordpiece元素的个数! 与前面WWM的做法不同的是，SpanBERT将位于Span内的要被掩膜的tokens采用同一种掩膜方式，现在掩膜方式有三种：使用Mask、随机Token、保持不变。</p>
<p>SBO 任务实际上是用Span边界位置未被掩膜掉的两个 token 预测被掩膜掉的 token 的分类任务，与 MLM 相互补充，只不过 MLM 用的是所有 token 的信息，SBO 只用了边界位置 token 的信息。比如输入的 token 序列是$x_1, \ldots, x_n$，然后$x_s, x_e$表示掩膜span的边界，则SBO使用$x_{s-1}, x_{e+1}$两个token配合在相对位置$i$上的信息来预测具体的 token $y_i$，如下式所示。</p>
<p>$$y_i = f(e_{s-1}, e_{e+1}, p_{i-s+1})$$</p>
<p>$p$为相对位置编码，维度是200，对应的函数$f$是一个两层的前向网络，激活函数为 GELU，上述三个输入数据拼接起来后送入到这个前向网络，伪代码如下。</p>
<p>$$h_0 = [ x_{s-1}; x_{e+1}; p_{i-s+1} ]$$</p>
<p>$$h_1 = \textrm{LayerNorm}(\textrm{GeLU}(W_1h_0))$$</p>
<p>$$y_i = \textrm{LayerNorm}(\textrm{GeLU}(W_2h_1))$$</p>
<p>然后SpanBERT对应的预训练任务用公式表示如下。</p>
<p>$$\mathcal{L}_(x _i) = \mathcal{L} _{MLM}(x _i) + \mathcal{L} _{SBO}(x _i) = -\log P(x _i | \mathbf{x _i}) - \log P(x _i | \mathbf{y _i})$$</p>
<p>其中，$x_i$为预测的 token 的索引，$\mathbf{x_i}$ 为模型输出的对应位置的特征向量，$\mathbf{y_i}$为上述通过 SBO 计算出来的特征向量。</p>
<p>去掉 NSP 任务是指，只用一个句子来计算上述的SBO任务，作者主要发现BERT用到的NSP任务中构造负样本的方式（其它doc的句子作为负样本）噪声太大，会妨碍模型的学习。</p>
<p>总结上文，SpanBERT论文里起始用到的两个掩膜Loss，一个是普通的 MLM，一个是 SBO，他们的关系以及计算方式如下图所示。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/span0.png" alt="图 - 1 SpanBERT计算MLM &amp;amp; SBO损失函数示意图">
    <figcaption>图 - 1 SpanBERT计算MLM &amp;amp; SBO损失函数示意图</figcaption>
    </center>
</figure></p>
<h3 id="spanbert代码实现">SpanBERT代码实现</h3>
<p>要分析SpanBERT的实现，代码部分主要就是三个部分：数据加载、模型计算、Loss计算，至于transformers &amp; fairseq 等框架问题都比较清洗，花点时间理解一下即可。</p>
<p>数据加载部分的重点在于掩码的生成，经过<code>spanbert.py::SpanBertTask.load_dataset()</code>函数内调用<code>indexed_dataset.py::IndexedRawTextDataset</code>以及<code>BlockDataset</code>等Dataset类，最终在<code>NoNSPSpanBertDataset</code>类内完成掩膜的生成以及对应label的计算等，这里采用的掩膜方式为<code>PairWithSpanMaskingScheme</code>类。下面是主要的<code>mask()</code>函数的代码。</p>
<p>送入到损失函数计算的数据生成代码是：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> __getitem__(<span style="color:#999">self</span>, index):
        <span style="color:#000;font-weight:bold">with</span> data_utils<span style="color:#000;font-weight:bold">.</span>numpy_seed(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>seed <span style="color:#000;font-weight:bold">+</span> index):
            block <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dataset[index]

        <span style="color:#998;font-style:italic"># tagmap -&gt; default None</span>
        tagmap <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dataset<span style="color:#000;font-weight:bold">.</span>tag_map[block[<span style="color:#099">0</span>]:block[<span style="color:#099">1</span>]] <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dataset<span style="color:#000;font-weight:bold">.</span>tag_map <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span> <span style="color:#000;font-weight:bold">else</span> <span style="color:#999">None</span>
        masked_block, masked_tgt, pair_targets <span style="color:#000;font-weight:bold">=</span> \
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_mask_block(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dataset<span style="color:#000;font-weight:bold">.</span>tokens[block[<span style="color:#099">0</span>]:block[<span style="color:#099">1</span>]], tagmap)        <span style="color:#998;font-style:italic">#   进行掩码!</span>

        item <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate(
            [
                [<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>vocab<span style="color:#000;font-weight:bold">.</span>cls()],
                masked_block,
                [<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>vocab<span style="color:#000;font-weight:bold">.</span>sep()],
            ]
        )
        target <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate([[<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>vocab<span style="color:#000;font-weight:bold">.</span>pad()], masked_tgt, [<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>vocab<span style="color:#000;font-weight:bold">.</span>pad()]])
        seg <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>zeros(block[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> block[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">2</span>)
        <span style="color:#000;font-weight:bold">if</span> pair_targets <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span> <span style="color:#000;font-weight:bold">and</span>  <span style="color:#0086b3">len</span>(pair_targets) <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
            <span style="color:#998;font-style:italic"># dummy = [[0 for i in range(self.args.max_pair_targets + 2)]]</span>
            <span style="color:#998;font-style:italic"># add 1 to the first two since they are input indices. Rest are targets.</span>
            pair_targets <span style="color:#000;font-weight:bold">=</span> [[(x<span style="color:#000;font-weight:bold">+</span><span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">if</span> i <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">else</span> x <span style="color:#000;font-weight:bold">for</span> i, x <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(pair_tgt)] <span style="color:#000;font-weight:bold">for</span> pair_tgt <span style="color:#000;font-weight:bold">in</span> pair_targets]
            <span style="color:#998;font-style:italic"># pair_targets = dummy + pair_targets</span>
            pair_targets <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>from_numpy(np<span style="color:#000;font-weight:bold">.</span>array(pair_targets))<span style="color:#000;font-weight:bold">.</span>long()
        <span style="color:#000;font-weight:bold">else</span>:
            pair_targets <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>max_pair_targets <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">2</span>), dtype<span style="color:#000;font-weight:bold">=</span>torch<span style="color:#000;font-weight:bold">.</span>long)
        <span style="color:#000;font-weight:bold">return</span> {
            <span style="color:#d14">&#39;id&#39;</span>: index,
            <span style="color:#d14">&#39;source&#39;</span>: torch<span style="color:#000;font-weight:bold">.</span>from_numpy(item)<span style="color:#000;font-weight:bold">.</span>long(),
            <span style="color:#d14">&#39;segment_labels&#39;</span>: torch<span style="color:#000;font-weight:bold">.</span>from_numpy(seg)<span style="color:#000;font-weight:bold">.</span>long(),
            <span style="color:#998;font-style:italic">## 重点是 lm_target &amp; pair_targets 的生成过程以及内容</span>
            <span style="color:#d14">&#39;lm_target&#39;</span>: torch<span style="color:#000;font-weight:bold">.</span>from_numpy(target)<span style="color:#000;font-weight:bold">.</span>long(),
            <span style="color:#d14">&#39;pair_targets&#39;</span>: pair_targets,
        }
</code></pre></div><p>可见，返回的就是 sample 的index，当前句子对应token的索引，还有就是segment type ids，前两者是BERT用到的数据，后面的就是MLM、SBO任务对应的标签数据了，分别是lm target 以及 pair targets。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">mask</span>(<span style="color:#999">self</span>, sentence, tagmap<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>):
        sent_length <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">len</span>(sentence)
        mask_num <span style="color:#000;font-weight:bold">=</span> math<span style="color:#000;font-weight:bold">.</span>ceil(sent_length <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mask_ratio)
        mask <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">set</span>()
        word_piece_map <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>paragraph_info<span style="color:#000;font-weight:bold">.</span>get_word_piece_map(sentence)
        spans <span style="color:#000;font-weight:bold">=</span> []
        <span style="color:#000;font-weight:bold">while</span> <span style="color:#0086b3">len</span>(mask) <span style="color:#000;font-weight:bold">&lt;</span> mask_num:
            span_len <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>choice(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>lens, p<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>len_distrib)
            tagged_indices <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
            <span style="color:#000;font-weight:bold">if</span> tagmap <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
                tagged_indices <span style="color:#000;font-weight:bold">=</span> [<span style="color:#0086b3">max</span>(<span style="color:#099">0</span>, i <span style="color:#000;font-weight:bold">-</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>randint(<span style="color:#099">0</span>, span_len)) <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(tagmap<span style="color:#000;font-weight:bold">.</span>length()) <span style="color:#000;font-weight:bold">if</span> tagmap[i]]
                tagged_indices <span style="color:#000;font-weight:bold">+=</span> [np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>choice(sent_length)] <span style="color:#000;font-weight:bold">*</span> <span style="color:#0086b3">int</span>(<span style="color:#0086b3">len</span>(tagged_indices) <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>)
            anchor  <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>choice(sent_length) <span style="color:#000;font-weight:bold">if</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>rand() <span style="color:#000;font-weight:bold">&gt;=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>tagged_anchor_prob <span style="color:#000;font-weight:bold">else</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>choice(tagged_indices)
            <span style="color:#000;font-weight:bold">if</span> anchor <span style="color:#000;font-weight:bold">in</span> mask:
                <span style="color:#000;font-weight:bold">continue</span>
            <span style="color:#998;font-style:italic"># find word start, end</span>
            left1, right1 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>paragraph_info<span style="color:#000;font-weight:bold">.</span>get_word_start(sentence, anchor, word_piece_map), <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>paragraph_info<span style="color:#000;font-weight:bold">.</span>get_word_end(sentence, anchor, word_piece_map)
            spans<span style="color:#000;font-weight:bold">.</span>append([left1, left1])
            <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(left1, right1):
                <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">len</span>(mask) <span style="color:#000;font-weight:bold">&gt;=</span> mask_num:
                    <span style="color:#000;font-weight:bold">break</span>
                mask<span style="color:#000;font-weight:bold">.</span>add(i)
                spans[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>][<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">=</span> i
            num_words <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>
            right2 <span style="color:#000;font-weight:bold">=</span> right1
            <span style="color:#000;font-weight:bold">while</span> num_words <span style="color:#000;font-weight:bold">&lt;</span> span_len <span style="color:#000;font-weight:bold">and</span> right2 <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#0086b3">len</span>(sentence) <span style="color:#000;font-weight:bold">and</span> <span style="color:#0086b3">len</span>(mask) <span style="color:#000;font-weight:bold">&lt;</span> mask_num:
                <span style="color:#998;font-style:italic"># complete current word</span>
                left2 <span style="color:#000;font-weight:bold">=</span> right2
                right2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>paragraph_info<span style="color:#000;font-weight:bold">.</span>get_word_end(sentence, right2, word_piece_map)
                num_words <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
                <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(left2, right2):
                    <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">len</span>(mask) <span style="color:#000;font-weight:bold">&gt;=</span> mask_num:
                        <span style="color:#000;font-weight:bold">break</span>
                    mask<span style="color:#000;font-weight:bold">.</span>add(i)
                    spans[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>][<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">=</span> i
        sentence, target, pair_targets <span style="color:#000;font-weight:bold">=</span> span_masking(sentence, spans, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>tokens, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pad, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mask_id, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>max_pair_targets, mask, replacement<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>replacement_method, endpoints<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>endpoints)
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>return_only_spans:
            pair_targets <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
        <span style="color:#000;font-weight:bold">return</span> sentence, target, pair_targets
</code></pre></div><p>变量<code>sentence</code>里面包含的就是所有的token元素，<code>get_word_piece_map()</code>函数将输入的对应位置tokens判断是否是词首还是非词首。外面的 <code>while</code> 循环主要就是生成 span 范围，这里每次生成 span 范围作为一个<code>[left, right]</code>保存到 span 变量里面。生成所有的 span 信息之后，就作为参数传入到 <code>span_masking()</code>函数里了，也是一个非常重要的函数。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">span_masking</span>(sentence, spans, tokens, pad, mask_id, pad_len, mask, replacement<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;word_piece&#39;</span>, endpoints<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;external&#39;</span>):
    sentence <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>copy(sentence)
    sent_length <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">len</span>(sentence)
    target <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>full(sent_length, pad)
    pair_targets <span style="color:#000;font-weight:bold">=</span> []
    spans <span style="color:#000;font-weight:bold">=</span> merge_intervals(spans)
    <span style="color:#000;font-weight:bold">assert</span> <span style="color:#0086b3">len</span>(mask) <span style="color:#000;font-weight:bold">==</span> <span style="color:#0086b3">sum</span>([e <span style="color:#000;font-weight:bold">-</span> s <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">for</span> s,e <span style="color:#000;font-weight:bold">in</span> spans])
    <span style="color:#998;font-style:italic"># print(list(enumerate(sentence)))</span>
    <span style="color:#000;font-weight:bold">for</span> start, end <span style="color:#000;font-weight:bold">in</span> spans:
        <span style="color:#998;font-style:italic"># endpoints = `external`</span>
        lower_limit <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span> <span style="color:#000;font-weight:bold">if</span> endpoints <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#39;external&#39;</span> <span style="color:#000;font-weight:bold">else</span> <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>
        upper_limit <span style="color:#000;font-weight:bold">=</span> sent_length <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">if</span> endpoints <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#39;external&#39;</span> <span style="color:#000;font-weight:bold">else</span> sent_length
        <span style="color:#000;font-weight:bold">if</span> start <span style="color:#000;font-weight:bold">&gt;</span> lower_limit <span style="color:#000;font-weight:bold">and</span> end <span style="color:#000;font-weight:bold">&lt;</span> upper_limit:
            <span style="color:#000;font-weight:bold">if</span> endpoints <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#39;external&#39;</span>:
                pair_targets <span style="color:#000;font-weight:bold">+=</span> [[start <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>, end <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>]]
            <span style="color:#000;font-weight:bold">else</span>:
                pair_targets <span style="color:#000;font-weight:bold">+=</span> [[start, end]]
            <span style="color:#998;font-style:italic"># pair_targets[-1]元素的结构是: [s-1, e+1, x_s, x_{s+1} ... x_{e}]，元素个数是 2 + （e - s)</span>
            pair_targets[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">+=</span> [sentence[i] <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(start, end <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>)]
        rand <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>random()       <span style="color:#998;font-style:italic"># 整个 span 只用一种替换方式，比如 mask 或者随机其它 token 或者 全保持不变</span>
        <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(start, end <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>):
            <span style="color:#000;font-weight:bold">assert</span> i <span style="color:#000;font-weight:bold">in</span> mask
            target[i] <span style="color:#000;font-weight:bold">=</span> sentence[i]
            <span style="color:#000;font-weight:bold">if</span> replacement <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#39;word_piece&#39;</span>:
                rand <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>random()
            <span style="color:#000;font-weight:bold">if</span> rand <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#099">0.8</span>:
                sentence[i] <span style="color:#000;font-weight:bold">=</span> mask_id
            <span style="color:#000;font-weight:bold">elif</span> rand <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#099">0.9</span>:
                <span style="color:#998;font-style:italic"># sample random token according to input distribution</span>
                sentence[i] <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>choice(tokens)
    <span style="color:#998;font-style:italic"># pair_targets 的维度是：(pair nums, 2 + (e - s))</span>
    <span style="color:#998;font-style:italic"># + 2 表示的是 s - 1, e + 1 这两个位置信息</span>
    pair_targets <span style="color:#000;font-weight:bold">=</span> pad_to_len(pair_targets, pad, pad_len <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">2</span>)
    <span style="color:#998;font-style:italic"># if pair_targets is None:</span>
    <span style="color:#000;font-weight:bold">return</span> sentence, target, pair_targets
</code></pre></div><p>这个函数，首先调用<code>merge_intervals()</code>函数合并那些有重叠的span区域，可以参考源代码，比较简单。<code>mask</code>参数保存的是那些位于 span 掩膜下面的位置信息。然后<code>for</code>循环里面可以分为两个部分，上面一部分生成SBO的标签，下面的<code>for</code>循环生成MLM的标签，这里<code>endpoints</code>的参数是<code>external</code>，对应的上面原理部分的$x_{s-1}, x_{e+1}$两个 token 对应的位置。SBO标签信息保存在<code>pair_targets</code>里面，这是一个二维list，里面的每个 list 表示一个 span 范围，并且元素的结构是<code>[s-1, e+1, x_{s}, x_{s+1} ... x_{e}]</code>，这里<code>x</code>表示真是的token字符。然后第二部分就体现了论文中提到的只用一个掩膜方式进行掩码，也就是有一个全局的<code>rand</code>参数存在，<code>for</code>循环里面就是正常的 MLM 标签生成过程了。注意送入<code>pad_to_len</code>中最大长度 + 2 了，<code>pad_len</code>对应模型实现部分的<code>max_targets</code>参数。</p>
<p>最后的<code>pad_to_len()</code>函数将<code>pair_targets</code>参数扩展到<code>pad_len + 2</code>的形式，产生的结果会直接用于Loss的计算，所以这里给出具体实现，函数如下。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">pad_to_len</span>(pair_targets, pad, max_pair_target_len):
    <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#0086b3">len</span>(pair_targets)):
        pair_targets[i] <span style="color:#000;font-weight:bold">=</span> pair_targets[i][:max_pair_target_len]
        this_len <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">len</span>(pair_targets[i])
        <span style="color:#998;font-style:italic"># 补全</span>
        <span style="color:#000;font-weight:bold">for</span> j <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(max_pair_target_len <span style="color:#000;font-weight:bold">-</span> this_len):
            pair_targets[i]<span style="color:#000;font-weight:bold">.</span>append(pad)
    <span style="color:#998;font-style:italic"># 返回的数据尺寸是（pair nums, max_pair_target_len）</span>
    <span style="color:#000;font-weight:bold">return</span> pair_targets
</code></pre></div><p>这个函数做的事情就是截断 &amp; 补全，返回的数据尺寸见注释。至此，就分析完了数据加载过程，送给模型输入的数据就是上面<code>__getitem__</code>函数的返回结果了。那么怎么组成一个 batch 数据呢，毕竟每个sample可能的mask长度以及 span pair 的个数不一定相同。具体的<code>collector</code>函数实现如下。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">_collate</span>(<span style="color:#999">self</span>, samples, pad_idx):
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">len</span>(samples) <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:
            <span style="color:#000;font-weight:bold">return</span> {}

        <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">merge</span>(key):
            <span style="color:#000;font-weight:bold">return</span> data_utils<span style="color:#000;font-weight:bold">.</span>collate_tokens(
                [s[key] <span style="color:#000;font-weight:bold">for</span> s <span style="color:#000;font-weight:bold">in</span> samples], pad_idx, left_pad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>,
            )
        <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">merge_2d</span>(key):
            <span style="color:#000;font-weight:bold">return</span> data_utils<span style="color:#000;font-weight:bold">.</span>collate_2d(
                [s[key] <span style="color:#000;font-weight:bold">for</span> s <span style="color:#000;font-weight:bold">in</span> samples], pad_idx, left_pad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>,
            )
        pair_targets <span style="color:#000;font-weight:bold">=</span> merge_2d(<span style="color:#d14">&#39;pair_targets&#39;</span>)

        <span style="color:#000;font-weight:bold">return</span> {
            <span style="color:#d14">&#39;id&#39;</span>: torch<span style="color:#000;font-weight:bold">.</span>LongTensor([s[<span style="color:#d14">&#39;id&#39;</span>] <span style="color:#000;font-weight:bold">for</span> s <span style="color:#000;font-weight:bold">in</span> samples]),
            <span style="color:#d14">&#39;ntokens&#39;</span>: <span style="color:#0086b3">sum</span>(<span style="color:#0086b3">len</span>(s[<span style="color:#d14">&#39;source&#39;</span>]) <span style="color:#000;font-weight:bold">for</span> s <span style="color:#000;font-weight:bold">in</span> samples),
            <span style="color:#d14">&#39;net_input&#39;</span>: {
                <span style="color:#d14">&#39;src_tokens&#39;</span>: merge(<span style="color:#d14">&#39;source&#39;</span>),
                <span style="color:#d14">&#39;segment_labels&#39;</span>: merge(<span style="color:#d14">&#39;segment_labels&#39;</span>),
                <span style="color:#d14">&#39;pairs&#39;</span>: pair_targets[:, :, :<span style="color:#099">2</span>]
            },
            <span style="color:#d14">&#39;lm_target&#39;</span>: merge(<span style="color:#d14">&#39;lm_target&#39;</span>),
            <span style="color:#d14">&#39;nsentences&#39;</span>: samples[<span style="color:#099">0</span>][<span style="color:#d14">&#39;source&#39;</span>]<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>),
            <span style="color:#d14">&#39;pair_targets&#39;</span>: pair_targets[:, :, <span style="color:#099">2</span>:]      <span style="color:#998;font-style:italic"># (pair nums, max pair target len)</span>
        }
</code></pre></div><p>以及对应的<code>merge_2d()</code>函数，这个函数的实现如下。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">merge_2d</span>(key):
            <span style="color:#000;font-weight:bold">return</span> data_utils<span style="color:#000;font-weight:bold">.</span>collate_2d(
                [s[key] <span style="color:#000;font-weight:bold">for</span> s <span style="color:#000;font-weight:bold">in</span> samples], pad_idx, left_pad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>,
            )
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">collate_2d</span>(values, pad_idx, left_pad, move_eos_to_beginning<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>):
    <span style="color:#d14">&#34;&#34;&#34;Convert a list of 1d tensors into a padded 2d tensor.&#34;&#34;&#34;</span>
    size_0 <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">max</span>(v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>) <span style="color:#000;font-weight:bold">for</span> v <span style="color:#000;font-weight:bold">in</span> values)
    size_1 <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">max</span>(v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">for</span> v <span style="color:#000;font-weight:bold">in</span> values)
    res <span style="color:#000;font-weight:bold">=</span> values[<span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">.</span>new(<span style="color:#0086b3">len</span>(values), size_0, size_1)<span style="color:#000;font-weight:bold">.</span>fill_(pad_idx)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">copy_tensor</span>(src, dst):
        <span style="color:#000;font-weight:bold">assert</span> dst<span style="color:#000;font-weight:bold">.</span>numel() <span style="color:#000;font-weight:bold">==</span> src<span style="color:#000;font-weight:bold">.</span>numel()
        <span style="color:#000;font-weight:bold">if</span> move_eos_to_beginning:
            <span style="color:#000;font-weight:bold">assert</span> src[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">==</span> eos_idx
            dst[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">=</span> eos_idx
            dst[<span style="color:#099">1</span>:] <span style="color:#000;font-weight:bold">=</span> src[:<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]
        <span style="color:#000;font-weight:bold">else</span>:
            dst<span style="color:#000;font-weight:bold">.</span>copy_(src)

    <span style="color:#000;font-weight:bold">for</span> i, v <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(values):
        copy_tensor(v, res[i, size_0 <span style="color:#000;font-weight:bold">-</span> v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>):, size_1 <span style="color:#000;font-weight:bold">-</span> v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">1</span>):] <span style="color:#000;font-weight:bold">if</span> left_pad <span style="color:#000;font-weight:bold">else</span> res[i, :v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), :v<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">1</span>)])
    <span style="color:#000;font-weight:bold">return</span> res

</code></pre></div><p>可以看出，这里将<code>pair_targets</code>进行了分离，分别用于模型前向 &amp; 损失计算。</p>
<p>然后再看模型结构。模型构成主要分为两部分，一个是底层的由正常 transformer 层构成的backbone，然后另一个就是在其上的 Head 部分，这里主要就是SBO任务对应的Head 的实现，这部分实现在<code>BertPairTargetPredictionHead</code>类中。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">BertPairTargetPredictionHead</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, config, bert_model_embedding_weights, max_targets<span style="color:#000;font-weight:bold">=</span><span style="color:#099">20</span>, position_embedding_size<span style="color:#000;font-weight:bold">=</span><span style="color:#099">200</span>):
        <span style="color:#0086b3">super</span>(BertPairTargetPredictionHead, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>position_embeddings <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(max_targets, position_embedding_size)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mlp_layer_norm <span style="color:#000;font-weight:bold">=</span> MLPWithLayerNorm(config, config<span style="color:#000;font-weight:bold">.</span>hidden_size <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">+</span> position_embedding_size)
        <span style="color:#998;font-style:italic"># The output weights are the same as the input embeddings, but there is</span>
        <span style="color:#998;font-style:italic"># an output-only bias for each token.</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Linear(bert_model_embedding_weights<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">1</span>),      <span style="color:#998;font-style:italic"># hidden size</span>
                                 bert_model_embedding_weights<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>),      <span style="color:#998;font-style:italic"># vocab size</span>
                                 bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>weight <span style="color:#000;font-weight:bold">=</span> bert_model_embedding_weights
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bias <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Parameter(torch<span style="color:#000;font-weight:bold">.</span>zeros(bert_model_embedding_weights<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>)))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>max_targets <span style="color:#000;font-weight:bold">=</span> max_targets

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, hidden_states, pairs):
        <span style="color:#998;font-style:italic">## 整体思路是使用 Span 边界的两个 token，预测被 mask 掉的 token 对应的 word</span>
        bs, num_pairs, _ <span style="color:#000;font-weight:bold">=</span> pairs<span style="color:#000;font-weight:bold">.</span>size()
        bs, seq_len, dim <span style="color:#000;font-weight:bold">=</span> hidden_states<span style="color:#000;font-weight:bold">.</span>size()
        <span style="color:#998;font-style:italic"># pair indices: (bs, num_pairs)</span>
        left, right <span style="color:#000;font-weight:bold">=</span> pairs[:,:, <span style="color:#099">0</span>], pairs[:, :, <span style="color:#099">1</span>]
        <span style="color:#998;font-style:italic"># (bs, num_pairs, dim)</span>
        left_hidden <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>gather(hidden_states, <span style="color:#099">1</span>, left<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)<span style="color:#000;font-weight:bold">.</span>repeat(<span style="color:#099">1</span>, <span style="color:#099">1</span>, dim))
        <span style="color:#998;font-style:italic"># pair states: bs * num_pairs, max_targets, dim</span>
        left_hidden <span style="color:#000;font-weight:bold">=</span> left_hidden<span style="color:#000;font-weight:bold">.</span>contiguous()<span style="color:#000;font-weight:bold">.</span>view(bs <span style="color:#000;font-weight:bold">*</span> num_pairs, dim)<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>repeat(<span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>max_targets, <span style="color:#099">1</span>)
        right_hidden <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>gather(hidden_states, <span style="color:#099">1</span>, right<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)<span style="color:#000;font-weight:bold">.</span>repeat(<span style="color:#099">1</span>, <span style="color:#099">1</span>, dim))
        <span style="color:#998;font-style:italic"># bs * num_pairs, max_targets, dim</span>
        right_hidden <span style="color:#000;font-weight:bold">=</span> right_hidden<span style="color:#000;font-weight:bold">.</span>contiguous()<span style="color:#000;font-weight:bold">.</span>view(bs <span style="color:#000;font-weight:bold">*</span> num_pairs, dim)<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>repeat(<span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>max_targets, <span style="color:#099">1</span>)

        <span style="color:#998;font-style:italic"># (max_targets, dim)</span>
        position_embeddings <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>position_embeddings<span style="color:#000;font-weight:bold">.</span>weight
        hidden_states <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mlp_layer_norm(torch<span style="color:#000;font-weight:bold">.</span>cat((left_hidden, right_hidden, position_embeddings<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>repeat(bs <span style="color:#000;font-weight:bold">*</span> num_pairs, <span style="color:#099">1</span>, <span style="color:#099">1</span>)), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>))
        <span style="color:#998;font-style:italic"># target scores : bs * num_pairs, max_targets, vocab_size</span>
        target_scores <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder(hidden_states) <span style="color:#000;font-weight:bold">+</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bias
        <span style="color:#000;font-weight:bold">return</span> target_scores
</code></pre></div><p>可以看出，主要过程就是将左边边界的token以及右边边界的 token 取出来，与相对位置编码<code>self.position_embedding</code>拼接起来送入前向计算网络。值得注意的是这里返回的数据尺寸是<code>(bs * num_pairs, max targets, vocab size)</code>。</p>
<p>提到的损失函数的实现如下。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#3c5d5d;font-weight:bold">@register_criterion</span>(<span style="color:#d14">&#39;span_bert_loss&#39;</span>)
<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">NoNSPPairLoss</span>(FairseqCriterion):
    <span style="color:#d14">&#34;&#34;&#34;Implementation for loss of SpanBert
</span><span style="color:#d14">        Combine masked language model loss with the SBO loss. 
</span><span style="color:#d14">    &#34;&#34;&#34;</span>

    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, args, task):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__(args, task)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args <span style="color:#000;font-weight:bold">=</span> args
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>aux_loss_weight <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">getattr</span>(args, <span style="color:#d14">&#39;pair_loss_weight&#39;</span>, <span style="color:#099">0</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, model, sample, <span style="color:#0086b3">reduce</span><span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>):
        net_output <span style="color:#000;font-weight:bold">=</span> model(<span style="color:#000;font-weight:bold">**</span>sample[<span style="color:#d14">&#39;net_input&#39;</span>])
        lm_targets <span style="color:#000;font-weight:bold">=</span> sample[<span style="color:#d14">&#39;lm_target&#39;</span>]<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)

        <span style="color:#998;font-style:italic"># mlm loss</span>
        lm_logits <span style="color:#000;font-weight:bold">=</span> net_output[<span style="color:#099">0</span>]
        lm_logits <span style="color:#000;font-weight:bold">=</span> lm_logits<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, lm_logits<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>))
        lm_loss <span style="color:#000;font-weight:bold">=</span> F<span style="color:#000;font-weight:bold">.</span>cross_entropy(
            lm_logits,      <span style="color:#998;font-style:italic"># (bs * seq_len, )</span>
            lm_targets,     <span style="color:#998;font-style:italic"># (bs * seq_len, )</span>
            size_average<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>,
            ignore_index<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>padding_idx,
            <span style="color:#0086b3">reduce</span><span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">reduce</span>
        )

        <span style="color:#998;font-style:italic"># SBO loss</span>
        pair_target_logits <span style="color:#000;font-weight:bold">=</span> net_output[<span style="color:#099">2</span>]
        pair_target_logits <span style="color:#000;font-weight:bold">=</span> pair_target_logits<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, pair_target_logits<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>))
        pair_targets <span style="color:#000;font-weight:bold">=</span> sample[<span style="color:#d14">&#39;pair_targets&#39;</span>]<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)
        pair_loss <span style="color:#000;font-weight:bold">=</span> F<span style="color:#000;font-weight:bold">.</span>cross_entropy(
            pair_target_logits,     <span style="color:#998;font-style:italic"># (bs * pair_nums * max_target, vocab size)</span>
            pair_targets,           <span style="color:#998;font-style:italic"># (bs * pair_nums * max_target, )</span>
            size_average<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>,
            ignore_index<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>padding_idx,
            <span style="color:#0086b3">reduce</span><span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">reduce</span>
        )


        nsentences <span style="color:#000;font-weight:bold">=</span> sample[<span style="color:#d14">&#39;lm_target&#39;</span>]<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>)
        ntokens <span style="color:#000;font-weight:bold">=</span> utils<span style="color:#000;font-weight:bold">.</span>strip_pad(lm_targets, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>padding_idx)<span style="color:#000;font-weight:bold">.</span>numel()
        npairs <span style="color:#000;font-weight:bold">=</span> utils<span style="color:#000;font-weight:bold">.</span>strip_pad(pair_targets, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>padding_idx)<span style="color:#000;font-weight:bold">.</span>numel() <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>

        sample_size <span style="color:#000;font-weight:bold">=</span> nsentences <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>args<span style="color:#000;font-weight:bold">.</span>sentence_avg <span style="color:#000;font-weight:bold">else</span> ntokens
        loss <span style="color:#000;font-weight:bold">=</span> lm_loss <span style="color:#000;font-weight:bold">/</span> ntokens <span style="color:#000;font-weight:bold">+</span> (<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>aux_loss_weight <span style="color:#000;font-weight:bold">*</span> pair_loss <span style="color:#000;font-weight:bold">/</span> npairs)
        logging_output <span style="color:#000;font-weight:bold">=</span> {
            <span style="color:#d14">&#39;loss&#39;</span>: utils<span style="color:#000;font-weight:bold">.</span>item(loss<span style="color:#000;font-weight:bold">.</span>data) <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">reduce</span> <span style="color:#000;font-weight:bold">else</span> loss<span style="color:#000;font-weight:bold">.</span>data,
            <span style="color:#d14">&#39;lm_loss&#39;</span>: utils<span style="color:#000;font-weight:bold">.</span>item(lm_loss<span style="color:#000;font-weight:bold">.</span>data) <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">reduce</span> <span style="color:#000;font-weight:bold">else</span> lm_loss<span style="color:#000;font-weight:bold">.</span>data,
            <span style="color:#d14">&#39;pair_loss&#39;</span>:  utils<span style="color:#000;font-weight:bold">.</span>item(pair_loss<span style="color:#000;font-weight:bold">.</span>data) <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">reduce</span> <span style="color:#000;font-weight:bold">else</span> pair_loss<span style="color:#000;font-weight:bold">.</span>data,
            <span style="color:#d14">&#39;ntokens&#39;</span>: ntokens,
            <span style="color:#d14">&#39;npairs&#39;</span>: npairs,
            <span style="color:#d14">&#39;nsentences&#39;</span>: nsentences,
            <span style="color:#d14">&#39;sample_size&#39;</span>: sample_size,
            <span style="color:#d14">&#39;aux_loss_weight&#39;</span>: <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>aux_loss_weight
        }
        <span style="color:#000;font-weight:bold">return</span> loss, sample_size, logging_output
</code></pre></div><p><code>forward</code>函数里面就是对应的MLM / SBO 两个Loss 的计算过程，两个计算过程没啥区别，都是分类任务预测被掩膜的token的具体是啥word。</p>
<p>需要在整理一下SBO任务的模型输出 &amp; 损失计算。首先模型输出的尺寸是(bs * pair num * max pair target len, vocab size)，对应的label的尺寸是(bs * pair num * max pair target len, )，所以尺寸上没有问题，但是相对位置编码的对应关系该如何呢？这里需要在提醒一下，模型实现中的这个position embedding是<strong>相对位置</strong>信息，不是绝对位置信息，所以<code>BertPairTargetPredictionHead</code>中使用<code>.repeat(1, self.max_targets, 1)</code>没有问题，在与pair target计算损失的时候，预测的数据就是根据相对位置编码预测出来的，然后与对应标签计算分类loss，也就是没有错了！（理解能力真是够了）</p>
<p>至此，就分析完SpanBERT的实现细节了。</p>
<h2 id="macbert">MacBERT</h2>
<p>与WWM类似，该作者又提出了 Mac (MLM As Correction) 方式的掩码生成过程。实现细节有以下三点。</p>
<ul>
<li>引入类似 SpanBERT 的 N-Gram 掩膜，N 由 1 - 4 的概率分别为 40%, 30%, 20%, 10%，注意N这个长度是指词组的个数，而不是字的个数，相当于在 WWM 基础上配合 Span 来实现(当N=1时即为WWM)，起始由上面 SpanBERT 的实现可以看出来，SpanBERT 就是英文版的 N-Gram Masking 了</li>
<li>使用<a href="https://github.com/chatopera/Synonyms">Synonyms</a>来得到相似字来作为掩码，但如果没有找到相似词，那么就降级为 Random Masking，即随机token替换，这一步称为 Mac，也就是 MLM As Correction。</li>
<li>掩码方式80%的用相似字替换、10%的随机替换、10%保持不变，总的掩码占比是 15%</li>
</ul>
<p>在分析阶段，作者给出了MacBERT各个部分对效果的影响，包括N-Gram掩膜、相似词替换，并且发现 NSP 不如 SOP 效果好，所以作者实际使用的是 SOP 预训练任务。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/mac2.png" alt="图 - 2 MacBERT中各个trick的影响">
    <figcaption>图 - 2 MacBERT中各个trick的影响</figcaption>
    </center>
</figure></p>
<p>作者与XLNet的作者也都提到<code>[MASK]</code>字符只在预训练阶段存在，在实际推理阶段并不存在，这种差别会导致效果变差，MacBERT作者实验了以下几个设置用于研究对效果的影响到底有多大。首先需要说明的是，BERT中的预训练任务包括 MLM / NSP，已经很多人发现这个MLM任务比 NSP 任务更重要，但是对于 MLM 任务，需要解答两个问题，首先是怎么选择被掩码的tokens，然后是这些被选出来的token应该用什么被替换，也就是掩码是什么。</p>
<p>下面四种情况都是，句子长度的 15 % 的字符被掩膜，并且15%中的10%部分保持不变，剩下的区别如下。</p>
<ul>
<li>MacBERT: 80% 的tokens被替换成相似tokens，10%被随机替换</li>
<li>Random Replace: 90% 的tokens都使用随机替换</li>
<li>Partial Mask: 也是 BERT 使用的方式，80% 的tokens被替换成<code>[MASK]</code>，10%的被随机替换</li>
<li>All Mask: 也就是90%的tokens都被替换成<code>[MASK]</code></li>
</ul>
<p>效果如下，这是在CMRC任务上的结果。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/mac1.png" alt="图 - 2 MacBERT几种MLM的效果">
    <figcaption>图 - 2 MacBERT几种MLM的效果</figcaption>
    </center>
</figure></p>
<p>可以发现，即使使用 Random Replace 效果都比BERT的Partial Mask方式更好。</p>
<p>当batch size大于1024时，作者选择使用 LAMB 优化器，而小于1024时，采用AdamW，又一个使用LAMB训练大Batch的论文。</p>
<p>作者也提到，不论是 WWM / SpanBERT / MacBERT 都仅仅设计到训练阶段MLM任务中Mask的生成有关，对其他部分都没有影响。所以这里就给出关键的 N-Gram Masking &amp; Mac 数据的生成过程。</p>
<p>但是现在就只有一个疑问，如果返回的近义词跟原词的长度不一致怎么办？一种办法是直接截断；另一种是如果没有长度相等的词组，那么就随机替换token，类似 WWM 中以 wordpiece 为单位进行随机替换总可以了吧。本来想根据代码里实现找到答案的，但奈何没有看到预训练时数据生成用到的脚本。难办。</p>
<h2 id="后记">后记</h2>
<p>阅读这个论文真正体现了英语能力限制论文理解层次，一直以为SpanBERT中的这句话</p>
<blockquote>
<p>However, we perform this replacement at the span level and not for each token individually.</p>
</blockquote>
<p>说的是将一个任意长度的文本只用一个 <code>[MASK]</code> 替换，然后对应的 SBO 任务是预测出这个<code>[MASK]</code>到底代表了几个 token，并且这些 token 的起始位置在哪里！想了半天、看了几天代码才弄明白。</p>
<p>不过按照之前的想法，是否可行呢，是否可以让模型学习到更深入层次的语言结构信息呢？未知、未验证。</p>
        
            <p>常见的几种分词算法小结，包括BERT用到WordPiece以及Albert用到的Byte-Pair-Encoding。</p>
<p>BERT 用到的分词算法称为 Word Piece Tokenizer，Albert 用到的是 <a href="https://github.com/google/sentencepiece">SentencePiece</a>。SentencePiece 用到的是 Byte-pair-Encoding 算法以及Unigram Language Model算法，Roberta用的也是这种。</p>
<p>Albert 等算法直接使用的是 SentencePiece，这个库是包含上面提到的 BPE / ULM 等子词算法。除此之外，SentencePiece也支持字符、词级别的分词。同时为了支持多语言，SentencePiece将句子视为Unicode编码序列，从而子词算法不依赖于语言的表示。</p>
<p>以BERT用到的分词算法为例进行说明，BERT 中 用到的 Tokenizer 分为两步，第一步是 <code>BasicTokenizer()</code>进行处理，第二步是<code>WordPieceTokenizer</code>进行处理。</p>
<p>参考包括BERT代码 以及 <a href="https://zhuanlan.zhihu.com/p/132361501">BERT 是如何分词的</a>。</p>
<h2 id="basictokenizer">BasicTokenizer</h2>
<p><code>BasicTokenizer</code>只对输入的文本基于空格进行分割。具体过程如下：</p>
<ul>
<li>
<p><code>_clean_text()</code> 去掉文本中的控制字符，然后将<code>\t, \n, \r</code>等字符用空格代替</p>
</li>
<li>
<p><code>_tokenizer_chinese_chars()</code>对中文输入中的每个字两边加上空格，英文输入天然以空格分离</p>
</li>
<li>
<p><code>_run_strip_accents()</code>去掉变音符号，代码如下</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">_run_strip_accents</span>(<span style="color:#999">self</span>, text):
      <span style="color:#d14">&#34;&#34;&#34;Strips accents from a piece of text.&#34;&#34;&#34;</span>
      text <span style="color:#000;font-weight:bold">=</span> unicodedata<span style="color:#000;font-weight:bold">.</span>normalize(<span style="color:#d14">&#34;NFD&#34;</span>, text)
      output <span style="color:#000;font-weight:bold">=</span> []
      <span style="color:#000;font-weight:bold">for</span> char <span style="color:#000;font-weight:bold">in</span> text:
          cat <span style="color:#000;font-weight:bold">=</span> unicodedata<span style="color:#000;font-weight:bold">.</span>category(char)
          <span style="color:#000;font-weight:bold">if</span> cat <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;Mn&#34;</span>:
              <span style="color:#000;font-weight:bold">continue</span>
          output<span style="color:#000;font-weight:bold">.</span>append(char)
      <span style="color:#000;font-weight:bold">return</span> <span style="color:#d14">&#34;&#34;</span><span style="color:#000;font-weight:bold">.</span>join(output)
</code></pre></td></tr></table>
</div>
</div><p>这里涉及两个函数：<code>unicodedata.normalize()</code>以及 <code>unicodedata.category()</code>。变音符是指这些符号: $\dot{a}, \ddot{u}$等，而这个函数可以将类似$r\acute{e}sum\acute{e}$变为$resume$符号。首先<code>unicodedata.normalize()</code>函数返回字符串的规范分解形式（Unicode字符有多种规范形式，代码里默认是<code>NFD</code>形式，即规范分解）；<code>unicodedata.category()</code>函数返回输入字符的<a href="https://www.compart.com/en/unicode/category">Unicode类别</a>。</p>
<p>实际上，变音符号由两个字符组成，通过<code>unicodedata.normalize()</code>可以将两者拆分出来；而<code>unicode_category()</code>函数可以得到每个拆分出来字符的类别。变音符号对应的字符表示是：Mn，即Nonspacing Mark，非间距标记，变音符号也属于这类；剩下的普通字符对应的类别是Ll，即Lowercase Letter，小写字母。</p>
<p>针对变音字符，Albert 的处理更直接：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  <span style="color:#000;font-weight:bold">if</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>keep_accents:
      outputs <span style="color:#000;font-weight:bold">=</span> unicodedata<span style="color:#000;font-weight:bold">.</span>normalize(<span style="color:#d14">&#34;NFKD&#34;</span>, outputs)
      outputs <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;&#34;</span><span style="color:#000;font-weight:bold">.</span>join([c <span style="color:#000;font-weight:bold">for</span> c <span style="color:#000;font-weight:bold">in</span> outputs <span style="color:#000;font-weight:bold">if</span> <span style="color:#000;font-weight:bold">not</span> unicodedata<span style="color:#000;font-weight:bold">.</span>combining(c)])
  <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>do_lower_case:
      outputs <span style="color:#000;font-weight:bold">=</span> outputs<span style="color:#000;font-weight:bold">.</span>lower()
</code></pre></div></li>
<li>
<p><code>_run_split_on_punc()</code>基于符号（逗号、感叹号、$等字符）进行分割</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">_run_split_on_punc</span>(<span style="color:#999">self</span>, text, never_split<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>):
      <span style="color:#d14">&#34;&#34;&#34;Splits punctuation on a piece of text.&#34;&#34;&#34;</span>
      <span style="color:#000;font-weight:bold">if</span> never_split <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span> <span style="color:#000;font-weight:bold">and</span> text <span style="color:#000;font-weight:bold">in</span> never_split:
          <span style="color:#000;font-weight:bold">return</span> [text]
      chars <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">list</span>(text)
      i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
      start_new_word <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
      output <span style="color:#000;font-weight:bold">=</span> []
      <span style="color:#000;font-weight:bold">while</span> i <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#0086b3">len</span>(chars):
          char <span style="color:#000;font-weight:bold">=</span> chars[i]
          <span style="color:#000;font-weight:bold">if</span> _is_punctuation(char):
              output<span style="color:#000;font-weight:bold">.</span>append([char])
              start_new_word <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
          <span style="color:#000;font-weight:bold">else</span>:
              <span style="color:#000;font-weight:bold">if</span> start_new_word:
                  output<span style="color:#000;font-weight:bold">.</span>append([])
              start_new_word <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
              output[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]<span style="color:#000;font-weight:bold">.</span>append(char)
          i <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>

      <span style="color:#000;font-weight:bold">return</span> [<span style="color:#d14">&#34;&#34;</span><span style="color:#000;font-weight:bold">.</span>join(x) <span style="color:#000;font-weight:bold">for</span> x <span style="color:#000;font-weight:bold">in</span> output]
</code></pre></div><p>这一步其实就是将连在一块的句子符号分离出来。初始<code>start_new_word=True</code>，如果遇到符号，那么就把这个符号单独压入<code>output</code>里面，然后再从下一个正常字符开始处理。</p>
</li>
<li>
<p>最后将上述分割结果用空格拼接起来然后再按照空格进行分割。</p>
</li>
</ul>
<h2 id="wordpiecetokenizer">WordPieceTokenizer</h2>
<p>这一步主要思路是根据词表里的单词按照从右向左贪婪的最长匹配方法对词进行分割成更小的单元，即Piece。</p>
<p>代码如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    output_tokens <span style="color:#000;font-weight:bold">=</span> []
    <span style="color:#000;font-weight:bold">for</span> token <span style="color:#000;font-weight:bold">in</span> whitespace_tokenize(text):
        chars <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">list</span>(token)
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">len</span>(chars) <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>max_input_chars_per_word:
            output_tokens<span style="color:#000;font-weight:bold">.</span>append(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>unk_token)
            <span style="color:#000;font-weight:bold">continue</span>

        is_bad <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
        start <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
        sub_tokens <span style="color:#000;font-weight:bold">=</span> []
        <span style="color:#000;font-weight:bold">while</span> start <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#0086b3">len</span>(chars):
            end <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">len</span>(chars)
            cur_substr <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
            <span style="color:#000;font-weight:bold">while</span> start <span style="color:#000;font-weight:bold">&lt;</span> end:
                substr <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;&#34;</span><span style="color:#000;font-weight:bold">.</span>join(chars[start:end])
                <span style="color:#000;font-weight:bold">if</span> start <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
                    substr <span style="color:#000;font-weight:bold">=</span> <span style="color:#d14">&#34;##&#34;</span> <span style="color:#000;font-weight:bold">+</span> substr
                <span style="color:#000;font-weight:bold">if</span> substr <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>vocab:
                    cur_substr <span style="color:#000;font-weight:bold">=</span> substr
                    <span style="color:#000;font-weight:bold">break</span>
                end <span style="color:#000;font-weight:bold">-=</span> <span style="color:#099">1</span>
            <span style="color:#000;font-weight:bold">if</span> cur_substr <span style="color:#000;font-weight:bold">is</span> <span style="color:#999">None</span>:
                is_bad <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
                <span style="color:#000;font-weight:bold">break</span>
            sub_tokens<span style="color:#000;font-weight:bold">.</span>append(cur_substr)
            start <span style="color:#000;font-weight:bold">=</span> end

        <span style="color:#000;font-weight:bold">if</span> is_bad:
            output_tokens<span style="color:#000;font-weight:bold">.</span>append(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>unk_token)
        <span style="color:#000;font-weight:bold">else</span>:
            output_tokens<span style="color:#000;font-weight:bold">.</span>extend(sub_tokens)
    <span style="color:#000;font-weight:bold">return</span> output_tokens
</code></pre></td></tr></table>
</div>
</div><p>可以看出来，两个while循环里面，从输入单词的右边开始，end不停的向前推进，同时测试start : end之间的字符是否在vocab字典里面，如果在的话，这些后面的 piece 会加上<code>##</code>前缀，作为分词结果保存起来。外面的while循环里会更新start参数，直至粉刺结束。也就是说，单词被分成子词，并且子词以<code>##</code>开头。</p>
<p>分词词表里那些以<code>##</code>开头的字符就是备选的 word piece，不是单词的开头，而是一个单词被分成好几片 piece 时后面的几个 piece。</p>
<p>Roberta / XLM 等模型中提到的 <code>&lt;s&gt;</code> 其实就是 <code>[CLS]</code>，同理<code>&lt;/s&gt;</code>对应<code>[SEP]</code>。</p>
<p>至此，BERT里面的分词过程分析完了。</p>
<h2 id="byte-pair-encoding">Byte-Pair-Encoding</h2>
<p>BPE 算法也被称为字节对编码或二元编码，简单来说，算法过程就是将相邻出现频次最高的两个连续字节数据用一个新的字节数据表示，直到满足词典中单词的个数或下一个最高频的字节对出现频率为1时终止。优点是可以平衡字典词表大小以及步长（编码句子所需要的token数量），缺点是合词过程是固定的，即没有考虑其它更有效的分词单元。</p>
<p>算法会现在每个词的结尾加上一个结束符<code>&lt;/w&gt;</code>，用于区分是否位于词的结尾还是词的中间，如<code>st</code>出现在<code>st ar</code>或<code>wide st&lt;/w&gt;</code>两个位置意义是完全不同的。</p>
<p>对应的代码主要函数包括：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">import</span> <span style="color:#555">re</span><span style="color:#000;font-weight:bold">,</span> <span style="color:#555">collections</span>
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get_stats</span>(vocab):
    pairs <span style="color:#000;font-weight:bold">=</span> collections<span style="color:#000;font-weight:bold">.</span>defaultdict(<span style="color:#0086b3">int</span>)
    <span style="color:#000;font-weight:bold">for</span> word, freq <span style="color:#000;font-weight:bold">in</span> vocab<span style="color:#000;font-weight:bold">.</span>items():
        symbols <span style="color:#000;font-weight:bold">=</span> word<span style="color:#000;font-weight:bold">.</span>split()
        <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#0086b3">len</span>(symbols)<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>):
            <span style="color:#998;font-style:italic"># 这里pairs的键是一个 list</span>
            pairs[symbols[i],symbols[i<span style="color:#000;font-weight:bold">+</span><span style="color:#099">1</span>]] <span style="color:#000;font-weight:bold">+=</span> freq
    <span style="color:#000;font-weight:bold">return</span> pairs

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">merge_vocab</span>(pair, v_in):
    v_out <span style="color:#000;font-weight:bold">=</span> {}
    bigram <span style="color:#000;font-weight:bold">=</span> re<span style="color:#000;font-weight:bold">.</span>escape(<span style="color:#d14">&#39; &#39;</span><span style="color:#000;font-weight:bold">.</span>join(pair))
    p <span style="color:#000;font-weight:bold">=</span> re<span style="color:#000;font-weight:bold">.</span>compile(<span style="color:#d14">r</span><span style="color:#d14">&#39;(?&lt;!\S)&#39;</span> <span style="color:#000;font-weight:bold">+</span> bigram <span style="color:#000;font-weight:bold">+</span> <span style="color:#d14">r</span><span style="color:#d14">&#39;(?!\S)&#39;</span>)
    <span style="color:#000;font-weight:bold">for</span> word <span style="color:#000;font-weight:bold">in</span> v_in:
        w_out <span style="color:#000;font-weight:bold">=</span> p<span style="color:#000;font-weight:bold">.</span>sub(<span style="color:#d14">&#39;&#39;</span><span style="color:#000;font-weight:bold">.</span>join(pair), word)
        v_out[w_out] <span style="color:#000;font-weight:bold">=</span> v_in[word]
    <span style="color:#000;font-weight:bold">return</span> v_out
</code></pre></div><p>其中，<code>get_stats()</code>函数就是将任意两个连续的bytes（char）拼接起来，加入到字典<code>pairs</code>里面，然后就可以取出出现频次最大的字节对了。<code>merge_vocab()</code>则根据传进来的连续字符拼接结果<code>best</code>(是一个list)进行处理，首先是去除<code>re</code>相关的特殊字符，<code>v_in</code>是当前字典，然后这个函数将字典中best出现的连续两个字符用新的字符替换掉。</p>
<h2 id="word-piece">Word Piece</h2>
<p>前面提到了 WordPiece 算法怎么使用，这里是一些制作 WordPiece 词表的细节。</p>
<p>与BPE算法类似，WordPiece 算法也是先将输入的句子分解成子词（如最细粒度的char级别），然后合并子词，不同的地方在于 WordPiece 在合并连续子词的时候会考虑句子的语言模型概率。简单来说，BPE选择的是频次最高的相邻子词进行合并，WordPiece选择的是能够提升语言模型概率最大的相邻子词进行合并加入到词表。</p>
<p>下面用数学公式进行说明。假设当前句子$S=(t_0, t_1, \ldots, t_n)$共n个子词构成， 并且假设句子中每个子词是独立分布的，则句子的语言模型概率定义为：</p>
<p>$$\log P(S) = \sum_{i=1}^{n} P(t_i)$$</p>
<p>即所有子词概率的乘积，子词概率的初始值基于训练预料统计子词频率得到。</p>
<p>然后就是合并子词，假设把相邻位置的 x 和 y 两个子词进行合并成 z，那么句子 S 的似然值变化表示为：</p>
<p>$$\log P(t_z) - (\logP(t_x) + \logP(t_y) = \log \frac{P(t_z)}{P(t_x)P(t_y)}$$</p>
<p>也就是似然值的变化是两个子词之间的互信息。简而言之，WordPiece每次选择合并子词，他们具有最大的互信息值，也就是两个子词在语言模型上具有较强的关联性，它们在语料中经常相邻着出现。</p>
<h2 id="unigram-language-model">Unigram Language Model</h2>
<p>与 WordPiece 类似，同样基于语言概率模型进行合并子词，不同的地方在于，BPE / WordPiece 两个算法的词表都是由小到大增加，而ULM的词表则是减量法，即先初始化一个大词表，然后根据评估准则不断丢弃词表，直到满足限定条件。不同的地方在于，ULM算法会考虑句子的不同分词形式（即子词单元不同），因而可以输出带概率的多个子词分段。</p>
<p>至于在所有可能的分词结果中选取概率最高的分词形式，计算量比较大，可以使用维特比算法实现；另一方面，子词的初始概率是通过 EM 算法得到的。具体的就了解不深入了。</p>
<p>具体可以参考<a href="https://zhuanlan.zhihu.com/p/198964217">NLP三大Subword模型详解：BPE、WordPiece、ULM</a>。</p>
<h2 id="中文分词工具">中文分词工具</h2>
<p>中文的 wordpiece 就是分字。基于准确率、分词速度角度对比的话，LTP准召比较高，但是速度最慢，整体的对比如图 - 1。</p>
<p><figure>
    <center>
    <img src="/imgs/tokenizers/comp0.png" alt="图 - 1 不同中文分词算法准召、速度对比">
    <figcaption>图 - 1 不同中文分词算法准召、速度对比</figcaption>
    </center>
</figure></p>
<ul>
<li>
<p>哈工大 LTP: <a href="https://github.com/HIT-SCIR/ltp/blob/master/docs/quickstart.rst">LTP</a></p>
<p>支持类似 HanLP 的几个功能！</p>
</li>
<li>
<p>结巴: <a href="https://github.com/fxsjy/jieba">jieba - github</a></p>
<ul>
<li>基于前缀字典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图</li>
<li>利用动态规划，查找最大概率路径，找出基于词频的最大切分组合</li>
<li>对于未登陆词，采用了基于汉字成词能力的HMM模型，使用了viterbi算法</li>
</ul>
<p>支持四种分词模式，精确模式、全模式、搜索引擎模型、paddle模式等。</p>
<p>此外，jieba 支持分词、自定义词典、关键词提取、词性标注。关键词提取支持基于 TF-IDF（返回TF-IDF权重最大的词）、TextRank提取关键词（分词，然后再固定窗口内计算词的共现关系，构建图，然后计算图中节点的PageRank）。</p>
</li>
<li>
<p>SnowNLP</p>
</li>
<li>
<p>PkuSeg</p>
</li>
<li>
<p>THULAC</p>
</li>
<li>
<p>HanLP: <a href="https://github.com/hankcs/HanLP">HanLP</a></p>
<p>支持多大10中任务：分词、词性标注、命名实体识别、依存句法分析、成分句法分析、语义依存分析、语义角色标注、词干提取、词法语法特征分析、抽象意义分析等等。</p>
</li>
</ul>
        
            <p>主要是几种常见的MLM的改进以及对应的代码实现，包括WWM, SpanBERT, ERNIE这三种。</p>
<p>这一部分可能会涉及到一些分词算法的概念，但是主要用于对分词结果进行处理。</p>
<h2 id="wwm">WWM</h2>
<p>WWM 也算是一种简单有效的引入语言知识的方法。直观来说，不再是类似BERT那样分字，然后每个字考虑被掩膜掉，而是使用了 <a href="http://ltp.ai/">LTP</a> 工具包进行分词，然后制作掩膜的时候，就可以一次性把一个完整的词组进行掩膜掉了。</p>
<p>下图给出了WWM 与 BERT 使用的两种掩膜方式的对比。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/wwm0.png" alt="图 - 1 WWM算法分词掩膜效果示意图">
    <figcaption>图 - 1 WWM算法分词掩膜效果示意图</figcaption>
    </center>
</figure></p>
<p>这篇论文还有一点是，作者使用LAMB进行优化，而不是AdamW，毕竟前者更适合Large Batch的情况（作者实际使用的Batch Size =2560(128) / 384(512)）。此外作者还有以下几点BERT训练心得。</p>
<ul>
<li>初始学习率对 BERT 的效果有重要影响，必须仔细调整，但BERT-WWM / BERT 两个模型最优的学习率比较接近，但是ERNIE的学习率差别就很大</li>
<li>如果预训练任务与下游任务之间差别较大，则建议基于下有任务也做一下预训练</li>
</ul>
<p>实际实现中，BERT-WWM / BERT两者最大的区别在于模型实现，数据输入、训练Loss等都没有变化。在英文版WWM中，如果说一个Word被分成若干个子词，那WWM的做法是将这些这些子词分别都进行处理（mask，保留，替换），注意所有的子词并非需要做相同的处理，即同一个Word的多个子词上，可以这个子词作替换，那个子词用mask，还有一个子词保留，这些都是可以的，具体例子参考：<a href="https://github.com/ymcui/Chinese-BERT-wwm/issues/4">mask的一个小细节</a>，对应的主要信息截图如下。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/wwm1.png" alt="图 - 2 WWM算法分词掩膜效果示意图2">
    <figcaption>图 - 2 WWM算法分词掩膜效果示意图2</figcaption>
    </center>
</figure></p>
<h3 id="wwm实现代码">WWM实现代码</h3>
<p>参考：<a href="https://github.com/interviewBubble/Google-ALBERT/blob/master/create_pretraining_data.py">tf bert-wwm</a></p>
<p>或者Transformer库里对应的<code>run_chinese_ref.py</code>以及<code>data_collator.py</code>文件中的<code>DataCollatorForWholeWordMask</code>类的实现。</p>
<p>代码分成两个主要部分。第一部分是根据LTP分词结果来为词语非第一个字的前面加上<code>##</code>符号；然后第二部分实现对应的 Mask 过程，注意Mask的最大长度不会超过 15% 的阈值。</p>
<p>第一部分根据 LTP 分词结果 配合最长词长度为3开始贪婪尝试。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get_new_segment</span>(segment):
    seq_cws <span style="color:#000;font-weight:bold">=</span> jieba<span style="color:#000;font-weight:bold">.</span>lcut(<span style="color:#d14">&#34;&#34;</span><span style="color:#000;font-weight:bold">.</span>join(segment)) <span style="color:#998;font-style:italic"># 分词</span>
    seq_cws_dict <span style="color:#000;font-weight:bold">=</span> {x: <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">for</span> x <span style="color:#000;font-weight:bold">in</span> seq_cws} <span style="color:#998;font-style:italic"># 分词后的词加入到词典dict</span>
    new_segment <span style="color:#000;font-weight:bold">=</span> []
    i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
    <span style="color:#000;font-weight:bold">while</span> i <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#0086b3">len</span>(segment): <span style="color:#998;font-style:italic"># 从句子的第一个字开始处理，知道处理完整个句子</span>
      <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">len</span>(re<span style="color:#000;font-weight:bold">.</span>findall(<span style="color:#d14">&#39;[</span><span style="color:#d14">\u4E00</span><span style="color:#d14">-</span><span style="color:#d14">\u9FA5</span><span style="color:#d14">]&#39;</span>, segment[i])) <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:  <span style="color:#998;font-style:italic"># 如果找不到中文的，原文加进去即不用特殊处理。</span>
        new_segment<span style="color:#000;font-weight:bold">.</span>append(segment[i])
        i <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
        <span style="color:#000;font-weight:bold">continue</span>

      has_add <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
      <span style="color:#000;font-weight:bold">for</span> length <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">3</span>, <span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>):
        <span style="color:#000;font-weight:bold">if</span> i <span style="color:#000;font-weight:bold">+</span> length <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#0086b3">len</span>(segment):
          <span style="color:#000;font-weight:bold">continue</span>
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#d14">&#39;&#39;</span><span style="color:#000;font-weight:bold">.</span>join(segment[i:i <span style="color:#000;font-weight:bold">+</span> length]) <span style="color:#000;font-weight:bold">in</span> seq_cws_dict:
          new_segment<span style="color:#000;font-weight:bold">.</span>append(segment[i])
          <span style="color:#000;font-weight:bold">for</span> l <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">1</span>, length):
            new_segment<span style="color:#000;font-weight:bold">.</span>append(<span style="color:#d14">&#39;##&#39;</span> <span style="color:#000;font-weight:bold">+</span> segment[i <span style="color:#000;font-weight:bold">+</span> l])
          i <span style="color:#000;font-weight:bold">+=</span> length
          has_add <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
          <span style="color:#000;font-weight:bold">break</span>
      <span style="color:#000;font-weight:bold">if</span> <span style="color:#000;font-weight:bold">not</span> has_add:
        new_segment<span style="color:#000;font-weight:bold">.</span>append(segment[i])
        i <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
    <span style="color:#998;font-style:italic"># print(&#34;get_new_segment.wwm.get_new_segment:&#34;,new_segment)</span>
    <span style="color:#000;font-weight:bold">return</span> new_segment
</code></pre></div><p>第二部分，根据第一步的结果进行Mask，具体的函数是第一个参考代码里的<code>create_masked_lm_predictions()</code>函数。代码省略，但是思路大体就是将属于同一个词组（以<code>#</code>开头）放到同一个列表中，然后所有的词组又放在额外一层列表中，至此构成一个两层的列表。接下来就是将这个列表随机打乱，然后从左到右依次mask，直到总的 mask 的长度大于 15% 了，注意这里是随机打乱，然后从左到右依次mask！至于Mask的过程就是以分字为单位进行的。另外一点是，代码里保证总的 mask 掉的长度小于15%，如果mask 掉下一个词组的话，那么就略过；还有一点是，最后通过一个 sort 函数来恢复被打乱的顺序，这个顺序信息是给每个 token 做了个索引。</p>
<p>另一种实现是 transformer 里面的实现，生成 mask 的主要过程在 <code>_whole_word_mask()</code> 函数里，主要思路与 Bert-wwm 的实现思路一致，也是随机打乱，然后从头向后一次生成掩膜，不过这里借助额外的单独文件离线生成词组信息。</p>
<h2 id="ernie">ERNIE</h2>
<p>百度 ERNIE 的主要思路是将phrase-level strategy &amp; entity-level strategy两种知识引入到模型训练中，具体对应的是 phrase-level masking &amp; entity-level masking，命名实体包括人物、地点、组织、产品等。示意图如图 - 3所示。</p>
<p><figure>
    <center>
    <img src="/imgs/mlm-related/ernie0.png" alt="图 - 3 ERNIE 与 BERT MLM区别示意图">
    <figcaption>图 - 3 ERNIE 与 BERT MLM区别示意图</figcaption>
    </center>
</figure></p>
<p>对于实现，<a href="https://github.com/lonePatient/ERNIE-text-classification-pytorch">ERNIE-text-classification-pytorch</a>仓库里的代码只适合微调训练，不支持预训练任务。</p>
<p>这里以 paddlepaddle 官方库中 ERNIE v1.0 的实现为例进行说明，模型的前向计算以及 Loss 的计算方面与普通 BERT 相同，所以重点在于<code>ErnieDataReader</code> 类的实现。</p>
<p>根据<a href="https://github.com/PaddlePaddle/ERNIE/blob/repro/README.zh.md#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">PaddlePaddle-ERNIE-github</a>文档里的说法，输入数据的一个示例如下:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">1 1048 492 1333 1361 1051 326 2508 5 1803 1827 98 164 133 2777 2696 983 121 4 19 9 634 551 844 85 14 2476 1895 33 13 983 121 23 7 1093 24 46 660 12043 2 1263 6 328 33 121 126 398 276 315 5 63 44 35 25 12043 2;0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55;-1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 -1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 -1;0
</code></pre></div><p>每个样本由5个 &lsquo;;&rsquo; 分隔的字段组成，数据格式: token_ids; sentence_type_ids; position_ids; seg_labels; next_sentence_label；其中 seg_labels 表示分词边界信息: 0表示词首、1表示非词首、-1为占位符, 占位符对应的词为 CLS 或者 SEP。代码中生成 mask 的主要逻辑在<code>mask()</code>函数内，这里仅给出实体词级的掩膜，忽略汉字 word piece级别的掩膜生成。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">mask</span>(batch_tokens,
         seg_labels,
         mask_word_tags,
         total_token_num,
         vocab_size,
         CLS<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>,
         SEP<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>,
         MASK<span style="color:#000;font-weight:bold">=</span><span style="color:#099">3</span>):
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Add mask for batch_tokens, return out, mask_label, mask_pos;
</span><span style="color:#d14">    Note: mask_pos responding the batch_tokens after padded;
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    max_len <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">max</span>([<span style="color:#0086b3">len</span>(sent) <span style="color:#000;font-weight:bold">for</span> sent <span style="color:#000;font-weight:bold">in</span> batch_tokens])
    mask_label <span style="color:#000;font-weight:bold">=</span> []
    mask_pos <span style="color:#000;font-weight:bold">=</span> []
    prob_mask <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>rand(total_token_num)
    <span style="color:#998;font-style:italic"># Note: the first token is [CLS], so [low=1]</span>
    replace_ids <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>random<span style="color:#000;font-weight:bold">.</span>randint(<span style="color:#099">1</span>, high<span style="color:#000;font-weight:bold">=</span>vocab_size, size<span style="color:#000;font-weight:bold">=</span>total_token_num)
    pre_sent_len <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
    prob_index <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
<span style="color:#998;font-style:italic"># [[sentence 0], [sentence 1], [sentence 2] ... [sentence N-1]], batch size = N</span>
    <span style="color:#000;font-weight:bold">for</span> sent_index, sent <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(batch_tokens):        <span style="color:#998;font-style:italic"># sent: current sentence</span>
        mask_flag <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
        mask_word <span style="color:#000;font-weight:bold">=</span> mask_word_tags[sent_index]
        prob_index <span style="color:#000;font-weight:bold">+=</span> pre_sent_len
        <span style="color:#000;font-weight:bold">if</span> mask_word:
            beg <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
            <span style="color:#000;font-weight:bold">for</span> token_index, token <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(sent):      <span style="color:#998;font-style:italic"># tokens in current sentence</span>
                seg_label <span style="color:#000;font-weight:bold">=</span> seg_labels[sent_index][token_index]     <span style="color:#998;font-style:italic"># 表示分词边界信息</span>
                <span style="color:#000;font-weight:bold">if</span> seg_label <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">1</span>:                                  <span style="color:#998;font-style:italic"># 非词首</span>
                    <span style="color:#000;font-weight:bold">continue</span>
                <span style="color:#000;font-weight:bold">if</span> beg <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:                                        <span style="color:#998;font-style:italic"># 从 3th token 开始</span>
                    <span style="color:#000;font-weight:bold">if</span> seg_label <span style="color:#000;font-weight:bold">!=</span> <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>:
                        beg <span style="color:#000;font-weight:bold">=</span> token_index
                    <span style="color:#000;font-weight:bold">continue</span>

                prob <span style="color:#000;font-weight:bold">=</span> prob_mask[prob_index <span style="color:#000;font-weight:bold">+</span> beg]                  <span style="color:#998;font-style:italic"># 当前词被掩膜的概率</span>
                <span style="color:#000;font-weight:bold">if</span> prob <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0.15</span>:
                    <span style="color:#000;font-weight:bold">pass</span>
                <span style="color:#000;font-weight:bold">else</span>:
                    <span style="color:#000;font-weight:bold">for</span> index <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">xrange</span>(beg, token_index):          <span style="color:#998;font-style:italic"># 对每个词里的所有 word 都进行掩膜</span>
                        prob <span style="color:#000;font-weight:bold">=</span> prob_mask[prob_index <span style="color:#000;font-weight:bold">+</span> index]
                        base_prob <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1.0</span>
                        <span style="color:#000;font-weight:bold">if</span> index <span style="color:#000;font-weight:bold">==</span> beg:            <span style="color:#998;font-style:italic"># 词组的首字</span>
                            base_prob <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0.15</span>
                        <span style="color:#000;font-weight:bold">if</span> base_prob <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">0.2</span> <span style="color:#000;font-weight:bold">&lt;</span> prob <span style="color:#000;font-weight:bold">&lt;=</span> base_prob:
                            mask_label<span style="color:#000;font-weight:bold">.</span>append(sent[index])
                            sent[index] <span style="color:#000;font-weight:bold">=</span> MASK      <span style="color:#998;font-style:italic"># 用 Mask 掩膜</span>
                            mask_flag <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
                            mask_pos<span style="color:#000;font-weight:bold">.</span>append(sent_index <span style="color:#000;font-weight:bold">*</span> max_len <span style="color:#000;font-weight:bold">+</span> index)
                        <span style="color:#000;font-weight:bold">elif</span> base_prob <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">0.1</span> <span style="color:#000;font-weight:bold">&lt;</span> prob <span style="color:#000;font-weight:bold">&lt;=</span> base_prob <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">0.2</span>:
                            mask_label<span style="color:#000;font-weight:bold">.</span>append(sent[index])
                            sent[index] <span style="color:#000;font-weight:bold">=</span> replace_ids[prob_index <span style="color:#000;font-weight:bold">+</span> index]       <span style="color:#998;font-style:italic"># 随机替换其它 token </span>
                            mask_flag <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
                            mask_pos<span style="color:#000;font-weight:bold">.</span>append(sent_index <span style="color:#000;font-weight:bold">*</span> max_len <span style="color:#000;font-weight:bold">+</span> index)
                        <span style="color:#000;font-weight:bold">else</span>:
                            mask_label<span style="color:#000;font-weight:bold">.</span>append(sent[index])                      <span style="color:#998;font-style:italic"># 保持不变</span>
                            mask_pos<span style="color:#000;font-weight:bold">.</span>append(sent_index <span style="color:#000;font-weight:bold">*</span> max_len <span style="color:#000;font-weight:bold">+</span> index)

                <span style="color:#000;font-weight:bold">if</span> seg_label <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>:
                    beg <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
                <span style="color:#000;font-weight:bold">else</span>:
                    beg <span style="color:#000;font-weight:bold">=</span> token_index
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#998;font-style:italic"># do wordpiece masking</span>

        pre_sent_len <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">len</span>(sent)

    mask_label <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array(mask_label)<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#d14">&#34;int64&#34;</span>)<span style="color:#000;font-weight:bold">.</span>reshape([<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>])
    mask_pos <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array(mask_pos)<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#d14">&#34;int64&#34;</span>)<span style="color:#000;font-weight:bold">.</span>reshape([<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>])
    <span style="color:#000;font-weight:bold">return</span> batch_tokens, mask_label, mask_pos
</code></pre></div><p>其中，<code>mask_pos</code>是用于提取出那些被mask掉的token计算 Loss，所以跟具体实现有关系，transformer 里的那种做法是不需要这个信息的。<code>mask_word</code>表示当前的句子是否以词组为单位进行掩膜，所以这里也保留了一定的概率进行 WordPiece 掩膜。<code>if beg == 0</code>对应的代码块里保证<code>beg</code>的取值范围是$[0, token_index)$，所以可以取到完整的一个词组；然后就是掩膜部分的计算了，这里每个字只有80%的概率用 <code>MASK</code> 替换，然后10%的概率随机Token替换，剩下的10%的概率保持不变，值得注意的地方在于首字的概率用来决定是否进行对词组进行掩膜，并且对应的调整<code>base_prob</code>为首字的最大概率，而非首字的最大概率仍为1.0。</p>
<p>至此，一方面是mask的生成过程，另一方面就是生成输入数据中的<code>seg_labels</code>这个信息了，只要掌握了算法思想，实现起来还是有很多种方法的。</p>
<h2 id="其它">其它</h2>
<p>分析完上述几个简单的掩膜方法，一个很直接的思路就是结合 ERNIE 的词组掩膜 + Span 思路，就是说用一个<code>[MASK]</code>来代替一个实体词组，然后借鉴 Span 的做法，也让模型预测这个词组的 SBO 任务！效果待验证。</p>
<p>Oscar 多模态模型用到了目标检测的结果作为 tag，这个tag与图片、文本的组织方式为：``。</p>
        
            <p>重读论文之后，发现还真是一个非常精巧的模型。</p>
<h2 id="模型结构">模型结构</h2>
<p>Swin-Transformer的创新点主要提出了下面几个新的结构：</p>
<ul>
<li>基于Transformer的层级结构提取多尺度的Feature Map</li>
<li>Shifted Window based Self-Attention</li>
</ul>
<p>对于多尺度Feature Map与ViT等模型的但尺度对比如下，左图红色框表示 Windows，灰色框表示Patch。SwinT 每层 Feature Map 的Windows 不同（每个 Windows 内Patch数量固定），包含的总的 Patch 也就不同（即分辨率不同）；而右边 ViT 模型自始至终都只包含一个 Windows，且 Windows 内都包含固定的 Patch 个数。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin0.png" alt="图 - 1 多尺度Feature Map示意图">
    <figcaption>图 - 1 多尺度Feature Map示意图</figcaption>
    </center>
</figure></p>
<p>Swin-Transformer 中涉及了几个层次概念：Pixels, Patch, Windows。Pixels 是在输入的原始图像中定义的，比如 224 * 224 的空间维度；Patch 是基于Pixels定义的，每个Patch论文中指定为 4 * 4 个像素，扁平化之后，每个 Patch 对应的尺寸是：<code>1 * 48</code>，其中48 = 4 * 4 * 3，即每个像素包含3个 RGB 通道；Windows 基于 Patch，论文中每个 Windows 内包含7 * 7 个Patch，计算Self-Attention时是在每个 Windows 内进行的，所以只要固定 Windows 大小，则 Windows 的个数与图像的 H * W 成正比，而不是与 $(H<em>W)^2$成正比了。注意，在不同的 Stage 中的 Windows 对应的 Pixels 个数是不同的，比如在 Stage1，对应的像素是 28 * 28(4</em>7)，在 Stage 2 就对应了 56 * 56(4 * 7 * 2)。</p>
<p>由于不同的 Windows 之间不会重叠，所以作者机智的引入了 Shifted Window Self Attention并引出了对应提高计算效率的做法。</p>
<p>总而言之，模型结构还是非常巧妙的。</p>
<h3 id="整体结构">整体结构</h3>
<p>整个模型结构可以分为3个部分：Stem / Backbone / Head 部分。</p>
<p>对于 Stem 部分，Swin-Transformer 定义了 Patch，每个 Patch 是 raw pixel RGB 数值拼接起来的，论文中每个 Patch 对应 4 * 4 个raw pixels，所以每个 Patch 的特征维度是 4 * 4 * 3 = 48。然后经过一个全连阶层映射到 embedding size 维度，也就是论文以及代码中的 C，实际实现中，这一步是通过一个 <code>conv2d(ks=4, stride=4)</code> 的卷积层实现的。</p>
<p>Backbone 部分是本文的主要内容，包含四个 Stage，每个 Stage 都是由若干层 Swin Transformer Block构成，不同 Stage 之间通过 Patch Merge 来下采样。Patch Merge 的实现过程就是将<code>2 * 2</code>个相邻的Patch拼接起来，由<code>2 * 2 * C</code>的数据得到一个<code>1 * 4C</code>的数据，然后再经过一个全连阶层降维到<code>1 * 2C</code>，经过这一步，Tokens 数量下降4倍，在空间维度相当于将Feature Map的维度下采样一倍，这既提高了后续层的计算效率，与提高了模型的感受野，生成具有不同感受野的Feature Map！每个 Stage 下采样1倍，Stage 2/3/4 对应的 Feature Map 的空间维度分别为：$\frac{H}{8} \times \frac{W}{8}, \frac{H}{16} \times \frac{W}{16}, \frac{H}{32} \times \frac{W}{32}$，输入是 224 * 224的图像，最后输出的是 7 * 7的Feature Map。</p>
<p>对于 Swin Transformer Block的结构细节在下面。</p>
<p>Head 部分就是一个普通的Global Pooling + Linear(feat dim, cls num) 的结构。</p>
<p>说回 Swin Transformer Block，与正常 Transformer 的唯一区别是计算 Self Attention的时候，FFN 与正常 Transformer 结构一致。Swin Transformer Block 使用了Shifted Window based Self-Attention进行计算，将普通 Self Attention 中计算 Attention Score 的复杂度由与像素个数的平方成正比下降到与像素个数成正比！</p>
<p>主要涉及到的想法有两点：</p>
<ul>
<li>Windows Based Self Attiontion: 只在 Windows 内计算Attention Score，假设Windows 的大小是 M（即每个 Windows 内包含 M * M 个Patch，对应M * 4 * M * 4个像素），则计算复杂度与 $M^2$成正比，不同 Windows 之间不重叠</li>
<li>Shifted Windows Based Self Attention: 上一步导致Windows 内的像素与 Windows 外的像素关联性较低，作者提出了 Shifted Window Partition in Successive Blocks，也就是下一层 Transformer 计算时，Feature Map 会有一个平移，所以原来不交互的相邻的两个 Windows现在属于同一个 Windows了</li>
</ul>
<p>对于第二点，在Shifted的时候会导致 Windows 的分区变多（因为要保证原来属于边界两边的像素不能互相计算相关性），作者提出了 Efficient Batch Computation For Shifted Configuration 的方法 + 配合 Attention Mask 来提高计算效率。</p>
<p>对于Patch / Shifted Windows 的示意图如下图。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin1.png" alt="图 - 2 Shifted Window示意图">
    <figcaption>图 - 2 Shifted Window示意图</figcaption>
    </center>
</figure></p>
<p>包含 Patch Merging 的整体模型结构如下图，可以看到核心的 Patch Parttition (Stem Block) 以及 Patch Merging 以及对应的 Token 的个数与特征维度。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin2.png" alt="图 - 3 多尺度Feature Map示意图">
    <figcaption>图 - 3 多尺度Feature Map示意图</figcaption>
    </center>
</figure></p>
<p>这里 Patch Merging 是在每个 Stage 开始的时候完成的，但实际在代码实现中，是在每个 Stage 的最后才进行的，这样 Feature Dim 导致模型的计算量增加速度会下降。</p>
<p>在Swin Transformer Block中，正常 Windows Based Self Attention 与 Shifted Windows Based Self Attention 的分布是相互交替的，并且这里 Shift 的大小是 <code>window_size // 2</code>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#998;font-style:italic"># ...</span>
        shift_size<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span> <span style="color:#000;font-weight:bold">if</span> (i <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>) <span style="color:#000;font-weight:bold">else</span> window_size <span style="color:#000;font-weight:bold">//</span> <span style="color:#099">2</span>,
    <span style="color:#998;font-style:italic"># ...</span>
</code></pre></td></tr></table>
</div>
</div><p>针对不同的参数量，模型结构如下。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin9.png" alt="图 - 4 不同参数量SwinT的结构">
    <figcaption>图 - 4 不同参数量SwinT的结构</figcaption>
    </center>
</figure></p>
<p>下面详细说明一下两种 Windows Based Self Attention 的实现。</p>
<h3 id="windows-based-self-attention">Windows based Self Attention</h3>
<p>与正常 Transformer 中 Self Attention 的区别在于这里计算 Attention Score 时仅限于 Windows 内的 Patch 之间计算，与 Windows 外的 Patch 不会计算，这一步通过将 Seq Len 由 $H * W$ 变为 $\frac{H}{window size} \times \frac{W}{windows size}$，其余多出来的数据加入到 Batch 维度实现的。这一步是基于<code>window_partition / window_reverse</code>函数实现的。</p>
<p>其中，<code>window_partion()</code>就是切分然后合并到 Batch 维度上。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">window_partition</span>(x, window_size):
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Args:
</span><span style="color:#d14">        x: (B, H, W, C)
</span><span style="color:#d14">        window_size (int): window size
</span><span style="color:#d14">
</span><span style="color:#d14">    Returns:
</span><span style="color:#d14">        windows: (num_windows*B, window_size, window_size, C)
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    B, H, W, C <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape
    x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>view(B, H <span style="color:#000;font-weight:bold">//</span> window_size, window_size, W <span style="color:#000;font-weight:bold">//</span> window_size, window_size, C)
    windows <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">0</span>, <span style="color:#099">1</span>, <span style="color:#099">3</span>, <span style="color:#099">2</span>, <span style="color:#099">4</span>, <span style="color:#099">5</span>)<span style="color:#000;font-weight:bold">.</span>contiguous()<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, window_size, window_size, C)
    <span style="color:#000;font-weight:bold">return</span> windows

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">window_reverse</span>(windows, window_size, H, W):
    <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">    Args:
</span><span style="color:#d14">        windows: (num_windows*B, window_size, window_size, C)
</span><span style="color:#d14">        window_size (int): Window size
</span><span style="color:#d14">        H (int): Height of image
</span><span style="color:#d14">        W (int): Width of image
</span><span style="color:#d14">
</span><span style="color:#d14">    Returns:
</span><span style="color:#d14">        x: (B, H, W, C)
</span><span style="color:#d14">    &#34;&#34;&#34;</span>
    B <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">int</span>(windows<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">/</span> (H <span style="color:#000;font-weight:bold">*</span> W <span style="color:#000;font-weight:bold">/</span> window_size <span style="color:#000;font-weight:bold">/</span> window_size))
    x <span style="color:#000;font-weight:bold">=</span> windows<span style="color:#000;font-weight:bold">.</span>view(B, H <span style="color:#000;font-weight:bold">//</span> window_size, W <span style="color:#000;font-weight:bold">//</span> window_size, window_size, window_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)
    x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">0</span>, <span style="color:#099">1</span>, <span style="color:#099">3</span>, <span style="color:#099">2</span>, <span style="color:#099">4</span>, <span style="color:#099">5</span>)<span style="color:#000;font-weight:bold">.</span>contiguous()<span style="color:#000;font-weight:bold">.</span>view(B, H, W, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)
    <span style="color:#000;font-weight:bold">return</span> x
</code></pre></td></tr></table>
</div>
</div><p>这一步的计算量变化。正常 Transformer 层的计算复杂度：</p>
<p>$$\Omega (\textrm{MSA}) = 4hwC^2 + 2 (hw)^2 C$$</p>
<p>变为 Windows Based Self Attention 的计算复杂度：</p>
<p>$$\Omega (\textrm{MSA}) = 4hwC^2 + 2 M^2 (hw) C$$</p>
<p>两个式子等号右边的第一项都是表示4个Linear层的计算复杂度（Q, K, V + 输出），第二项是 Self Attention 的计算复杂度，可以看出，当每个 Windows 内的 Patch 数量 M * M 固定时，这一项与像素个数（图像大小）成正比，而第一个式子是与像素个数的平方成正比！这里每个 Windows 之间互相不重叠。</p>
<p>另一点是，虽然这里是在 Windows 范围内计算 Self Attention，但是对于计算Multi Head的方式计算并不影响，所以还是可以实现MultiHead Windows based Self Attention。<code>WindowAttention</code>类的实现如下。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">WindowAttention</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#d14">r</span><span style="color:#d14">&#34;&#34;&#34; Window based multi-head self attention (W-MSA) module with relative position bias.
</span><span style="color:#d14">    It supports both of shifted and non-shifted window.
</span><span style="color:#d14">
</span><span style="color:#d14">    Args:
</span><span style="color:#d14">        dim (int): Number of input channels.
</span><span style="color:#d14">        window_size (tuple[int]): The height and width of the window.
</span><span style="color:#d14">        num_heads (int): Number of attention heads.
</span><span style="color:#d14">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
</span><span style="color:#d14">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
</span><span style="color:#d14">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
</span><span style="color:#d14">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
</span><span style="color:#d14">    &#34;&#34;&#34;</span>

    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, dim, window_size, num_heads, qkv_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>, qk_scale<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>, attn_drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>, proj_drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>):

        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dim <span style="color:#000;font-weight:bold">=</span> dim
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">=</span> window_size  <span style="color:#998;font-style:italic"># Wh, Ww</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads <span style="color:#000;font-weight:bold">=</span> num_heads
        head_dim <span style="color:#000;font-weight:bold">=</span> dim <span style="color:#000;font-weight:bold">//</span> num_heads
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>scale <span style="color:#000;font-weight:bold">=</span> qk_scale <span style="color:#000;font-weight:bold">or</span> head_dim <span style="color:#000;font-weight:bold">**</span> <span style="color:#000;font-weight:bold">-</span><span style="color:#099">0.5</span>

        <span style="color:#998;font-style:italic"># define a parameter table of relative position bias</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>relative_position_bias_table <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Parameter(
            torch<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> (<span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>), num_heads))  <span style="color:#998;font-style:italic"># 2*Wh-1 * 2*Ww-1, nH</span>

        <span style="color:#998;font-style:italic"># get pair-wise relative position index for each token inside the window</span>
        coords_h <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>])
        coords_w <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>])
        coords <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>stack(torch<span style="color:#000;font-weight:bold">.</span>meshgrid([coords_h, coords_w]))  <span style="color:#998;font-style:italic"># 2, Wh, Ww</span>
        coords_flatten <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>flatten(coords, <span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># 2, Wh*Ww</span>
        relative_coords <span style="color:#000;font-weight:bold">=</span> coords_flatten[:, :, <span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">-</span> coords_flatten[:, <span style="color:#999">None</span>, :]  <span style="color:#998;font-style:italic"># 2, Wh*Ww, Wh*Ww</span>
        relative_coords <span style="color:#000;font-weight:bold">=</span> relative_coords<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">1</span>, <span style="color:#099">2</span>, <span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>contiguous()  <span style="color:#998;font-style:italic"># Wh*Ww, Wh*Ww, 2</span>
        relative_coords[:, :, <span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">+=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>  <span style="color:#998;font-style:italic"># shift to start from 0</span>
        relative_coords[:, :, <span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">+=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
        relative_coords[:, :, <span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*=</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
        relative_position_index <span style="color:#000;font-weight:bold">=</span> relative_coords<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># Wh*Ww, Wh*Ww</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>register_buffer(<span style="color:#d14">&#34;relative_position_index&#34;</span>, relative_position_index)

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>qkv <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Linear(dim, dim <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">3</span>, bias<span style="color:#000;font-weight:bold">=</span>qkv_bias)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attn_drop <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dropout(attn_drop)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>proj <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Linear(dim, dim)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>proj_drop <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dropout(proj_drop)

        trunc_normal_(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>relative_position_bias_table, std<span style="color:#000;font-weight:bold">=.</span><span style="color:#099">02</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>softmax <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Softmax(dim<span style="color:#000;font-weight:bold">=-</span><span style="color:#099">1</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x, mask<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>):
        <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">        Args:
</span><span style="color:#d14">            x: input features with shape of (num_windows*B, N, C)
</span><span style="color:#d14">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
</span><span style="color:#d14">        &#34;&#34;&#34;</span>
        B_, N, C <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape
        <span style="color:#998;font-style:italic"># B_ = num_windows * B</span>
        <span style="color:#998;font-style:italic"># (B_, num_heads, N, C // num_heads), N = Wh * Ww</span>
        qkv <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>qkv(x)<span style="color:#000;font-weight:bold">.</span>reshape(B_, N, <span style="color:#099">3</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads, C <span style="color:#000;font-weight:bold">//</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads)<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">2</span>, <span style="color:#099">0</span>, <span style="color:#099">3</span>, <span style="color:#099">1</span>, <span style="color:#099">4</span>)
        q, k, v <span style="color:#000;font-weight:bold">=</span> qkv[<span style="color:#099">0</span>], qkv[<span style="color:#099">1</span>], qkv[<span style="color:#099">2</span>]  <span style="color:#998;font-style:italic"># make torchscript happy (cannot use tensor as tuple)</span>

        q <span style="color:#000;font-weight:bold">=</span> q <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>scale
        attn <span style="color:#000;font-weight:bold">=</span> (q <span style="color:#a61717;background-color:#e3d2d2">@</span> k<span style="color:#000;font-weight:bold">.</span>transpose(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">2</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>))    <span style="color:#998;font-style:italic"># (B_, num_heads, N, N), N = Wh * Ww</span>

        relative_position_bias <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>relative_position_bias_table[<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>relative_position_index<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)]<span style="color:#000;font-weight:bold">.</span>view(
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>], <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>], <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># Wh*Ww,Wh*Ww,nH</span>
        <span style="color:#998;font-style:italic"># nH = num_heads</span>
        relative_position_bias <span style="color:#000;font-weight:bold">=</span> relative_position_bias<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">2</span>, <span style="color:#099">0</span>, <span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>contiguous()  <span style="color:#998;font-style:italic"># nH, Wh*Ww, Wh*Ww</span>
        attn <span style="color:#000;font-weight:bold">=</span> attn <span style="color:#000;font-weight:bold">+</span> relative_position_bias<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">0</span>)

        <span style="color:#000;font-weight:bold">if</span> mask <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            nW <span style="color:#000;font-weight:bold">=</span> mask<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>]
            attn <span style="color:#000;font-weight:bold">=</span> attn<span style="color:#000;font-weight:bold">.</span>view(B_ <span style="color:#000;font-weight:bold">//</span> nW, nW, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads, N, N) <span style="color:#000;font-weight:bold">+</span> mask<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">0</span>)
            attn <span style="color:#000;font-weight:bold">=</span> attn<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads, N, N)
            attn <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>softmax(attn)
        <span style="color:#000;font-weight:bold">else</span>:
            attn <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>softmax(attn)

        attn <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attn_drop(attn)

        x <span style="color:#000;font-weight:bold">=</span> (attn <span style="color:#a61717;background-color:#e3d2d2">@</span> v)<span style="color:#000;font-weight:bold">.</span>transpose(<span style="color:#099">1</span>, <span style="color:#099">2</span>)<span style="color:#000;font-weight:bold">.</span>reshape(B_, N, C)
        x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>proj(x)
        x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>proj_drop(x)
        <span style="color:#000;font-weight:bold">return</span> x
</code></pre></div><h3 id="shifted-windows-based-self-attention">Shifted Windows based Self-Attention</h3>
<p>主要是提高不同 window 之间的信息交互程度。</p>
<p>正常的 attention mask的输入尺寸是:<code>[bs, seq_len]</code>，然后被扩展到 <code>[bs, 1, 1, seq_len]</code>，其中第二维对应的是 head 维，第三维对应的是batch内当前样本的Token输入。</p>
<p>使用<code>[PAD]</code>表示仅用于拼接的无效Token，然后一个正常的输入 Token 系列是：<code>[我][是][谁][PAD][PAD][PAD]</code>等，也就是<code>seq_len=6</code>，对应的 <code>attention_mask=[1, 1, 1, 0, 0, 0]</code>，并假设每个 Token 对应的维度是128，则计算 Attention Score 的结果（相似度得分）如下：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">我-我       我-是       我-谁       我-PAD      我-PAD      我-PAD
是-我       是-是       是-谁       是-PAD      是-PAD      是-PAD
谁-我       谁-是       谁-谁       谁-PAD      谁-PAD      谁-PAD
PAD-PAD     PAD-PAD       PAD-PAD       PAD-PAD      PAD-PAD      PAD-PAD
PAD-PAD     PAD-PAD       PAD-PAD       PAD-PAD      PAD-PAD      PAD-PAD
PAD-PAD     PAD-PAD       PAD-PAD       PAD-PAD      PAD-PAD      PAD-PAD
</code></pre></div><p>然后对应的 Attention Mask 就是：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">1   1   1   0   0   0
1   1   1   0   0   0
1   1   1   0   0   0
1   1   1   0   0   0
1   1   1   0   0   0
1   1   1   0   0   0
</code></pre></div><p>所以与Attention Score矩阵求和时，只有前三个字计算 Softmax 权重，后面三个 PAD 的权重都非常小；但是这里也有一个问题，就是 Attention Score矩阵的下面三行以及左边三列对应的Softmax权重也不是一个非常小的数，所以下一层的时候，对应上一层 PAD 位置的输出就包含了上一层中的<code>我是谁</code>三个字的信息了。这会不会造成什么污染呢？有待验证。</p>
<p>图-2展示了 Shifted Window 前后 Windows 范围的比较，论文中选在向Top-Left方向进行平移，借助<code>torch.roll(+-)</code>来实现以及复原。需要指出的，平移之后，原本在左上角的Patch会移动到右下角，与正常 Feature Map的右下角的 Patch 变成相邻的，但是此时虽然相邻，但是不能将它们按照正常 Windows based Self Attention 进行计算，而应该分开计算，即原来相邻的 Patch 计算 Attention Score，不相邻的Patch之间不能计算 Attention Score。对应下图，其中A, C, D, E, F, G几个区域都应该单独计算 Attention Score。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin3.png" alt="图 - 5 Shifted Windows分区示意图">
    <figcaption>图 - 5 Shifted Windows分区示意图</figcaption>
    </center>
</figure></p>
<p>这导致Windows的个数由正常的$\lceil \frac{H}{M} \rceil \times \lceil \frac{W}{M} \rceil$ 变成 $(\lceil \frac{H}{M} \rceil + 1) \times (\lceil \frac{W}{M} \rceil + 1)$，这一点虽然看上增加不多，但是当$\lceil \frac{H}{M} \rceil = 2$时，由2 变为3，则相当于计算量增加了2.25倍，所以有必要对一点进行优化。</p>
<p>作者提出使用Batch computation for shifted configuration计算 Self Attention，但当前的代码前提是所有输入图片的尺寸是不一致的。主要思想就是配合 Attention Mask 来保证只关注自己区域内的 Patch。代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
            <span style="color:#998;font-style:italic"># calculate attention mask for SW-MSA</span>
            H, W <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution
            img_mask <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">1</span>, H, W, <span style="color:#099">1</span>))  <span style="color:#998;font-style:italic"># 1 H W 1</span>
            h_slices <span style="color:#000;font-weight:bold">=</span> (<span style="color:#0086b3">slice</span>(<span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size),    <span style="color:#998;font-style:italic"># start = 0, stop = -window_size, step=None</span>
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#999">None</span>))
            w_slices <span style="color:#000;font-weight:bold">=</span> (<span style="color:#0086b3">slice</span>(<span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#999">None</span>))
            cnt <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
            <span style="color:#000;font-weight:bold">for</span> h <span style="color:#000;font-weight:bold">in</span> h_slices:
                <span style="color:#000;font-weight:bold">for</span> w <span style="color:#000;font-weight:bold">in</span> w_slices:
                    img_mask[:, h, w, :] <span style="color:#000;font-weight:bold">=</span> cnt
                    cnt <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>

            mask_windows <span style="color:#000;font-weight:bold">=</span> window_partition(img_mask, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size)  <span style="color:#998;font-style:italic"># nW, window_size, window_size, 1</span>
            mask_windows <span style="color:#000;font-weight:bold">=</span> mask_windows<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size)
            attn_mask <span style="color:#000;font-weight:bold">=</span> mask_windows<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">-</span> mask_windows<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)
            attn_mask <span style="color:#000;font-weight:bold">=</span> attn_mask<span style="color:#000;font-weight:bold">.</span>masked_fill(attn_mask <span style="color:#000;font-weight:bold">!=</span> <span style="color:#099">0</span>, <span style="color:#0086b3">float</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">100.0</span>))<span style="color:#000;font-weight:bold">.</span>masked_fill(attn_mask <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>, <span style="color:#0086b3">float</span>(<span style="color:#099">0.0</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            attn_mask <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
</code></pre></td></tr></table>
</div>
</div><p>这里主要思路就是将属于同一个区域内的 Patch 赋值一个相同的标记<code>cnt</code>，然后同一个Windows的Patch的标记与其它patch的标记相减以后，相同 Windows 内对应的数值就为0，而不为0就意味着需要Masked掉对应位置的Attention Score的计算，也就是赋值为-100.0。对于<code>window_partion()</code>函数的实现见下面具体的代码实现。</p>
<h3 id="relative-position-bias">Relative position bias</h3>
<p>SwinT使用Windows内的相对位置编码来学习位置信息。</p>
<p>$$\textrm{Attention}(Q, K, V) = \textrm{SoftMax}(QK^T/\sqrt{d} + B) V$$</p>
<p>其中，$Q, K, V \in \mathbb{R}^{M^2 \times d}$，由于所有的 Stage 中相对位置的取值范围是：$[-M + 1, M - 1]$，所以作者让 B 的取值来自于$\hat{B} \in \mathbb{R}^{(2M-1) \times (2M-1)}$，这样对于Windows内任意两个Patch之间都会有一个相对位置编码向量进行表示(而且还考虑了分axis)！注意，每个 Swin Transformer Block层的 B 取值是不同的。</p>
<p>这里最主要的代码逻辑是要将所有的相对位置$[(-M + 1, -M+1) \times (M-1, M-1)]$映射到$[0, (2M - 1) * (2M - 1)]$范围内唯一的索引。对应代码实现，就是将每个元素的用二维坐标进行编码，然后将二维坐标映射到一维数字。映射成二维坐标是通过三行代码实现的：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    coords_h <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>])
    coords_w <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>])
    <span style="color:#998;font-style:italic"># [0, ...] 表示行索引，[1, ...]表示列索引</span>
    coords <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>stack(torch<span style="color:#000;font-weight:bold">.</span>meshgrid([coords_h, coords_w]))  <span style="color:#998;font-style:italic"># 2, Wh, Ww</span>
</code></pre></div><p>然后是基于二维坐标计算相对位置，这里相对位置的计算也是分成行、列分别进行计算。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    coords_flatten <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>flatten(coords, <span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># 2, Wh*Ww</span>
    relative_coords <span style="color:#000;font-weight:bold">=</span> coords_flatten[:, :, <span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">-</span> coords_flatten[:, <span style="color:#999">None</span>, :]  <span style="color:#998;font-style:italic"># 2, Wh*Ww, Wh*Ww</span>
    relative_coords <span style="color:#000;font-weight:bold">=</span> relative_coords<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">1</span>, <span style="color:#099">2</span>, <span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>contiguous()  <span style="color:#998;font-style:italic"># Wh*Ww, Wh*Ww, 2</span>
</code></pre></div><p>现在<code>relative_coords</code>的最后两维分别表示行、列的相对位置，取值范围都是：$[-M + 1, M - 1]$。</p>
<p>然后就是将二维相对位置信息映射到一维索引，首先是将相对位置的取值范围变为：$[0, 2M - 2]$，这一步通过加上$M - 1$实现，然后就是将行相对位置 * 宽度 + 列相对位置，这里宽度是$2M - 1$，映射后的一维索引取值范围是：$[0, (2M - 1) \times (2M - 1)$。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    relative_coords[:, :, <span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">+=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>  <span style="color:#998;font-style:italic"># shift to start from 0</span>
    relative_coords[:, :, <span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">+=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
    relative_coords[:, :, <span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*=</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
    relative_position_index <span style="color:#000;font-weight:bold">=</span> relative_coords<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># Wh*Ww, Wh*Ww</span>
    <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>register_buffer(<span style="color:#d14">&#34;relative_position_index&#34;</span>, relative_position_index)
</code></pre></div><p>上述得到相对位置的一维索引后，需要根据一个 Table 来获取对应的相对位置编码，也就是从 $\hat{B}$ 中获取，定义如下：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>relative_position_bias_table <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Parameter(
    torch<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> window_size[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> (<span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> window_size[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>), num_heads))  <span style="color:#998;font-style:italic"># 2*Wh-1 * 2*Ww-1, nH</span>
</code></pre></div><p>至此就是所有的Swin Transformer的算法结构了。</p>
<h2 id="代码实现与一些细节">代码实现与一些细节</h2>
<ul>
<li>drop rate: 0.0</li>
<li>atten drop rate: 0.0</li>
<li>drop path rate: 0.1</li>
</ul>
<p>然后 Multi-Head Self Attention中Output Mlp用到的 drop rate 与 FFN 中用到的 drop rate 是同一个，atten drop rate 只用于Self Attention中对 Attention Mask进行处理。FFN中的Dropout的位置即结构如下: Linear + Act + Dropout + Linear + Dropout。</p>
<p>Patch Merge 层的位置，论文中这个层被放在每个 Stage 开始的位置，但是实际代码里是被放在每个Stage最后一层的。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">BasicLayer</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio<span style="color:#000;font-weight:bold">=</span><span style="color:#099">4.</span>, qkv_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>, qk_scale<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>, drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>, attn_drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>,
                 drop_path<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>, norm_layer<span style="color:#000;font-weight:bold">=</span>nn<span style="color:#000;font-weight:bold">.</span>LayerNorm, downsample<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>, use_checkpoint<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>):

        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dim <span style="color:#000;font-weight:bold">=</span> dim
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution <span style="color:#000;font-weight:bold">=</span> input_resolution
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>depth <span style="color:#000;font-weight:bold">=</span> depth
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>use_checkpoint <span style="color:#000;font-weight:bold">=</span> use_checkpoint

        <span style="color:#998;font-style:italic"># build blocks</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blocks <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ModuleList([
            SwinTransformerBlock(dim<span style="color:#000;font-weight:bold">=</span>dim, input_resolution<span style="color:#000;font-weight:bold">=</span>input_resolution,
                                 num_heads<span style="color:#000;font-weight:bold">=</span>num_heads, window_size<span style="color:#000;font-weight:bold">=</span>window_size,
                                 shift_size<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span> <span style="color:#000;font-weight:bold">if</span> (i <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>) <span style="color:#000;font-weight:bold">else</span> window_size <span style="color:#000;font-weight:bold">//</span> <span style="color:#099">2</span>,
                                 mlp_ratio<span style="color:#000;font-weight:bold">=</span>mlp_ratio,
                                 qkv_bias<span style="color:#000;font-weight:bold">=</span>qkv_bias, qk_scale<span style="color:#000;font-weight:bold">=</span>qk_scale,
                                 drop<span style="color:#000;font-weight:bold">=</span>drop, attn_drop<span style="color:#000;font-weight:bold">=</span>attn_drop,
                                 drop_path<span style="color:#000;font-weight:bold">=</span>drop_path[i] <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">isinstance</span>(drop_path, <span style="color:#0086b3">list</span>) <span style="color:#000;font-weight:bold">else</span> drop_path,
                                 norm_layer<span style="color:#000;font-weight:bold">=</span>norm_layer)
            <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(depth)])

        <span style="color:#998;font-style:italic"># patch merging layer</span>
        <span style="color:#000;font-weight:bold">if</span> downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> downsample(input_resolution, dim<span style="color:#000;font-weight:bold">=</span>dim, norm_layer<span style="color:#000;font-weight:bold">=</span>norm_layer)
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        <span style="color:#000;font-weight:bold">for</span> blk <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blocks:
            <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>use_checkpoint:
                x <span style="color:#000;font-weight:bold">=</span> checkpoint<span style="color:#000;font-weight:bold">.</span>checkpoint(blk, x)
            <span style="color:#000;font-weight:bold">else</span>:
                x <span style="color:#000;font-weight:bold">=</span> blk(x)
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample(x)
        <span style="color:#000;font-weight:bold">return</span> x

</code></pre></td></tr></table>
</div>
</div><h3 id="patchmerging">PatchMerging</h3>
<p>可以看出，后续每个Patch的Merge过程就是取出 x0, x1, x2, x3 四个矩阵，然后拼接起来，送入一个<code>LayerNorm + Linear</code>层，后面的 Linear 层将channel维度从 4C 映射到 2C，也就是每个 Stage 下采样一倍，然后特征维度也只增加一倍。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">PatchMerging</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, input_resolution, dim, norm_layer<span style="color:#000;font-weight:bold">=</span>nn<span style="color:#000;font-weight:bold">.</span>LayerNorm):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution <span style="color:#000;font-weight:bold">=</span> input_resolution
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dim <span style="color:#000;font-weight:bold">=</span> dim
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>reduction <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Linear(<span style="color:#099">4</span> <span style="color:#000;font-weight:bold">*</span> dim, <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">*</span> dim, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm <span style="color:#000;font-weight:bold">=</span> norm_layer(<span style="color:#099">4</span> <span style="color:#000;font-weight:bold">*</span> dim)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">        x: B, H*W, C
</span><span style="color:#d14">        &#34;&#34;&#34;</span>
        H, W <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution
        B, L, C <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape
        <span style="color:#000;font-weight:bold">assert</span> L <span style="color:#000;font-weight:bold">==</span> H <span style="color:#000;font-weight:bold">*</span> W, <span style="color:#d14">&#34;input feature has wrong size&#34;</span>
        <span style="color:#000;font-weight:bold">assert</span> H <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span> <span style="color:#000;font-weight:bold">and</span> W <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">2</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>, f<span style="color:#d14">&#34;x size ({H}*{W}) are not even.&#34;</span>

        x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>view(B, H, W, C)

        x0 <span style="color:#000;font-weight:bold">=</span> x[:, <span style="color:#099">0</span>::<span style="color:#099">2</span>, <span style="color:#099">0</span>::<span style="color:#099">2</span>, :]  <span style="color:#998;font-style:italic"># B H/2 W/2 C</span>
        x1 <span style="color:#000;font-weight:bold">=</span> x[:, <span style="color:#099">1</span>::<span style="color:#099">2</span>, <span style="color:#099">0</span>::<span style="color:#099">2</span>, :]  <span style="color:#998;font-style:italic"># B H/2 W/2 C</span>
        x2 <span style="color:#000;font-weight:bold">=</span> x[:, <span style="color:#099">0</span>::<span style="color:#099">2</span>, <span style="color:#099">1</span>::<span style="color:#099">2</span>, :]  <span style="color:#998;font-style:italic"># B H/2 W/2 C</span>
        x3 <span style="color:#000;font-weight:bold">=</span> x[:, <span style="color:#099">1</span>::<span style="color:#099">2</span>, <span style="color:#099">1</span>::<span style="color:#099">2</span>, :]  <span style="color:#998;font-style:italic"># B H/2 W/2 C</span>
        x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>cat([x0, x1, x2, x3], <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># B H/2 W/2 4*C</span>
        x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>view(B, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">4</span> <span style="color:#000;font-weight:bold">*</span> C)  <span style="color:#998;font-style:italic"># B H/2*W/2 4*C</span>

        x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm(x)
        x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>reduction(x)

        <span style="color:#000;font-weight:bold">return</span> x
</code></pre></td></tr></table>
</div>
</div><h3 id="swintransformerblock">SwinTransformerBlock</h3>
<p>基于上面提到的 Shifted Windows based Self Attention 中 Mask 的分析以及 window partion以及WindowAttention等函数/类的实现，现在可以给出 Swin Transormer Block 的完整实现了。</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">SwinTransformerBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#d14">r</span><span style="color:#d14">&#34;&#34;&#34; Swin Transformer Block.
</span><span style="color:#d14">
</span><span style="color:#d14">    Args:
</span><span style="color:#d14">        dim (int): Number of input channels.
</span><span style="color:#d14">        input_resolution (tuple[int]): Input resulotion.
</span><span style="color:#d14">        num_heads (int): Number of attention heads.
</span><span style="color:#d14">        window_size (int): Window size.
</span><span style="color:#d14">        shift_size (int): Shift size for SW-MSA.
</span><span style="color:#d14">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
</span><span style="color:#d14">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
</span><span style="color:#d14">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
</span><span style="color:#d14">        drop (float, optional): Dropout rate. Default: 0.0
</span><span style="color:#d14">        attn_drop (float, optional): Attention dropout rate. Default: 0.0
</span><span style="color:#d14">        drop_path (float, optional): Stochastic depth rate. Default: 0.0
</span><span style="color:#d14">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
</span><span style="color:#d14">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
</span><span style="color:#d14">    &#34;&#34;&#34;</span>

    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, dim, input_resolution, num_heads, window_size<span style="color:#000;font-weight:bold">=</span><span style="color:#099">7</span>, shift_size<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>,
                 mlp_ratio<span style="color:#000;font-weight:bold">=</span><span style="color:#099">4.</span>, qkv_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>, qk_scale<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>, drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>, attn_drop<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>, drop_path<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.</span>,
                 act_layer<span style="color:#000;font-weight:bold">=</span>nn<span style="color:#000;font-weight:bold">.</span>GELU, norm_layer<span style="color:#000;font-weight:bold">=</span>nn<span style="color:#000;font-weight:bold">.</span>LayerNorm):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dim <span style="color:#000;font-weight:bold">=</span> dim
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution <span style="color:#000;font-weight:bold">=</span> input_resolution
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_heads <span style="color:#000;font-weight:bold">=</span> num_heads
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">=</span> window_size
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">=</span> shift_size
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mlp_ratio <span style="color:#000;font-weight:bold">=</span> mlp_ratio
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">min</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution) <span style="color:#000;font-weight:bold">&lt;=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size:
            <span style="color:#998;font-style:italic"># if window size is larger than input resolution, we don&#39;t partition windows</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">min</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution)
        <span style="color:#000;font-weight:bold">assert</span> <span style="color:#099">0</span> <span style="color:#000;font-weight:bold">&lt;=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#d14">&#34;shift_size must in 0-window_size&#34;</span>

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm1 <span style="color:#000;font-weight:bold">=</span> norm_layer(dim)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attn <span style="color:#000;font-weight:bold">=</span> WindowAttention(
            dim, window_size<span style="color:#000;font-weight:bold">=</span>to_2tuple(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size), num_heads<span style="color:#000;font-weight:bold">=</span>num_heads,
            qkv_bias<span style="color:#000;font-weight:bold">=</span>qkv_bias, qk_scale<span style="color:#000;font-weight:bold">=</span>qk_scale, attn_drop<span style="color:#000;font-weight:bold">=</span>attn_drop, proj_drop<span style="color:#000;font-weight:bold">=</span>drop)

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>drop_path <span style="color:#000;font-weight:bold">=</span> DropPath(drop_path) <span style="color:#000;font-weight:bold">if</span> drop_path <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0.</span> <span style="color:#000;font-weight:bold">else</span> nn<span style="color:#000;font-weight:bold">.</span>Identity()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm2 <span style="color:#000;font-weight:bold">=</span> norm_layer(dim)
        mlp_hidden_dim <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">int</span>(dim <span style="color:#000;font-weight:bold">*</span> mlp_ratio)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mlp <span style="color:#000;font-weight:bold">=</span> Mlp(in_features<span style="color:#000;font-weight:bold">=</span>dim, hidden_features<span style="color:#000;font-weight:bold">=</span>mlp_hidden_dim, act_layer<span style="color:#000;font-weight:bold">=</span>act_layer, drop<span style="color:#000;font-weight:bold">=</span>drop)

        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
            <span style="color:#998;font-style:italic"># calculate attention mask for SW-MSA</span>
            H, W <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution
            img_mask <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">1</span>, H, W, <span style="color:#099">1</span>))  <span style="color:#998;font-style:italic"># 1 H W 1</span>
            h_slices <span style="color:#000;font-weight:bold">=</span> (<span style="color:#0086b3">slice</span>(<span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size),    <span style="color:#998;font-style:italic"># start = 0, stop = -window_size, step=None</span>
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#999">None</span>))
            w_slices <span style="color:#000;font-weight:bold">=</span> (<span style="color:#0086b3">slice</span>(<span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size),
                        <span style="color:#0086b3">slice</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#999">None</span>))
            cnt <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
            <span style="color:#000;font-weight:bold">for</span> h <span style="color:#000;font-weight:bold">in</span> h_slices:
                <span style="color:#000;font-weight:bold">for</span> w <span style="color:#000;font-weight:bold">in</span> w_slices:
                    img_mask[:, h, w, :] <span style="color:#000;font-weight:bold">=</span> cnt
                    cnt <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>

            mask_windows <span style="color:#000;font-weight:bold">=</span> window_partition(img_mask, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size)  <span style="color:#998;font-style:italic"># nW, window_size, window_size, 1</span>
            mask_windows <span style="color:#000;font-weight:bold">=</span> mask_windows<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size)
            attn_mask <span style="color:#000;font-weight:bold">=</span> mask_windows<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">-</span> mask_windows<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)
            attn_mask <span style="color:#000;font-weight:bold">=</span> attn_mask<span style="color:#000;font-weight:bold">.</span>masked_fill(attn_mask <span style="color:#000;font-weight:bold">!=</span> <span style="color:#099">0</span>, <span style="color:#0086b3">float</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">100.0</span>))<span style="color:#000;font-weight:bold">.</span>masked_fill(attn_mask <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>, <span style="color:#0086b3">float</span>(<span style="color:#099">0.0</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            attn_mask <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>register_buffer(<span style="color:#d14">&#34;attn_mask&#34;</span>, attn_mask)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        H, W <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>input_resolution
        B, L, C <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape
        <span style="color:#000;font-weight:bold">assert</span> L <span style="color:#000;font-weight:bold">==</span> H <span style="color:#000;font-weight:bold">*</span> W, <span style="color:#d14">&#34;input feature has wrong size&#34;</span>

        shortcut <span style="color:#000;font-weight:bold">=</span> x
        x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm1(x)
        x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>view(B, H, W, C)

        <span style="color:#998;font-style:italic"># cyclic shift</span>
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
            shifted_x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>roll(x, shifts<span style="color:#000;font-weight:bold">=</span>(<span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#000;font-weight:bold">-</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size), dims<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">1</span>, <span style="color:#099">2</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            shifted_x <span style="color:#000;font-weight:bold">=</span> x

        <span style="color:#998;font-style:italic"># partition windows</span>
        x_windows <span style="color:#000;font-weight:bold">=</span> window_partition(shifted_x, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size)  <span style="color:#998;font-style:italic"># nW*B, window_size, window_size, C</span>
        x_windows <span style="color:#000;font-weight:bold">=</span> x_windows<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, C)  <span style="color:#998;font-style:italic"># nW*B, window_size*window_size, C</span>

        <span style="color:#998;font-style:italic"># W-MSA/SW-MSA</span>
        attn_windows <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attn(x_windows, mask<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attn_mask)  <span style="color:#998;font-style:italic"># nW*B, window_size*window_size, C</span>

        <span style="color:#998;font-style:italic"># merge windows</span>
        attn_windows <span style="color:#000;font-weight:bold">=</span> attn_windows<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, C)
        shifted_x <span style="color:#000;font-weight:bold">=</span> window_reverse(attn_windows, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>window_size, H, W)  <span style="color:#998;font-style:italic"># B H&#39; W&#39; C</span>

        <span style="color:#998;font-style:italic"># reverse cyclic shift</span>
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
            x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>roll(shifted_x, shifts<span style="color:#000;font-weight:bold">=</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>shift_size), dims<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">1</span>, <span style="color:#099">2</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            x <span style="color:#000;font-weight:bold">=</span> shifted_x
        x <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>view(B, H <span style="color:#000;font-weight:bold">*</span> W, C)

        <span style="color:#998;font-style:italic"># FFN</span>
        x <span style="color:#000;font-weight:bold">=</span> shortcut <span style="color:#000;font-weight:bold">+</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>drop_path(x)
        x <span style="color:#000;font-weight:bold">=</span> x <span style="color:#000;font-weight:bold">+</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>drop_path(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>mlp(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>norm2(x)))

        <span style="color:#000;font-weight:bold">return</span> x
</code></pre></div><h2 id="实验结果">实验结果</h2>
<p>Swin Transformer 在 分类、识别、分割几个任务上都获得了提高，尤其是识别、分割提升显著。在训练分类时，数据增广与 ViT 同，且训练300 epochs，warmup epochs为20。</p>
<p>ImageNet上的性能表现。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin4.png" alt="图 - 6 SwinT在ImageNet上的性能对比">
    <figcaption>图 - 6 SwinT在ImageNet上的性能对比</figcaption>
    </center>
</figure></p>
<p>COCO上的性能表现。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin5.png" alt="图 - 7 SwinT在COCO上识别性能对比">
    <figcaption>图 - 7 SwinT在COCO上识别性能对比</figcaption>
    </center>
</figure></p>
<p>分割性能表现。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin6.png" alt="图 - 8 SwinT在分割任务上的性能对比">
    <figcaption>图 - 8 SwinT在分割任务上的性能对比</figcaption>
    </center>
</figure></p>
<p>不同位置编码的影响。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin7.png" alt="图 - 9 SwinT中不同位置编码对精度的影响">
    <figcaption>图 - 9 SwinT中不同位置编码对精度的影响</figcaption>
    </center>
</figure></p>
<p>速度对比，GPU为V100。</p>
<p><figure>
    <center>
    <img src="/imgs/swin-transformer/swin8.png" alt="图 - 10 SwinT速度对比">
    <figcaption>图 - 10 SwinT速度对比</figcaption>
    </center>
</figure></p>
        
            <p>对PGD算法的改进，包括FreeAT, FreeLB, SMART等。</p>
<p>博客<a href="/content/posts/adversarial-training.md">Adversarial Trainig</a>里面的PGD模型在对抗样本训练中可以起到很好的效果，但是缺点在于求解最优扰动时需要进行迭代，比如迭代K次，则意味着训练耗时增加K + 1倍。本文提到的几个算法主要就是针对这个问题进行优化，包括 FreeAT, FreeLB, SMART 等算法。</p>
<h2 id="freeat">FreeAT</h2>
<p>首先在PGD算法中，为了得到$\max_{\delta \in \mathcal{S}} L(\theta, x + \delta, y)$，需要对 adversarial samples 进行迭代生成，$x^{t+1} = \prod_{x + \mathcal{S}} (x^t + \alpha \mathrm{sign} \nabla_x L(\theta, x, y))$。FreeAT主要是为了降低后面迭代求解最优Adversarial Samples的过程。</p>
<p>对于一个 PGD-K 算法而言，每一个batch的训练中，需要计算K次前向-后向，每次前向-后向会更新一下最新的扰动数值，这个扰动数值也会累加，然后使用第K次的adversarial examples计算$L_{adv}$以及 第 K + 1 次前向计算 $L$，最终反向传播一次$L + L_{adv}$。</p>
<p>而对于FreeAT算法，在K次前向-后向计算计算最优扰动项的时候，也会使用$L$进行一次前向-后向传播，并且总的 Epoch 数降低为$ N / K$，N 为正常训练时需要的 epoch 个数。与PGD-K的区别是，每次计算扰动项的时候，模型的参数也会更新，同时降低总的训练epoch数，保证总的iteration数不变。在训练下一个 Batch 的时候，上一个Batch最新的扰动数值作为当前Batch的初始扰动数值。</p>
<p>为了避免参数遗忘，这里的 $FreeAT-K$ 中的K不能太大。</p>
<p>最终，FreeAT 的算法如下。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freeat0.png" alt="图 - 1 FreeAT算法伪代码">
    <figcaption>图 - 1 FreeAT算法伪代码</figcaption>
    </center>
</figure></p>
<p>模型的超参数$m$（也就是上文的$K$）是重要的超参数，图-2展示了在 CIFAR-100数据集上的影响。可见随着 $m$ 的增加，精度会下降，但当$m&lt;10$的时候对精度影响都比较小，而且别PGD-7的效果都好，可以作为备选范围。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freeat1.png" alt="图 - 2 FreeAT算法中m的影响">
    <figcaption>图 - 2 FreeAT算法中m的影响</figcaption>
    </center>
</figure></p>
<p>在ResNet-50 + ImageNet 的配置下，当$m=4$的时候效果最好，这一切都是不像PGD那样增加训练计算量的前提下实现的。</p>
<h3 id="gradient-masking">Gradient Masking</h3>
<p>Gradient Masking 的意思是指模型的输出对输入的梯度趋近于零，所以当输入发生微小变化时，不会影响模型的输出，从而实现鲁棒性。但是这种方法并没有真正的提高模型的鲁棒性，因为考虑到对抗样本的 transferability，换成另一个模型，这个模型对输入的梯度不接近于零，导致用这个模型生成的对抗样本导致目前正在训练的模型还是会预测错误。如下图所示，</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/gradientmasking0.png" alt="图 - 3 Gradient Masking导致的后果">
    <figcaption>图 - 3 Gradient Masking导致的后果</figcaption>
    </center>
</figure></p>
<p>其中，(a) 中的模型对与输入 $x$ 附近的梯度已经为0了，所以此时对 $x$ 的扰动有一定的鲁棒性，但是当使用另一个模型 (b) 对 $x$ 的梯度来生成对抗样本 $x^*$ 时，基于transferability，这个对抗样本对模型 (a) 仍然是有效的。所以当发生 Gradient Masking 时，模型并非是真正的鲁棒。</p>
<p>Label smoothing 一定程度上也可以提高对抗样本效果，因为知识蒸馏之类的Loss可以让学习到的模型更平滑，也就是对输入更不敏感。</p>
<p>更多的可以参考：<a href="https://arxiv.org/pdf/1611.03814.pdf">SoK: Towards the Science of Security and Privacy in Machine Learning</a></p>
<h2 id="freelb">FreeLB</h2>
<p>首先，FreeAT那种在每次更新扰动项$\delta$的时候都会更新模型的权重（梯度下降），这会导致<code>stale gradient</code>的发生，也就是对于第 $t$ 步，扰动的更新不是最大化模型在 $t$ 时刻的参数$\theta_t$，而是基于下式$\nabla_{\delta} L(f_{\theta_{t-1}}(x + \delta_{t-1}), y)$计算得来的(这个梯度计算公式还是以PGD那里的为准)。</p>
<p>FreeLB算法其实对计算量并没有减少，主要是提出了另一种梯度更新过程。在PGD-K的K次前向-后向计算用于构造adversarial examples时，FreeLB会累加每次后向传播中模型参数的，最后使用这个累加的梯度更新模型的参数。总的来说，前向-后向次数由K + 1次变为 K 次。另一个好处是，这个累加的梯度可以包含更多扰动的信息，可以认为每次模型的梯度更新都使用了更大的Batch的样本计算得到，即$x+\delta_0, \ldots, x + \delta_{K-1}$，而PGD算法只能最小化$x+\delta_{k-1}$位置的扰动损失，理论认为这会比PGD得到更好的泛化性能。</p>
<p>上述改动等价于在两个高维球里求解最优的扰动：</p>
<p>$$\mathcal{I}_ t=\mathcal{B}_{x+\delta_0}(\alpha t) \cap \mathcal{B}_{x}(\epsilon)$$</p>
<p>其中$\mathcal{B}_x(\epsilon)$表示半径为$\epsilon$的球。而通过梯度累加移动平均，则等价于优化下面的损失函数。</p>
<p>$$\min_\theta \mathbb{E}_{(z, y) \sim \mathcal{D}} \left[  \frac{1}{K} \sum_{t=0}^{K-1} \max_{\delta_t \in \mathcal{I}_t} L(f_\theta(x + \delta_t), y) \right]$$</p>
<p>使用FreeLB算法需要特别注意的地方在于，在包含Dropout的模型中，需要保证 K 次前向-后向计算时 Dropout 的Mask保持一致，否则的话，得到的扰动就不是针对某一模型的最优扰动了。所以，使用时需要保证在一个Step内，Dropout用到的 Mask 保持不变。</p>
<p>最终，对应的FreeLB算法伪代码如下。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/freelb0.png" alt="图 - 4 FreeLB算法伪代码">
    <figcaption>图 - 4 FreeLB算法伪代码</figcaption>
    </center>
</figure></p>
<p>需要说明的是，论文里提到的 PGD 与原文中的公式定义不太一致。</p>
<p>$$\delta_{t+1} = \prod_{\parallel \delta \parallel_F \le \epsilon} (\delta_t + \alpha g(\delta_t) / \parallel g(\delta_t) \parallel_F)$$</p>
<p>其中，$g(\alpha_t) = \nabla_{\delta} L(f_\theta(x + \delta_t), y)$，这里定义的是对扰动的梯度，而不是对输入$x$的梯度。这一点FreeLB对应的代码里是对应论文里的公式的，需要找PGD的官方实现进行验证。</p>
<h2 id="smart">SMART</h2>
<p><a href="https://arxiv.org/pdf/1911.03437.pdf">SMoothness-inducing Adversarial Regularization</a></p>
<p>SMART论文主要提出了两个创新点。</p>
<ul>
<li>
<p>Smoothness-inducing adversarial regularization</p>
<p>这个正则项主要是为了提高模型的鲁棒性。</p>
</li>
<li>
<p>Bregman proximal point optimization</p>
<p>这里是为了提高模型的效果，类似<code>Mean Teacher</code>。</p>
</li>
</ul>
<h3 id="smoothness-inducing-adversarial-regularization">Smoothness-inducing adversarial regularization</h3>
<p>提出优化下面的损失函数，</p>
<p>$$\min_\theta \mathcal{F}(\theta) = \mathcal{L}(\theta) + \lambda_s \mathcal{R}_s (\theta)$$</p>
<p>其中，$\mathcal{L}(\theta)$为正常损失函数，是基于数据对$(x_i, y_i)$的有监督损失函数。</p>
<p>$$\mathcal{L}(\theta) = \frac{1}{n}\sum_{i=1}^n \ell(f(x_i; \theta), y_i)$$</p>
<p>$\mathcal{R}_s(\theta)$为smoothness-inducing adversarial regularizer项：</p>
<p>$$\mathcal{R}_ s(\theta) = \frac{1}{n} \sum_{i=1}^n \max_{\parallel \tilde{x}_i - x_i \parallel_p \le \epsilon} \ell_s(f(\tilde{x}_i; \theta), f(x_i; \theta))$$</p>
<p>最新的SMART论文里，基于TRADES论文中的损失函数定义$\mathcal{R}_s$，适用于模型输出端是概率分布的情况。</p>
<p>$$\elll_s(P, Q) = \mathcal{D}<em>{KL} (P \parallel Q) + \mathcal{D}</em>{KL} (Q \parallel P)$$</p>
<p>当模型的输出是一个Scalar，也就是做类似回归任务时，有：</p>
<p>$$\ell_s(p, q) = \parallel p - q \parallel^2$$</p>
<h3 id="bregman-proximal-point-optimization">Bregman Proximal Point Optimization</h3>
<p>这一步主要是为了防止模型的参数在每个Step中更新过大。Transformer模型中的小的 lr 本身也是一种正则化，也就是让模型的参数不会变化非常大，提高模型的泛化性能、鲁棒性等。</p>
<p>Vanilla Bregman Proximal Point (VBPP) 算法定义了模型参数更新过程：</p>
<p>$$\theta_{t+1} = \mathrm{ArgMin}_{\theta} \mathcal{F}(\theta) + \mu \mathcal{D}_{Breg}(\theta, \theta_t)$$</p>
<p>其中，$\mu &gt; 0$，$\mathcal{D}_{Breg}$用于阻止模型的参数更新过大：</p>
<p>$$\mathcal{D} _{Breg}(\theta, \theta_t) = \frac{1}{n}\sum _{i=1}^n \ell_s (f(x_i; \theta), f(x_i; \theta_t))$$</p>
<p>实际使用中，可以借助Momentum Update的方式来更新参考的模型权重：</p>
<p>$$\tilde{\theta}<em>t = (1 - \beta)\theta_t + \beta \tilde{\theta}</em>{t-1}$$</p>
<p>然后计算$\mathcal{D}_{Breg}(\theta, \tilde{\theta}_t)$。这个过程与EMA非常相似，在自监督学习中经常被使用。</p>
<h3 id="伪代码">伪代码</h3>
<p>SMART算法实现的伪代码如图-5。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/smart0.png" alt="图 - 5 SMART算法伪代码">
    <figcaption>图 - 5 SMART算法伪代码</figcaption>
    </center>
</figure></p>
<h2 id="其它">其它</h2>
<ul>
<li>
<p>YOPO: <a href="https://arxiv.org/pdf/1905.00877.pdf">You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</a></p>
<p>通过分析Pontryagin’s Maximum Principle，观察到每次对抗样本的更新只与模型的前几层的梯度有关，基于这个观察，作者提出了 YOPO 算法。</p>
<p>关于 PMP &amp; Hamiltonian 看不懂，略过。</p>
<blockquote>
<p>Momentum should be accumulated between mini-batches other than different adversarial examples from one mini-batch, otherwise overfitting will become a serious problem.</p>
</blockquote>
</li>
<li>
<p>ALUM: <a href="https://arxiv.org/pdf/2004.08994.pdf">Adversarial training for large neural LangUage Models</a></p>
<p>这篇文章就提到了，对抗训练一方面是为了提高鲁棒性，另一方面是为了提高泛化性。本文提出的模型叫做 MT-DNN，上面的 SMART 算法与这个算法也可以结合起来。本文发现对抗学习也可以提高预训练阶段的效果，。</p>
<p>发现，virtual adversarial training 比 conventional adversarial trainging 效果更好，尤其是存在 noisy label 的时候。 BERT 预训练的 MLM 就是属于 noisy label 的情况，因为被 mask 的 word 实际上可以有很多的选择。所以 SMART 中的 $\lambda$ 在预训练阶段会比较大，比如 = 10，微调阶段为 = 1。</p>
</li>
<li>
<p>CIFS: <a href="https://arxiv.org/abs/2102.05311">CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection</a></p>
<p>对每个channel进行channel-wise 的扰动！</p>
</li>
</ul>
<h2 id="代码实现">代码实现</h2>
<p>先来看下 FreeLB 的代码实现，参考：<a href="https://github.com/zhuchen03/FreeLB">FreeLB - github</a>。在这个<a href="https://github.com/zhuchen03/FreeLB/blob/master/huggingface-transformers/launch/run_glue.sh">run_glue.sh</a> 脚本中，不同的下游任务对 $\delta$ 的初始化也不同，比如全零，或者<code>uniform()</code>等。<a href="https://github.com/mahyarnajibi/FreeAdversarialTraining">FreeAT - github</a>的实现代码其实与此类似。</p>
<p>代码里的亮点在于：(1) 首先获取输入文本的 Embedding 表示 (2) 初始化 $\delta$ (3) 迭代求解最优的对抗样本。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">74
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">78
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">79
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">80
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">81
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">82
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">83
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">84
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">85
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">86
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">87
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">88
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">89
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">90
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">91
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">92
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">93
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">94
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">for</span> step, batch <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(dataloader):
    <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">isinstance</span>(model, torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>DataParallel):
        embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>module<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
    <span style="color:#000;font-weight:bold">else</span>:
        embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
    <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_init_mag <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:

        input_mask <span style="color:#000;font-weight:bold">=</span> inputs[<span style="color:#d14">&#39;attention_mask&#39;</span>]<span style="color:#000;font-weight:bold">.</span>to(embeds_init)
        input_lengths <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(input_mask, <span style="color:#099">1</span>)
        <span style="color:#998;font-style:italic"># check the shape of the mask here..</span>

        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;l2&#34;</span>:
            delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)<span style="color:#000;font-weight:bold">.</span>uniform_(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>,<span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> input_mask<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)
            dims <span style="color:#000;font-weight:bold">=</span> input_lengths <span style="color:#000;font-weight:bold">*</span> embeds_init<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)
            mag <span style="color:#000;font-weight:bold">=</span> args<span style="color:#000;font-weight:bold">.</span>adv_init_mag <span style="color:#000;font-weight:bold">/</span> torch<span style="color:#000;font-weight:bold">.</span>sqrt(dims)
            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">*</span> mag<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>))<span style="color:#000;font-weight:bold">.</span>detach()
        <span style="color:#000;font-weight:bold">elif</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;linf&#34;</span>:
            delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)<span style="color:#000;font-weight:bold">.</span>uniform_(<span style="color:#000;font-weight:bold">-</span>args<span style="color:#000;font-weight:bold">.</span>adv_init_mag,
                                                            args<span style="color:#000;font-weight:bold">.</span>adv_init_mag) <span style="color:#000;font-weight:bold">*</span> input_mask<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">2</span>)

    <span style="color:#000;font-weight:bold">else</span>:
        delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(embeds_init)

    <span style="color:#998;font-style:italic"># the main loop</span>
    dp_masks <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
    <span style="color:#000;font-weight:bold">for</span> astep <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(args<span style="color:#000;font-weight:bold">.</span>adv_steps):
        <span style="color:#998;font-style:italic"># (0) forward</span>
        delta<span style="color:#000;font-weight:bold">.</span>requires_grad_()
        inputs[<span style="color:#d14">&#39;inputs_embeds&#39;</span>] <span style="color:#000;font-weight:bold">=</span> delta <span style="color:#000;font-weight:bold">+</span> embeds_init
        inputs[<span style="color:#d14">&#39;dp_masks&#39;</span>] <span style="color:#000;font-weight:bold">=</span> dp_masks

        outputs, dp_masks <span style="color:#000;font-weight:bold">=</span> model(<span style="color:#000;font-weight:bold">**</span>inputs)
        loss <span style="color:#000;font-weight:bold">=</span> outputs[<span style="color:#099">0</span>]  <span style="color:#998;font-style:italic"># model outputs are always tuple in transformers (see doc)</span>
        <span style="color:#998;font-style:italic"># (1) backward</span>
        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>n_gpu <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1</span>:
            loss <span style="color:#000;font-weight:bold">=</span> loss<span style="color:#000;font-weight:bold">.</span>mean()  <span style="color:#998;font-style:italic"># mean() to average on multi-gpu parallel training</span>
        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1</span>:
            loss <span style="color:#000;font-weight:bold">=</span> loss <span style="color:#000;font-weight:bold">/</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps

        loss <span style="color:#000;font-weight:bold">=</span> loss <span style="color:#000;font-weight:bold">/</span> args<span style="color:#000;font-weight:bold">.</span>adv_steps        <span style="color:#998;font-style:italic"># 求解梯度的平均</span>

        tr_loss <span style="color:#000;font-weight:bold">+=</span> loss<span style="color:#000;font-weight:bold">.</span>item()

        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>fp16:
            <span style="color:#000;font-weight:bold">with</span> amp<span style="color:#000;font-weight:bold">.</span>scale_loss(loss, optimizer) <span style="color:#000;font-weight:bold">as</span> scaled_loss:
                scaled_loss<span style="color:#000;font-weight:bold">.</span>backward()
        <span style="color:#000;font-weight:bold">else</span>:
            loss<span style="color:#000;font-weight:bold">.</span>backward()

        <span style="color:#000;font-weight:bold">if</span> astep <span style="color:#000;font-weight:bold">==</span> args<span style="color:#000;font-weight:bold">.</span>adv_steps <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>:
            <span style="color:#998;font-style:italic"># further updates on delta</span>
            <span style="color:#000;font-weight:bold">break</span>

        <span style="color:#998;font-style:italic"># (2) get gradient on delta</span>
        delta_grad <span style="color:#000;font-weight:bold">=</span> delta<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>clone()<span style="color:#000;font-weight:bold">.</span>detach()

        <span style="color:#998;font-style:italic"># (3) update and clip</span>
        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;l2&#34;</span>:
            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta_grad<span style="color:#000;font-weight:bold">.</span>view(delta_grad<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>), dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(denorm, <span style="color:#0086b3">min</span><span style="color:#000;font-weight:bold">=</span><span style="color:#099">1e-8</span>)
            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">+</span> args<span style="color:#000;font-weight:bold">.</span>adv_lr <span style="color:#000;font-weight:bold">*</span> delta_grad <span style="color:#000;font-weight:bold">/</span> denorm)<span style="color:#000;font-weight:bold">.</span>detach()
            <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:   <span style="color:#998;font-style:italic"># 通常为0 或者 1e-7</span>
                delta_norm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta<span style="color:#000;font-weight:bold">.</span>view(delta<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>float(), p<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>, dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>detach()
                exceed_mask <span style="color:#000;font-weight:bold">=</span> (delta_norm <span style="color:#000;font-weight:bold">&gt;</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm)<span style="color:#000;font-weight:bold">.</span>to(embeds_init)
                reweights <span style="color:#000;font-weight:bold">=</span> (args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">/</span> delta_norm <span style="color:#000;font-weight:bold">*</span> exceed_mask \
                                <span style="color:#000;font-weight:bold">+</span> (<span style="color:#099">1</span><span style="color:#000;font-weight:bold">-</span>exceed_mask))<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
                delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">*</span> reweights)<span style="color:#000;font-weight:bold">.</span>detach()            <span style="color:#998;font-style:italic"># 进入下一次循环</span>
        <span style="color:#000;font-weight:bold">elif</span> args<span style="color:#000;font-weight:bold">.</span>norm_type <span style="color:#000;font-weight:bold">==</span> <span style="color:#d14">&#34;linf&#34;</span>:
            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(delta_grad<span style="color:#000;font-weight:bold">.</span>view(delta_grad<span style="color:#000;font-weight:bold">.</span>size(<span style="color:#099">0</span>), <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>), dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, p<span style="color:#000;font-weight:bold">=</span><span style="color:#0086b3">float</span>(<span style="color:#d14">&#34;inf&#34;</span>))<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>, <span style="color:#099">1</span>)
            denorm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(denorm, <span style="color:#0086b3">min</span><span style="color:#000;font-weight:bold">=</span><span style="color:#099">1e-8</span>)
            delta <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">+</span> args<span style="color:#000;font-weight:bold">.</span>adv_lr <span style="color:#000;font-weight:bold">*</span> delta_grad <span style="color:#000;font-weight:bold">/</span> denorm)<span style="color:#000;font-weight:bold">.</span>detach()
            <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>adv_max_norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">0</span>:
                delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>clamp(delta, <span style="color:#000;font-weight:bold">-</span>args<span style="color:#000;font-weight:bold">.</span>adv_max_norm, args<span style="color:#000;font-weight:bold">.</span>adv_max_norm)<span style="color:#000;font-weight:bold">.</span>detach()
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#000;font-weight:bold">print</span>(<span style="color:#d14">&#34;Norm type {} not specified.&#34;</span><span style="color:#000;font-weight:bold">.</span>format(args<span style="color:#000;font-weight:bold">.</span>norm_type))
            <span style="color:#0086b3">exit</span>()

        <span style="color:#000;font-weight:bold">if</span> <span style="color:#0086b3">isinstance</span>(model, torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>DataParallel):
            embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>module<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])
        <span style="color:#000;font-weight:bold">else</span>:
            embeds_init <span style="color:#000;font-weight:bold">=</span> model<span style="color:#000;font-weight:bold">.</span>encoder<span style="color:#000;font-weight:bold">.</span>embeddings<span style="color:#000;font-weight:bold">.</span>word_embeddings(batch[<span style="color:#099">0</span>])

    <span style="color:#998;font-style:italic"># ============================ End (2) ==================</span>

    <span style="color:#000;font-weight:bold">if</span> (step <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">%</span> args<span style="color:#000;font-weight:bold">.</span>gradient_accumulation_steps <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:
        <span style="color:#000;font-weight:bold">if</span> args<span style="color:#000;font-weight:bold">.</span>fp16:
            torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>utils<span style="color:#000;font-weight:bold">.</span>clip_grad_norm_(amp<span style="color:#000;font-weight:bold">.</span>master_params(optimizer), args<span style="color:#000;font-weight:bold">.</span>max_grad_norm)
        <span style="color:#000;font-weight:bold">else</span>:
            torch<span style="color:#000;font-weight:bold">.</span>nn<span style="color:#000;font-weight:bold">.</span>utils<span style="color:#000;font-weight:bold">.</span>clip_grad_norm_(model<span style="color:#000;font-weight:bold">.</span>parameters(), args<span style="color:#000;font-weight:bold">.</span>max_grad_norm)

        optimizer<span style="color:#000;font-weight:bold">.</span>step()
        scheduler<span style="color:#000;font-weight:bold">.</span>step()  <span style="color:#998;font-style:italic"># Update learning rate schedule</span>
        model<span style="color:#000;font-weight:bold">.</span>zero_grad()
        global_step <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
</code></pre></td></tr></table>
</div>
</div><p>对应的 SMART 代码在<a href="https://github.com/namisan/mt-dnn">mt-dnn</a>。</p>
        
            <p>几个基础的常见的对抗样本的生成算法，包括FGM/FGSM, PGD等。</p>
<p>通常来说，对抗训练样本一般比哪些噪声样本破坏性更大，并且一般来说导致模型A失效的样本也会导致模型B失效。</p>
<h2 id="fgsm-fgm">FGSM-FGM</h2>
<h3 id="fgsm">FGSM</h3>
<p>论文：<a href="https://arxiv.org/pdf/1412.6572.pdf">FGSM</a>。</p>
<p>在引言中，作者提到，对抗样本可以对模型提供类似于 Dropout / model average 等方法的正则效果，提高模型的泛化性能；而且作者认为，我们需要平衡下面两个因素：更线性化（浅层）的模型更容易训练，但是非线性更高的模型对对抗样本会有更大帮助。关于后者，已经有论文证明，复杂度更高的模型确实对对抗样本效果更好，如<a href="https://arxiv.org/abs/1611.01236">Adversarial Machine Learning at Scale</a>以及<a href="https://arxiv.org/pdf/1706.06083.pdf">Towards Deep Learning Models Resistant to Adversarial
Attacks</a>等。</p>
<p>对抗样本是指哪些对正确分类的样本做一点人眼看不出来的微小改动就导致模型预测错误的样本，这也说明模型没有学习到每个类真正的内容含义。</p>
<p>正常的样本为 $x$，对抗样本，也就是有一个扰动的样本变为 $\tilde{x} = x + \eta$，则在深度学习模型中乘以权重$w$后得到的输出为：$\omega^T \tilde{x} = \omega^Tx + \omega^T \eta$，也就是后一项$\omega^T\eta$的存在导致模型的输出与正常数据$\omega^T x$不一致了，严重的导致样本预测类别发生变化。所以，我们希望对抗样本可以实现的是，当 $\eta$ 很小的时候，即满足$\parallel \eta \parallel_{\infty} &lt; \epsilon$ 时，模型的预测结果没有变化。</p>
<p>然而什么时候会导致模型的输出变化最大呢？也就是$\omega^T \eta$最大，当把$w,\eta$都看成是向量时，两者的方向一致的时候这个数值是最大的。直接体现在$\eta = \textrm{sign}(\omega)$，考虑到存在$| \eta |_{\infty} &lt; \epsilon$ 的存在，实际当$\eta = \epsilon \textrm{sign}(\omega)$ 时，这项在输入上的扰动对输出影响最大，当$\omega$的维度为n，每个元素的平均大小是 m 时，输出的变化数值是 $\epsilon mn$，当n足够大的时候，这个数值就会变得非常大，所以当模型的channel 数上来以后，就会更容易受到对抗样本的攻击。</p>
<p>上面对线性模型的分析表明，输入信号中那些与权重方向更一致的数据对输出的影响会更大，这也是从一个简单的角度说明为什么模型会存在对抗样本。</p>
<p>对于存在非线性激活函数的模型而言，上述分析也有一定的适用性。因为实际模型中，通常使用 relu, maxout, sigmoid 等激活函数，为了方便模型的训练，这些激活函数也通常具有较高的线形特征，比如 sigmoid，需要让输入在 0 范围内，否则就会饱和，而在0附近正好对应 sigmoid 最接近线性函数的区域。对于这一类（存在非线性层）模型，我们选择让损失函数变大来代替让输出变化项$\omega^T \eta$变大，对应的扰动是让损失函数变大的方向，也就是梯度的方向：</p>
<p>$$\eta = \epsilon \textrm{sign} (\nabla_x J(\theta, x, y))$$</p>
<p>这一过程也就是将模型在$\theta$处线性展开，即：$f(x_0 + \eta) = f(x_0) +f(x_0)' \eta$，所以当$\eta$的方向与$f(x_0)'$方向一致的时候，才会使得$f(x_0 + \eta) - f(x_0)$的差别最大。这个算法就对应<code>Fast gradient sign method (FGSM)</code>。实验发现，当$\epsilon = 0.25$时，99% 的扰动输入都可以让一个浅层的 Softmax 模型预测错误。</p>
<p>虽然按照万能逼近定理来说，深度学习模型可以逼近任何函数，所以可以对抗adversarial样本的扰动，然而，前提是我们需要明确提供这一类的样本让模型进行学习，所以作者提出了下面的损失函数训练模型。</p>
<p>$$\tilde{J}(\theta, x, y) = \alpha J(\theta, x, y) + (1 - \alpha) J(\theta, x + \epsilon \textrm{sign}(\nabla_x J(\theta, x, y)))$$</p>
<p>这里作者设置$\alpha=0.5$。有一点，这里采用$\textrm{sign}$函数，这个函数是不可求导的，所以模型不会知道对抗样本对自身权重变化的反应，而下面提到的 FGM 采用的 <code>l2 norm</code> 则会知道，后者会让学习过程变得容易。</p>
<blockquote>
<p>However, we did not find nearly as powerful of a regularizing result from this process, perhaps because these kinds of adversarial examples are not as difficult to solve.</p>
</blockquote>
<p>总结一下，对抗样本中涉及的扰动的方向与权重的方向一致才是重要原因，而不是扰动的大小；对抗样本存在是因为模型太线性化了，而不是太非线性化了。Rubbish Class Samples 是指那些没有意义的数据，也就是在人类看来，这些样本不属于任何一个类别，而不是真实数据 + 扰动。</p>
<h3 id="为啥-adversarial-样本具有-transferability">为啥 Adversarial 样本具有 transferability</h3>
<p>作者简要分析了一下为啥同一个对抗样本在多个模型上都会生效，而且还会预测成同一个类别。简单来说，是因为在同一个数据集上训练的模型权重一般也都比较相似，毕竟学习到的模型具有一定程度的泛化性，而权重的相似性也就导致对抗样本的一致性，也就是与权重的方向比较一致。</p>
<blockquote>
<p>The generalization of adversarial examples across different models can be explained as a result of adversarial perturbations being highly aligned with the weight vectors of a model, and different models learning similar functions when trained to perform the same task.</p>
</blockquote>
<h3 id="fgsm-伪代码实现">FGSM 伪代码实现</h3>
<p>参考的是: <a href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html">ADVERSARIAL EXAMPLE GENERATION</a></p>
<p>用于训练中需要验证。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">fgsm_attack</span>(data, model, label, loss_fn, optimizer, epsilon):
    data<span style="color:#000;font-weight:bold">.</span>requires_grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">True</span>
    output <span style="color:#000;font-weight:bold">=</span> model(data)
    loss <span style="color:#000;font-weight:bold">=</span> loss_fn(output, label)
    loss<span style="color:#000;font-weight:bold">.</span>backward()

    data_grad <span style="color:#000;font-weight:bold">=</span> data<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>data
    perturbed_data <span style="color:#000;font-weight:bold">=</span> data <span style="color:#000;font-weight:bold">+</span> epsilon <span style="color:#000;font-weight:bold">*</span> data_grad<span style="color:#000;font-weight:bold">.</span>sign()
    output_adv <span style="color:#000;font-weight:bold">=</span> model(perturbed_data)
    loss_adv <span style="color:#000;font-weight:bold">=</span> loss_fn(output_adv, label)

    loss_total <span style="color:#000;font-weight:bold">=</span> loss <span style="color:#000;font-weight:bold">+</span> loss_adv
    optimizer<span style="color:#000;font-weight:bold">.</span>zero_grad()
    <span style="color:#000;font-weight:bold">return</span> loss_total
</code></pre></td></tr></table>
</div>
</div><p>另外一个参考代码是：<a href="https://towardsdatascience.com/adversarial-attack-and-defense-on-neural-networks-in-pytorch-82b5bcd9171">Adversarial Attack and Defense on Neural Networks in PyTorch</a>。</p>
<h3 id="fgm">FGM</h3>
<p>FGM对应的论文是：<a href="https://arxiv.org/pdf/1605.07725.pdf">FGM</a></p>
<p>FGM算法是FGSM算法在文本领域的扩展，文本领域输入的是 Token 在字典中的索引，所以没法对这个索引进行扰动，而FGM选择对token的 Embedding 进行扰动，如下图所示。</p>
<p><figure>
    <center>
    <img src="/imgs/adversarial-training/fgm0.png" alt="图 - 1 FGM中对 Embedding 的扰动">
    <figcaption>图 - 1 FGM中对 Embedding 的扰动</figcaption>
    </center>
</figure></p>
<p>此时，模型可以学到一个技巧，就是让Embedding $v_k$ 的Norm足够大，此时小的扰动相对于模型的输入而言就非常微小了，但是这样的Embedding并没有那么大的意义，因此作者选择对Embedding进行归一化。</p>
<p>$$\bar{v}_k = \frac{v_k - E(v)}{\sqrt{\textrm{Var}(v)}}$$</p>
<p>其中，</p>
<p>$$E(v) = \sum_{j=1}^K f_j v_j, \textrm{Var}(v) = \sum_{j=1}^K f_j(v_j - E(v))^2$$</p>
<p>是字典中单词的个数，$f_i$为第i个单词在所有训练数据中出现的频率。</p>
<p>FGM中对应的小扰动范围内导致Loss增加最大的数值为：</p>
<p>$$r_{adv} = -\epsilon \mathbf{g} / \parallel \mathbf{g} \parallel_2$$</p>
<p>其中，</p>
<p>$$\mathbf{g} = \nabla_x \log p(y | \mathbf{x}; \hat{\theta})$$</p>
<p>注意是对输入 x 的梯度，计算Loss的梯度时需要Label信息$y$的存在，$\hat{\theta}$为模型的权重，在计算题的过程中，这个权重不发生变化。</p>
<p>结合上文提到对于文本任务，这里对 Token Embedding 进行加性扰动。此时用$\mathbf{s}$表示word embedding vectors，Label为y，则定义扰动数值为：</p>
<p>$$r_{adv} = -\epsilon \mathbf{g} / \parallel \mathbf{g} \parallel_2, \mathrm{where} \mathbf{g} = \nabla_s \log p(y | \mathbf{s}; \hat{\theta})$$</p>
<p>实际实现中，这里应该是$+\epsilon \mathbf{g} / \parallel \mathbf{g} \parallel_2$而不是-。然后在训练过程中对应的 Loss 项为：</p>
<p>$$L_{\mathrm{adv}}(\theta) = - \frac{1}{N} \sum_{n=1}^N \log p(y_n | s_n + r_{\mathrm{adv}, n}; \theta)$$</p>
<p>其中，N是样本的个数，可以是 Mini-Batch 内样本的个数。</p>
<h3 id="virtual-adversarial-training">Virtual adversarial training</h3>
<p>Adversarial training，是针对有label的训练而言的，训练模型让该模型可以对unmodified examples以及adversarial examples都可以正确分类，不仅提高对 adversarial samples 的鲁棒性，也可以提高模型的泛化性。</p>
<blockquote>
<p>Adversarial training requires the use of labels when training models that use a supervised cost, because the label appears in the cost function that the adversarial perturbation is designed to maximize.</p>
</blockquote>
<p>Virtual adversarial traingin，可以适用于哪些半监督学习，输入可以是 unlabeled examples。</p>
<blockquote>
<p>This is done by regularizing the model so that given an example, the model will produce the same output distribution as it produces on an adversarial perturbation of that example. Virtual adversarial training achieves good generalization performance for both supervised and semi-supervised learning tasks.</p>
</blockquote>
<p>VAT对应的Loss函数是：</p>
<p>$$\mathrm{KL} [p (\cdot | x; \hat{\theta}) \parallel p(\cdot | x + r_{v-adv}; \theta)]$$</p>
<p>其中，</p>
<p>$$r_{v-adv} = \mathrm{ArgMax}_{r, \parallel r \parallel \le \epsilon} \mathrm{KL} [p(\cdot | x; \hat{\theta}) \parallel p(\cdot | x + r; \hat{\theta})$$</p>
<p>对于文本的 Embedding Vector 添加扰动，实现公式如下。</p>
<p>$$\mathrm{KL} [p (\cdot | s; \hat{\theta}) \parallel p(\cdot | s + r_{v-adv}; \theta)]$$</p>
<p>其中，</p>
<p>$$r_{v-adv} = \nabla_{s+d} \mathrm{KL} [p(\cdot | s; \hat{\theta}) \parallel p(\cdot | s + r; \hat{\theta})$$</p>
<p>这里，$d$为初始扰动，是随机初始化的，对应的Adv Loss项为：</p>
<p>$$L_{v-adv}(\theta) = \frac{1}{N'} \sum_{n'=1}^N' \mathrm{KL}[p(\cdot | s_{n'}; \hat{\theta}) \parallel p(\cdot | s_{n'} + r_{v-adv, n'}; \theta)]$$</p>
<p>$N'$为所有的labeled / unlabeled样本。</p>
<h2 id="pgd">PGD</h2>
<p>本文的一个贡献是将Adversarial Training不再看成是一个Adversarial Samples训练问题了，而是统一到一个通用的优化Loss函数里。</p>
<p>$$\min \rho_\theta, \mathrm{Where}, \rho_\theta = \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta \in \mathcal{S}} L(\theta, x + \delta, y) \right]$$</p>
<p>其中，$\delta$为添加的扰动。</p>
<p>可以看出模型优化过程是一个min-max的过程，里面的 max 过程可以被认为是在$\parallel \cdot \parallel_{\infty} &lt; \epsilon$ 范围内寻找Loss最大的扰动，前面提到的 FGM / FGSM 可以认为只是迭代一次寻找最大值（one-step scheme for maximizing），本文的另一个贡献是提出了多步优化寻找最大值（multi-step），被称作Projected Gradient Descent (PGD)。$x^0$为原始干净输入，$x^1$为干净输入基础上使用下式进行扰动。更新扰动的过程有两种定义。</p>
<p>$$x^{t+1} = \prod_{x+\mathcal{S}} (x^t + \alpha \mathrm{sgn} (\nabla_{x^t} L(\theta, x^t, y))$$</p>
<p>另一种是：</p>
<p>$$\delta^{t+1} = \prod_{\parallel \delta \parallel_\infty \le \epsilon} (\delta^t + \alpha \mathrm{sgn} (\nabla_{\delta^t} L(\theta, x + \delta^t, y))$$</p>
<p>其中，$\prod_{x+\mathcal{S}}$为Project计算，也就是将x的数值范围约束在$x + \mathcal{S}$范围内，实际可以通过<code>clip()</code>函数来完成。注意第二种方式里的$\delta = x^t - x^0$是相对于最开始的干净样本总的扰动大小而言的。前者的代码对应：<a href="https://github.com/Harry24k/PGD-pytorch/blob/master/PGD.ipynb">PGD-pytorch-github</a>，以及<a href="https://fyubang.com/2019/10/15/adversarial-train/">功守道：NLP中的对抗训练 + PyTorch实现</a>，而FreeAT / FreeLB等官方代码里用到的是后者，而且 PGD 论文里也提到一句：the adversary of choice will be projected gradient descent (PGD) starting from a random perturbation around the natural example，所以这里倾向于后者，毕竟如果说需要随机初始化$\delta_0$，则应该对应的是后者。当然了，不管是哪种方式，都需要保证最终的扰动项$\delta_t &lt; \epsilon$，而$\delta_t = x^t - x^0$。</p>
<p>数学分析中，两种定义方式的区别在于前者的第$t+1$次对抗样本是:$x^{t+1} = x + \delta^t + \nabla_{x^t} + \nabla_{\delta^t}$，而后者对应的是：$x^{t+1} = x + \delta^{t+1} = x + \delta^t + \nabla_{\delta^t}$，显然后者会更好一些。</p>
<p>这个函数中，一种重要的参数是求解 $x^t$ 时所需要的迭代次数，常见的数值是20 / 10等，说明对训练耗时的增加还是挺严重的。</p>
<h2 id="trades">TRADES</h2>
<p><a href="https://arxiv.org/pdf/1901.08573.pdf">Theoretically Principled Trade-off between Robustness and Accuracy</a></p>
<p>定义了一个新的min-max损失函数，用于平衡泛化性与鲁棒性，也就是提高模型在non-adversarial examples 以及 adversarial examples 上的性能。</p>
<p>$$\min_\theta \mathbb{E}_{(x, y) \sim \mathcal{D}} \max_{\parallel \eta \parallel \le \epsilon} \left( \ell (f_\theta (x), y) + \mathcal{L} (f_\theta(x), f_\theta (x + \eta) / \lambda)  \right)$$</p>
<p>对应的 SMART 算法里使用了这种形式。</p>
<h2 id="代码实现">代码实现</h2>
<p>主要包含FGM 的代码示例，参考博客是：<a href="https://fyubang.com/2019/10/15/adversarial-train/">功守道：NLP中的对抗训练 + PyTorch实现</a>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">FGM</span>():
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, model):
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model <span style="color:#000;font-weight:bold">=</span> model
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>backup <span style="color:#000;font-weight:bold">=</span> {}
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">attack</span>(<span style="color:#999">self</span>, img, epsilon, emb_name):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> param<span style="color:#000;font-weight:bold">.</span>requires_grad <span style="color:#000;font-weight:bold">and</span> embed_name <span style="color:#000;font-weight:bold">in</span> name:
                <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>backup[name] <span style="color:#000;font-weight:bold">=</span> param<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>clone()
                norm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(param<span style="color:#000;font-weight:bold">.</span>grad)
                <span style="color:#000;font-weight:bold">if</span> norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1e-6</span>:
                    r_adv <span style="color:#000;font-weight:bold">=</span> epsilon <span style="color:#000;font-weight:bold">*</span> param<span style="color:#000;font-weight:bold">.</span>grad <span style="color:#000;font-weight:bold">/</span> norm
                    param<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>add_(r_adv)
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">restore</span>(<span style="color:#999">self</span>, embed_name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;.embed&#39;</span>):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> param<span style="color:#000;font-weight:bold">.</span>requires_grad <span style="color:#000;font-weight:bold">and</span> embed_name <span style="color:#000;font-weight:bold">in</span> name:
                <span style="color:#000;font-weight:bold">assert</span> name <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>backup
                param<span style="color:#000;font-weight:bold">.</span>data <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>backup[name]
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>backup <span style="color:#000;font-weight:bold">=</span> {}
</code></pre></td></tr></table>
</div>
</div><p>使用代码如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fgm <span style="color:#000;font-weight:bold">=</span> FGM(model)
<span style="color:#000;font-weight:bold">for</span> batch, label <span style="color:#000;font-weight:bold">in</span> data:
    loss <span style="color:#000;font-weight:bold">=</span> model(batch, label)
    loss<span style="color:#000;font-weight:bold">.</span>backward()
    fgm<span style="color:#000;font-weight:bold">.</span>attack()
    loss_adv <span style="color:#000;font-weight:bold">=</span> model(batch, label)
    loss_adv<span style="color:#000;font-weight:bold">.</span>backward()
    fgm<span style="color:#000;font-weight:bold">.</span>restore()
    optimizer<span style="color:#000;font-weight:bold">.</span>step()
    model<span style="color:#000;font-weight:bold">.</span>zero_grad()
</code></pre></td></tr></table>
</div>
</div><p>PGD的实现实际有两种，一种是对$x^t$求导用于更新adversarial，另一种是对$\delta_t$求导并更新adversarial样本为$x + \delta_{t+1}$。</p>
<p>第一种示例代码如下，即对输入数据求导。这里的参考是上面的博客链接以及这个仓库：<a href="https://github.com/Harry24k/PGD-pytorch/blob/master/PGD.ipynb">PGD-pytorch - github</a></p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">PGD</span>():
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, model):
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model <span style="color:#000;font-weight:bold">=</span> model
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup <span style="color:#000;font-weight:bold">=</span> {}
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>grad_backup <span style="color:#000;font-weight:bold">=</span> {}
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">attack</span>(<span style="color:#999">self</span>, epsilon<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1.</span>, alpha<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.3</span>, emb_name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;.embed&#39;</span>, is_first_attack<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> param<span style="color:#000;font-weight:bold">.</span>requires_grad <span style="color:#000;font-weight:bold">and</span> embed_name <span style="color:#000;font-weight:bold">in</span> name:
                <span style="color:#000;font-weight:bold">if</span> is_first_attack:
                    <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup[name] <span style="color:#000;font-weight:bold">=</span> param<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>clone()
                norm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(param<span style="color:#000;font-weight:bold">.</span>grad)
                <span style="color:#000;font-weight:bold">if</span> norm <span style="color:#000;font-weight:bold">&gt;</span> <span style="color:#099">1e-6</span>:
                    r_adv <span style="color:#000;font-weight:bold">=</span> alpha <span style="color:#000;font-weight:bold">*</span> param<span style="color:#000;font-weight:bold">.</span>grad <span style="color:#000;font-weight:bold">/</span> norm
                    param<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>add_(r_adv)
                    param<span style="color:#000;font-weight:bold">.</span>data <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>project(name, param<span style="color:#000;font-weight:bold">.</span>data, epsilon)
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">project</span>(<span style="color:#999">self</span>, name, data, epsilon):
        r_adv <span style="color:#000;font-weight:bold">=</span> data <span style="color:#000;font-weight:bold">-</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>emb_backup[name]        <span style="color:#998;font-style:italic"># 是相对于原始数据的扰动大小</span>
        norm <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>norm(r_adv)
        <span style="color:#000;font-weight:bold">if</span> r_adv <span style="color:#000;font-weight:bold">&gt;</span> epsilon:
            r_adv <span style="color:#000;font-weight:bold">=</span> epsilon <span style="color:#000;font-weight:bold">*</span> r_adv <span style="color:#000;font-weight:bold">/</span> norm
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup[name] <span style="color:#000;font-weight:bold">+</span> r_adv
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">restore</span>(<span style="color:#999">self</span>, embed_name<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;.embed&#39;</span>):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> embed_name <span style="color:#000;font-weight:bold">in</span> name:
                <span style="color:#000;font-weight:bold">assert</span> name <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup
                param<span style="color:#000;font-weight:bold">.</span>data <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup[name]
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embed_backup <span style="color:#000;font-weight:bold">=</span> {}
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">backup_grad</span>(<span style="color:#999">self</span>):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> param<span style="color:#000;font-weight:bold">.</span>requires_grad:
                <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>grad_backup[name] <span style="color:#000;font-weight:bold">=</span> param<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>clone()
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">restore_grad</span>(<span style="color:#999">self</span>):
        <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>named_parameters():
            <span style="color:#000;font-weight:bold">if</span> param<span style="color:#000;font-weight:bold">.</span>requires_grad:
                param<span style="color:#000;font-weight:bold">.</span>grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>grad_backup[name]
</code></pre></td></tr></table>
</div>
</div><p>对应的使用过程也稍微麻烦一点。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pgd <span style="color:#000;font-weight:bold">=</span> PGD(model)
K <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">3</span>
<span style="color:#000;font-weight:bold">for</span> batch, label <span style="color:#000;font-weight:bold">in</span> data:
    loss <span style="color:#000;font-weight:bold">=</span> model(batch, label)
    loss<span style="color:#000;font-weight:bold">.</span>backward()
    pgd<span style="color:#000;font-weight:bold">.</span>backup_grad()
    <span style="color:#000;font-weight:bold">for</span> idx <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(K):
        pgd<span style="color:#000;font-weight:bold">.</span>attack(is_first_attack<span style="color:#000;font-weight:bold">=</span>(idx <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>))
        <span style="color:#000;font-weight:bold">if</span> idx <span style="color:#000;font-weight:bold">!=</span> K <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>:
            model<span style="color:#000;font-weight:bold">.</span>zero_grad()
        <span style="color:#000;font-weight:bold">else</span>:
            pgd<span style="color:#000;font-weight:bold">.</span>restore_grad()
        loss_adv <span style="color:#000;font-weight:bold">=</span> model(batch, label)
        loss_adv<span style="color:#000;font-weight:bold">.</span>backward()     <span style="color:#998;font-style:italic"># 最后一轮的对抗样本的梯度会被保留</span>
    pgd<span style="color:#000;font-weight:bold">.</span>restore()               <span style="color:#998;font-style:italic"># 恢复 Embedding 参数</span>
    optimizer<span style="color:#000;font-weight:bold">.</span>step()
    model<span style="color:#000;font-weight:bold">.</span>zero_grad()
</code></pre></td></tr></table>
</div>
</div><p>第二种 PGD 对应的代码稍微在保证$\parallel \delta \parallel_\infty &lt; \epsilon$时会方便一些，参考代码<a href="https://adversarial-ml-tutorial.org/adversarial_examples/">Chapter 3 - Adversarial examples, solving the inner maximization</a>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">9
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">pgd</span>(model, x, y, epsilon, alpha, K):
    delta <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>zeros_like(x, requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)
    <span style="color:#000;font-weight:bold">for</span> idx <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(K):
        loss <span style="color:#000;font-weight:bold">=</span> loss_fn(model(x <span style="color:#000;font-weight:bold">+</span> delta), y)
        loss<span style="color:#000;font-weight:bold">.</span>backward()
        delta<span style="color:#000;font-weight:bold">.</span>data <span style="color:#000;font-weight:bold">=</span> (delta <span style="color:#000;font-weight:bold">+</span> x<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">*</span> alpha <span style="color:#000;font-weight:bold">*</span> delta<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>data)<span style="color:#000;font-weight:bold">.</span>clamp(<span style="color:#000;font-weight:bold">-</span>epsilon, epsilon)
        delta<span style="color:#000;font-weight:bold">.</span>zero_grad_()
    <span style="color:#000;font-weight:bold">return</span> delta<span style="color:#000;font-weight:bold">.</span>detach()
</code></pre></td></tr></table>
</div>
</div>
        
            <p>记录一些Torch使用过程中会用到的小知识点。</p>
<h2 id="求解中间变量的梯度">求解中间变量的梯度</h2>
<p>前面提到，<code>backward()</code>函数只会保存Leaf Node的梯度，如果要想保留中间计算结果的梯度，可以使用<code>Tensor.retain_grad()</code>来实现。是不是Leaf Node可以使用 <code>Tensor.is_leaf</code>来判断，简单来说Leaf Node有两类：</p>
<ul>
<li><code>Tensor.requires_grad=False</code> 的Tensor属于Leaf Node</li>
<li><code>Tesnor.requires_grad=True</code>并且是由用户创建的Tensor也属于Leaf Node；用户创建意味着不是其它Op产生的Tensor.</li>
</ul>
<p>实际要获取中间变量的梯度，有以下方法：</p>
<ol>
<li>使用<code>retain_grad()</code></li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get_inter_grad</span>():
  x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>ones((<span style="color:#099">2</span>, <span style="color:#099">2</span>), requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) 
  <span style="color:#000;font-weight:bold">print</span>(x<span style="color:#000;font-weight:bold">.</span>is_leaf)

  y <span style="color:#000;font-weight:bold">=</span> x <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">2</span>
  y<span style="color:#000;font-weight:bold">.</span>retain_grad()
  z <span style="color:#000;font-weight:bold">=</span> y <span style="color:#000;font-weight:bold">**</span> <span style="color:#099">2</span>

  z<span style="color:#000;font-weight:bold">.</span>backward(torch<span style="color:#000;font-weight:bold">.</span>ones_like(z))
  <span style="color:#000;font-weight:bold">print</span>(x<span style="color:#000;font-weight:bold">.</span>grad)
  <span style="color:#000;font-weight:bold">print</span>(y<span style="color:#000;font-weight:bold">.</span>grad)       <span style="color:#998;font-style:italic"># not None</span>
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>使用<code>torch.autograd.grad(outputs, inputs)</code></li>
</ol>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get_inter_grad</span>():
  x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>ones((<span style="color:#099">2</span>, <span style="color:#099">2</span>), requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) 
  <span style="color:#000;font-weight:bold">print</span>(x<span style="color:#000;font-weight:bold">.</span>is_leaf)

  z <span style="color:#000;font-weight:bold">=</span> y <span style="color:#000;font-weight:bold">**</span> <span style="color:#099">2</span>
  t <span style="color:#000;font-weight:bold">=</span> z<span style="color:#000;font-weight:bold">.</span>mean()
  x_res <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>autograd<span style="color:#000;font-weight:bold">.</span>grad(t, x, create_graph<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)[<span style="color:#099">0</span>]
  <span style="color:#000;font-weight:bold">print</span>(x_res)
  y_res <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>autograd<span style="color:#000;font-weight:bold">.</span>grad(t, y, create_graph<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)[<span style="color:#099">0</span>]
  <span style="color:#000;font-weight:bold">print</span>(y_res)
</code></pre></td></tr></table>
</div>
</div><p>注意，<code>torch.autograd.grad()</code>只能对Scalar output计算梯度，所以才用了<code>t = z.mean()</code>进行反向传播。</p>
<ol start="3">
<li>使用<code>torch.Tensor.register_hook()</code></li>
</ol>
<p><code>register_hook()</code>函数会注册一个backward hook，每次计算该Tensor的梯度时，都会调用这个<code>hook</code>函数。函数签名是<code>hook(grad) -&gt; Tensor or None</code>，一般来说，这里的 hook 函数不应该对输入的 grad 进行修改，而是返回一个新的梯度来代替 grad。<code>register_hook()</code>函数会返回一个 handle，可以调用<code>handle.remove()</code>来从当前的Tensor中去掉这个 hook 函数。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  global_grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0.0</span>
  <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">extract_grad</span>(grad):
      <span style="color:#000;font-weight:bold">print</span>(<span style="color:#d14">&#39;current grad: &#39;</span>, grad)
      <span style="color:#000;font-weight:bold">global</span> global_feat
      global_grad <span style="color:#000;font-weight:bold">=</span> grad
      <span style="color:#000;font-weight:bold">return</span> grad

  <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">get_inter_grad</span>():
      <span style="color:#000;font-weight:bold">global</span> global_grad
      x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>ones((<span style="color:#099">2</span>, <span style="color:#099">2</span>), requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) 
      <span style="color:#000;font-weight:bold">print</span>(x<span style="color:#000;font-weight:bold">.</span>is_leaf)

      y <span style="color:#000;font-weight:bold">=</span> x <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">2</span>
      z <span style="color:#000;font-weight:bold">=</span> y <span style="color:#000;font-weight:bold">**</span> <span style="color:#099">2</span>
      t <span style="color:#000;font-weight:bold">=</span> z<span style="color:#000;font-weight:bold">.</span>mean()

      y_hook <span style="color:#000;font-weight:bold">=</span> y<span style="color:#000;font-weight:bold">.</span>register_hook(extract_grad)
      t<span style="color:#000;font-weight:bold">.</span>backward()
      <span style="color:#000;font-weight:bold">print</span>(<span style="color:#d14">&#39;y grad: &#39;</span>, global_grad)
      <span style="color:#000;font-weight:bold">print</span>(<span style="color:#d14">&#39;x grad: &#39;</span>, x<span style="color:#000;font-weight:bold">.</span>grad)

      y_hook<span style="color:#000;font-weight:bold">.</span>remove()
</code></pre></td></tr></table>
</div>
</div><p>但是实际使用下来，第三种方法获取到的还是 <code>global_grad</code> 原始的数值，有待进一步查原因。</p>
<h2 id="获取模型权重的梯度">获取模型权重的梯度</h2>
<p>获取权重的梯度代码非常简单：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#998;font-style:italic"># ...</span>
    <span style="color:#000;font-weight:bold">for</span> name, param <span style="color:#000;font-weight:bold">in</span> model<span style="color:#000;font-weight:bold">.</span>named_parameters():
        <span style="color:#000;font-weight:bold">print</span>(name)
        <span style="color:#000;font-weight:bold">print</span>(param<span style="color:#000;font-weight:bold">.</span>grad)           <span style="color:#998;font-style:italic"># 真实梯度，param.grad 是一个 Tensor</span>
        <span style="color:#000;font-weight:bold">print</span>(param<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>grad)      <span style="color:#998;font-style:italic"># None</span>
    <span style="color:#998;font-style:italic"># ...</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="使用checkpoint功能">使用checkpoint功能</h2>
<p>gradient checkpointing的意思是说，在反向传播时，重新计算对应代码段的前向计算，这样就可以不用在前向计算时保存临时中间激活输出值以及对应的梯度等。</p>
<p>但是有一点需要注意就是需要保证那些具有随机属性的计算的两次前向输出应该是一致的，比如 Dropout，因此需要将<code>preserve_rng_state=True</code>传入到<code>torch.utils.checkpoint.checkpint()</code>函数中，但是这样做会导致性能下降较大，所以如果没有涉及到RNG 类的操作，那么需要将<code>preserve_rng_state=False</code>。另一点是，即使设置了<code>preserve_rng_state=True</code>，但是在<code>run_fn</code>函数里面将变量移动到一个新的device上的话，那么 RNG 状态的一致性也还是无法保证，所谓的新的device，就是当前device + 传入到 <code>run_fn</code> 的参数的device 的合集。</p>
<p>对应实现 <code>checkpinting</code> 的函数是：<code>torch.utils.checkpoint.checkpoint(function, *args, **kwargs)</code>函数。</p>
<p>checkpointing的工作原理是：<code>trading compute for memory</code>，也就是不会保存计算过程中的中间激活值，而是在反向传播时重新计算这些数值。可以应用到任意部分的模型计算。</p>
<p>具体来说，<code>function</code>表示的计算前向计算时是在<code>torch.no_grad()</code>里面执行的，但是<code>checkpoint()</code>函数会保存输入的tuple以及function parameters等。<code>function</code>计算可以输出非Tensor的参数，但是gradient recording 只会作用于那些Tensor的输出。注意，如果输出包含在<code>list, dict, custom objects</code>等结构体里，即使是Tensor，也不会被计算gradients。</p>
<p>一个具体的使用例子是 Albef 仓库里 <code>xbert</code> 的实现:</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">create_custom_forward</span>(module):
        <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">custom_forward</span>(<span style="color:#000;font-weight:bold">*</span>inputs):
            <span style="color:#000;font-weight:bold">return</span> module(<span style="color:#000;font-weight:bold">*</span>inputs, past_key_value, output_attentions)
        <span style="color:#000;font-weight:bold">return</span> custom_forward

    layer_outputs <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>utils<span style="color:#000;font-weight:bold">.</span>checkpoint<span style="color:#000;font-weight:bold">.</span>checkpoint(
        create_custom_forward(layer_module),
        hidden_states,
        attention_mask,
        layer_head_mask,
        encoder_hidden_states,
        encoder_attention_mask,
    )
</code></pre></td></tr></table>
</div>
</div><p>这里使用了python的闭包方式进行实现function。</p>
<p>另一个API<code>torch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs)</code>可以实现对sequential models进行checkpoints。</p>
<blockquote>
<p>Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.</p>
</blockquote>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    model <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential(<span style="color:#000;font-weight:bold">...</span>)
    input_var <span style="color:#000;font-weight:bold">=</span> checkpoint_sequential(model, chunks, input_var)
</code></pre></div>
        
	
		<span>1</span>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2022-01-16 21:38:36.530227 &#43;0800 CST m=&#43;0.131156331">2022</time> triloon. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
