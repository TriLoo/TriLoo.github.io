<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Posts &middot; Triloon</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		
		<link rel="icon" href="favicon.ico" />
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Triloon" />
	</head>

    <body>
        
		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h1 class="nav-title">Triloon</h1>
					
				</a>
				<ul>
    
    
        <li>
            <a href="/about/about">
                
                <span>About</span>
                
            </a>
        </li>
    
        <li>
            <a href="/posts/">
                
                <span>Posts</span>
                
            </a>
        </li>
    
</ul>
			</div>
		</nav>

        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">

        

<main>
	<div class="catalogue">
		
			<a href="https://triloon.space/posts/torch-impl-0/" class="catalogue-item">
    <div>
        <time datetime="2021-09-18 11:08:11 &#43;0800 CST" class="catalogue-time">September 18, 2021</time>
        <h2 class="catalogue-title">Torch实现原理分析积累</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Pytorch 实现学习积累。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/model-visualization/" class="catalogue-item">
    <div>
        <time datetime="2021-09-11 15:16:07 &#43;0800 CST" class="catalogue-time">September 11, 2021</time>
        <h2 class="catalogue-title">Model Visualization</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>深度学习中的一些可视化技术。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/origin-transformer/" class="catalogue-item">
    <div>
        <time datetime="2021-09-06 20:04:32 &#43;0800 CST" class="catalogue-time">September 6, 2021</time>
        <h2 class="catalogue-title">Origin Transformer</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Attention is all your need.</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/cnn-transformer-volo/" class="catalogue-item">
    <div>
        <time datetime="2021-09-06 16:26:11 &#43;0800 CST" class="catalogue-time">September 6, 2021</time>
        <h2 class="catalogue-title">Cnn Transformer 系列之 Volo</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>VOLO论文。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/efficient-transformer-reformer/" class="catalogue-item">
    <div>
        <time datetime="2021-09-06 10:50:19 &#43;0800 CST" class="catalogue-time">September 6, 2021</time>
        <h2 class="catalogue-title">Efficient Transformer 系列之 Reformer</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Reformer论文。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/adam-adamw/" class="catalogue-item">
    <div>
        <time datetime="2021-09-03 20:30:59 &#43;0800 CST" class="catalogue-time">September 3, 2021</time>
        <h2 class="catalogue-title">从Adam到AdamW</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Adam算法的实现以及一个主要改进AdamW的原理与实现。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/resnet-series/" class="catalogue-item">
    <div>
        <time datetime="2021-09-02 14:12:59 &#43;0800 CST" class="catalogue-time">September 2, 2021</time>
        <h2 class="catalogue-title">Resnet Series</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Residual Connection以及后续发展。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/binary-search-tree/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 15:00:22 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">Binary Search Tree</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>二叉搜索树相关笔记</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/ssl-image-representation/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 14:15:48 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">图像表征算法中的自监督学习方法</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>经典自监督模型，包括MoCo / SimCLR / SwAV / BYOL / SimSiam 等。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/big-batchsize-train/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 14:09:48 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">大Batchsize训练用到的优化器</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>一些为了适应大的Batch Size训练的优化算法。</p>
        </p>
    </div>
</a>

		
	</div>
	
	<div class="pagination">
		
			<a href="/posts/" class="left arrow">&#8592;</a>
		
		
			<a href="/posts/page/3/" class="right arrow">&#8594;</a>
		
        
            <p>Pytorch 实现学习积累。</p>
<h2 id="基础">基础</h2>
<ul>
<li>All objects in pytorch are passed by reference in python. But doing <code>a=</code> does not try to change <code>a</code> in-place, it only give the name <code>a</code> to the object returned by the right hand side.</li>
<li>矩阵乘：@， Matmul，mm（后两者的区别在于 mm 仅适用于二维Tensor，matmul适合高维Tensor）；*，mul 实现的是element-wise乘</li>
<li><code>_</code> suffix ops 是in-place操作</li>
<li>Tensor 与 Numpy 之间可以共享底层存储空间，所以修改一个也会导致另一个变量发生变化。如<code>.numpy()</code>操作，<code>from_numpy()</code>等</li>
<li>自定义Dataset，需要自己实现<code>__init__</code>、<code>__len__</code>、<code>__getitem__</code>等函数；<code>ToTensor</code>会将PIL Image、NumPy ndarry转换成<code>FloatTensor</code>，并且将像素上的数值范围缩放到(0.0, 1.0)之间。</li>
<li>继承<code>nn.Module</code>创建模型的时候，会自动收集定义在models内的fields，并且让所有的 parameters 都可以被<code>parameters()</code>以及<code>named_parameters()</code>等方法获取到</li>
</ul>
<h2 id="module">Module</h2>
<p>Module 在调用的时候实际会调用<code>Module._call_impl()</code>函数，这个函数里调用顺序如下。</p>
<ol>
<li>调用<code>_global_forward_pre_hooks</code>或者<code>self._forward_pre_hooks</code>里面所有的hook，对当前的Module以及输入数据进行处理，hook 函数的格式是：<code>hook(module, input) -&gt; None or modified input</code>，如果 hook 函数会返回数据，那么这个返回的数据才是真正的输入 forward() 函数进行计算的数据</li>
<li>调用<code>forward_call()</code>函数完成前向计算</li>
<li>调用<code>_global_forward_hooks</code>或者<code>self._forward_hooks</code>里面的所有hook，hook函数签名是<code>hook(module, input, output) -&gt; None or modified output</code>，函数的输出是最终的输出</li>
<li><code>full_backward_hooks</code>里的 hooks</li>
</ol>
<h2 id="autograd">Autograd</h2>
<p>通过设置Tensor的<code>requires_grad</code>来决定是否需要计算 Loss 对该 Tensor 的梯度。</p>
<ul>
<li>
<p>torch.autograd.Function</p>
<p>记录对Tensor的操作，是一个类，包含<code>forward()</code>、<code>backward()</code>两个静态成员函数。每个Function完成对 Tensor 的一个操作，并记录发生的事情。所有的 Function 被组织成有向无环图（DAG），边表示数据依赖(input &lt;&ndash; output)。当反向传播时，按照拓扑顺序依次调用Function的<code>backward()</code>函数。</p>
<p>实际使用的时候就是继承Function类并实现这两个静态成员函数。一个具体例子如下，所以都是静态成员函数进行操作，无需创建具体实例。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">Exp</span>(Function):
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(ctx, i):
        result <span style="color:#000;font-weight:bold">=</span> i<span style="color:#000;font-weight:bold">.</span>exp()
        ctx<span style="color:#000;font-weight:bold">.</span>save_for_backward(result)
        <span style="color:#000;font-weight:bold">return</span> result
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">backward</span>(ctx, grad_output):
        result, <span style="color:#000;font-weight:bold">=</span> ctx<span style="color:#000;font-weight:bold">.</span>saved_tensors
        <span style="color:#000;font-weight:bold">return</span> grad_output <span style="color:#000;font-weight:bold">*</span> result

  output <span style="color:#000;font-weight:bold">=</span> Exp<span style="color:#000;font-weight:bold">.</span>apply(<span style="color:#0086b3">input</span>)
</code></pre></td></tr></table>
</div>
</div><p>注意，Function知道Tensor的前向计算，也支持后向传播，后向传播函数保存在<code>tensor.grad_fn</code>属性中。也就是说Function 是计算图中的节点，边才是 Tensor。</p>
</li>
<li>
<p>is_leaf</p>
<p>这个函数用来判断Tensor是否保存了grad。</p>
<ul>
<li>如果Tensor的<code>requires_grad=False</code>，则通常是 Leaf</li>
<li>如果 Tensor 是用户创建的，那么即使<code>requires_grad=True</code>也是Leaf，意味着这些Tensor不是一个Op的结果，并且<code>grad_fn=None</code></li>
<li>只有Leaf Tensor 才会在<code>backward()</code>过程中保存梯度结果；如果需要获取那些non-leaf节点的grad，可以使用<code>Tensor.retain_grad()</code>来修改</li>
<li>第三条与第一条貌似冲突，其实不冲突，因为 <code>requires_grad=False</code>的含义是指这个 Tensor 的梯度不需要向后传播了，而不是不会计算该 Tensor 的梯度，也就是实际是指<code>grad_fn=None</code>。</li>
<li>从CPU拷贝到 GPU 上也算是一个 Op 操作，具体例子可以查看：<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html?highlight=is_leaf#torch.Tensor.is_leaf">torch.tensor.is_leaf</a></li>
</ul>
</li>
<li>
<p>Disabling Gradient Tracking</p>
<p>有时候需要停止一些 Tensor 的梯度后向传播，那些<code>requires_grad=True</code>的 Tensor 都会跟踪该Tensor 的计算历史，并支持梯度计算。所以要想阻止后向传播，有两种方式：</p>
<ul>
<li>使用 <code>torch.no_grad()</code> block 进行封装</li>
<li>使用 <code>detach()</code>，相当于新建了一个Tensor返回的，所以计算梯度更新这个新的 Tensor，之前旧的 Tensor 数值也会保持不变。</li>
</ul>
<p>下面的方式适合单个 Parameter 的梯度更新。</p>
<ul>
<li>设置<code>parameter.requires_grad=False</code></li>
<li>设置<code>parameter.grad=None</code>，优化器在根据梯度更新这个参数时，如果发现 <code>grad=None</code>，则略过当前参数，从而实现防止梯度反向传播的目的</li>
</ul>
<p>经过上述两种方式处理后的 Tensor 直接影响是，不会向后传播 Gradient，也不会发生数值变化。</p>
</li>
<li>
<p>Tensor Gradients and Jacobian Products</p>
<p>大部分情况下，Loss函数计算得到的是一个Scalar数值，计算梯度容易理解。但是当 Loss 是一个多维的Tensor时，反向传播计算的就是<code>Jacobian product</code>，而不是真正的梯度了。</p>
<p>一般来说，输入、输出都是 Tensor 时，反向传播得到的是一个<code>Jacobian matrix</code>，但是 pytorch 支持<code>Jacobian product</code>的计算，此时需要一个与输出Loss同等尺寸的Tensor作为<code>backward()</code>函数的输入。</p>
<p>下式中，<code>x, y</code>为输入输出，计算<code>y</code>对<code>x</code>的梯度时，引入的 <code>v</code> 就是上面提到的需要跟 <code>y</code> 尺寸相同的新引入的 Tensor，具体例子可参考<a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">Automatic Diff</a>下方。</p>
<p>$$y=f(x), J = \frac{\partial y}{\partial x}, v^T \cdot J$$</p>
</li>
<li>
<p>optimize steps</p>
<ol>
<li>call <code>optimizer.zero_grad()</code></li>
<li>call <code>loss.backward()</code></li>
<li>call <code>optimizer.step()</code></li>
</ol>
</li>
<li>
<p>其它</p>
<ul>
<li>每次<code>backward()</code>之后，创建的计算图都会被重置，从而支持每次 iter 之间修改数据的尺寸、条件判断修改计算图等，也就是对动态计算图的支持；如果想保留当前的计算图，可以在 <code>backward()</code>函数中设置<code>retain_graph=True</code></li>
<li>但是连续两次<code>backward()</code>时，同一个 Tensor 的梯度会被累加。</li>
</ul>
</li>
</ul>
<h2 id="extending-pytorch">Extending Pytorch</h2>
<p>主要参考：<a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">Extending Pytorch</a></p>
<h3 id="extending-autograd">Extending Autograd</h3>
<p>TODO</p>
<h2 id="optimizer">Optimizer</h2>
<p>实现自己的 Optimizer 的时候，需要继承<code>torch.optim.Optimizer</code>类。需要实现<code>__init__、__setstate__、step</code>等函数；然后将新 Optimizer 的参数，比如lr, eps, betas等参数保存到<code>defaults</code>字典中，并跟parameters一起传给Base类的<code>__init__</code>函数。<code>__setstate__</code>函数主要是为了比如在pickle等序列化中使用，并做必要的更新，比如<code>self.param_groups</code>里的成员。在 <code>step()</code>函数里，会更新<code>self.state</code>成员变量，然后后面更新的时候就可以直接从 <code>state</code> 里面取出来进行更新就可以了。</p>
<p>此外，defaults 字典里面的信息在<code>add_param_group()</code>函数里面被放入<code>self.param_groups</code>里面了，如lr, eps, betas等；特定Optimizer的相关数据放在<code>self.states</code>里面了，如Adam里面的 m / v 等。</p>
<p>具体例子可以参考 TIMM 库里的AdamW算法实现。</p>
        
            <p>深度学习中的一些可视化技术。</p>
<p>主要包括CAM、t-SNE两个方面，CAM又包括Gradient Free 以及 Gradient Based 两种实现思路；t-SNE更多的是用于高维空间在低维空间的可视化。</p>
<h2 id="cam">CAM</h2>
<p>再记录GradCam之前，可以先看下Cam算法的实现。参考博客是：<a href="https://zhuanlan.zhihu.com/p/269702192">万字长文：特征可视化技术(CAM)</a>。</p>
<h3 id="cam基础">CAM基础</h3>
<p>全称 Class Activation Mapping。也就是获取每个类别在Feature Map上关注点的分布，比如利用最后一层CNN的Feature Map，将所有的Channel加权融合为一个二维图片，然后这个二维图片就被认作激活图。以ResNet18为例，最后一层CNN的Feature Map包含512个Channel，如果单独可视化每个通道，则比较难理解，所以CAM会根据每个通道不同的贡献大小对所有的通道进行加权融合获取一张CAM。</p>
<p>效果如图-1。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam0.png" alt="图 - 1 CAM结果示意图">
    <figcaption>图 - 1 CAM结果示意图</figcaption>
    </center>
</figure></p>
<p>CAM实现的步骤如下：</p>
<ol>
<li>提取需要可视化的特征层，例如尺寸为7 * 7 * 512的张量</li>
<li>获取该张量的每个channel的权重，即长度为512的向量</li>
<li>对 Step1 中的张量按照 Step2 中的权重进行加权，获取尺寸为 7 * 7 的Map</li>
<li>对该Map进行归一化，并通过插值的方式</li>
</ol>
<p>上面提到，CAM可以分为Gradient Free / Gradient Based两种方式，两者的主要区别在于Step 2中计算Channel的权重方式不同，后者会利用梯度信息，前者不需要。</p>
<p>Gradient Based常见的算法包括</p>
<ul>
<li>Grad CAM (2016.10)</li>
<li>Grad CAM ++ (2017.10)</li>
<li>Smooth Grad-CAM++ (2019.08)</li>
</ul>
<p>Gradient Free常见的算法包括</p>
<ul>
<li>CAM (2015.12)</li>
<li>score-CAM (2019.10)</li>
<li>ss-CAM (2020.06)</li>
<li>Ablation-CAM (2020)</li>
</ul>
<h3 id="利用gap获取cam">利用GAP获取CAM</h3>
<p>属于Gradient Free类算法。</p>
<p>CAM的实现依赖于CNN卷积之后使用Global Average Pooling (GAP) 来实现；也就是说网络结构具有如下特征，经过若干层CNN得到Feature Map，然后利用GAP来压缩空间维度，然后压缩后的向量利用一个线性变换得到对应的类别预测。然后CAM就利用最后一个线性变换的权重作为每个类别的Channel的权重进行加权。示意图如图2所示，对于Australian terrier类的全连接权重为 $w_1, \ldots, w_n$。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam1.png" alt="图 - 2 CAM实现示意图">
    <figcaption>图 - 2 CAM实现示意图</figcaption>
    </center>
</figure></p>
<blockquote>
<p>Global average pooling outputs the spatial average of the feature map of each unit at the last convolutional layer. A weighted sum of these values is used to generate the final output. Similarly, we compute a weighted sum of the feature maps of the last convolutional layer to obtain our class activation maps.</p>
</blockquote>
<p>利用数学公式说明就是，设$f_k(x, y)$表示最后一层CNN的Feature Map中第 k 个unit (channel)在空间位置(x, y)处的数值，则GAP的计算就是$\sum_{x, y}f_k(x, y)$。然后对于类别 c，线性变换得到输入 Softmax 的数值，也就是 $S_c = \sum_k w_k^c F_k$，其中$w_k^c$也就是第 k channel 对类别 c 分类的重要度。</p>
<p>然后，论文定义class activation map ($M_c$) 为：</p>
<p>$$M_c(x, y) = \sum_k w_k^c f_k(x, y)$$</p>
<p>CAM这种Gradient Free算法的定义就是上面公示了。</p>
<p>作者分析了为啥不使用Global Max Pooling (GMP)，而是使用GAP。简单来说，GAP会让模型学习Object边界内的所有点的信息，而GMP则可以让模型只关注Object内最有区分性的空间位置即可，有很大可能 Object 内的其它位置对结果没有影响，毕竟对Max Pooling的结果没有影响。</p>
<p>CAM的缺点就是要求模型输出层之前需要使用 GAP，如果模型默认不是这一计算，则还需要替换成 GAP 重新进行训练。实现代码示例如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># 获取全连接层的权重</span>
<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_fc_weights <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>_modules<span style="color:#000;font-weight:bold">.</span>get(fc_layer)<span style="color:#000;font-weight:bold">.</span>weight<span style="color:#000;font-weight:bold">.</span>data
<span style="color:#998;font-style:italic"># 获取目标类别的权重作为特征权重</span>
weights<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_fc_weights[class_idx, :]
<span style="color:#998;font-style:italic"># 这里self.hook_a为最后一层特征图的输出</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> (weights<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> 
                <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>hook_a<span style="color:#000;font-weight:bold">.</span>squeeze(<span style="color:#099">0</span>))<span style="color:#000;font-weight:bold">.</span>sum(dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
<span style="color:#998;font-style:italic"># relu操作,去除负值</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> F<span style="color:#000;font-weight:bold">.</span>relu(batch_cams, inplace<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)
<span style="color:#998;font-style:italic"># 归一化操作</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_normalize(batch_cams)
</code></pre></td></tr></table>
</div>
</div><h3 id="grad-cam">Grad CAM</h3>
<p>此算法可以克服上面CAM算法中要求模型必须包含GAP的存在才行。基本思路是用梯度来计算Channel加权的权重，而且用ReLU去除权重为负数的那些channel。Grad CAM支持对任意一层CNN的 Feature Map 进行CAM可视化。</p>
<p>首先计算类别 c 的得分(score, before the softmax)对CNN层$A^k$的梯度，也就是$\frac{\partial y^c}{\partial A^k}$，然后这些题都在 $A^k$ 的空间维度上进行求平均，也就是 GAP（作者发现 GAP 好于 GMP），然后作为权重对$A^k$的channel进行加权，也就是</p>
<p>$$\alpha_k^c = \overbrace{\frac{1}{Z}\sum_i \sum_j}^\text{global average pooling} \underbrace{\frac{\partial y^c}{\partial A_{ij}^k}}_\text{gradients via backprop}$$</p>
<p>实验表明，越是浅层的CNN，Grad CAM的效果越差，因为这些层的感受野也越小。后就是ReLU的使用保留那些只起正向作用的空间点：</p>
<p>$$L_{\text{Grad-CAM}}^c = ReLU(\sum_k \alpha_k^c A^k)$$</p>
<p>注意，$y^c$在这里不一定必须是分类模型的class score，可以是任何可微分的激活函数的输出。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam2.png" alt="图 - 3 Grad-CAM实现示意图">
    <figcaption>图 - 3 Grad-CAM实现示意图</figcaption>
    </center>
</figure></p>
<p>示意图可以看出，Grad CAM支持多种模型的输出层结构以及对应的任务。为了实现更高分辨率的可视化，作者提出首先将$L_{\text{Grad-CAM}}$使用双线性插值进行上采样，然后与 Guided Backpropagation 的结果进行按元素乘的结果作为可视化结果。</p>
<p>另外一个方面，论文中提到使用Softmax之前的输入作为 $y^c$，但是某些代码中也使用了 Softmax 的输出作为 $y^c$，知乎文章里有给出为啥可能采用Softmax之前（论文中的做法）会效果更好。实现代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># 利用onehot的形式锁定目标类别</span>
one_hot <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">1</span>, output<span style="color:#000;font-weight:bold">.</span>size()[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]), dtype<span style="color:#000;font-weight:bold">=</span>np<span style="color:#000;font-weight:bold">.</span>float32)
one_hot[<span style="color:#099">0</span>][index] <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>
one_hot <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>from_numpy(one_hot)<span style="color:#000;font-weight:bold">.</span>requires_grad_(<span style="color:#999">True</span>) 
<span style="color:#998;font-style:italic"># 获取目标类别的输出,该值带有梯度链接关系,可进行求导操作</span>
one_hot <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(one_hot <span style="color:#000;font-weight:bold">*</span> output)
<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>zero_grad()
one_hot<span style="color:#000;font-weight:bold">.</span>backward(retain_graph<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) <span style="color:#998;font-style:italic"># backward 求导</span>
<span style="color:#998;font-style:italic"># 获取对应特征层的梯度map</span>
grads_val <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>extractor<span style="color:#000;font-weight:bold">.</span>get_gradients()[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]<span style="color:#000;font-weight:bold">.</span>cpu()<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>numpy()
target <span style="color:#000;font-weight:bold">=</span> features[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]<span style="color:#000;font-weight:bold">.</span>cpu()<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>numpy()[<span style="color:#099">0</span>, :] <span style="color:#998;font-style:italic"># 获取目标特征输出</span>
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>mean(grads_val, axis<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">2</span>, <span style="color:#099">3</span>))[<span style="color:#099">0</span>, :] <span style="color:#998;font-style:italic"># 利用GAP操作, 获取特征权重</span>
cam <span style="color:#000;font-weight:bold">=</span> weights<span style="color:#000;font-weight:bold">.</span>dot(target<span style="color:#000;font-weight:bold">.</span>reshape((nc, h <span style="color:#000;font-weight:bold">*</span> w)))
<span style="color:#998;font-style:italic"># relu操作,去除负值, 并缩放到原图尺寸</span>
cam <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>maximum(cam, <span style="color:#099">0</span>)
cam <span style="color:#000;font-weight:bold">=</span> cv2<span style="color:#000;font-weight:bold">.</span>resize(cam, <span style="color:#0086b3">input</span><span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">2</span>:])
<span style="color:#998;font-style:italic"># 归一化操作</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_normalize(batch_cams)
</code></pre></td></tr></table>
</div>
</div><h3 id="grad-cam-1">Grad CAM++</h3>
<p>Grad CAM++ 是对 Grad CAM 的进一步改进，优势在于定位更加精准，也更适用于图像中包含不止一个目标类别物体的情况。</p>
<p>具体来说，Grad CAM认为 Feature Map上每一个点的重要度是一样的（使用GAP得到），而Grad CAM++认为每个位置上点的贡献度不同，因此额外增加了一个权重用来表示Feature Map上每个元素的重要度。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam3.png" alt="图 - 4 Grad-CAM&#43;&#43;实现示意图">
    <figcaption>图 - 4 Grad-CAM&#43;&#43;实现示意图</figcaption>
    </center>
</figure></p>
<p>这里的重点也是权重$\alpha_{ij}^{kc}$的计算。</p>
<p>$$\alpha_{ij}^{kc} = \frac{(\frac{\partial S^c}{\partial A_{ij}^k})^2}{2(\frac{\partial S^c}{\partial A^k_{ij}})^2 + \sum_a \sum_b A_{ab}^k (\frac{\partial S^c}{\partial A^k_{ij}})^3}$$</p>
<p>知乎参考博客里还有其他一些比较新的Feature Map的可视化方法，可以去参考一下，这里略过。</p>
<h2 id="t-sne">t-SNE</h2>
<p>主要的参考是 <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding</a>。</p>
<p>这个算法其实适用于展示高维数据的，也就是将高维数据降维到低维数据，然后方便查看两个数据点之间的距离。与其他降维算法（如PCA）相比，t-SNE创建一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。最后一句话可以通过下面的例子进行说明。</p>
<p>整体来说，t-SNE是将高维数据映射到低维空间，所以就涉及到几个问题：这个映射过程怎么确定，等价于低维空间的点应该怎么选择；然后就是维度灾难，也就是原始高维空间的数据计算距离的时候会受维度灾难的影响。</p>
<p>t-SNE其实更是为了更好的可视化相似性，而不是为了降维。整体思路也是保证映射前后，也就是高维空间、低维空间中，相似的点继续保持相似，也就是距离比较近。采取的方法是对两个空间中的点的距离进行概率建模，也就是一个点与其它所有点的距离映射成一个概率分布。首先是高维空间，将欧氏距离映射成高斯分布。</p>
<p>N个高维空间点，$x_1, \ldots, x_N$，计算概率$p_{ij}$为与点$x_i, x_j$之间的欧式距离成正比。</p>
<p>$$p_{j|i} = \frac{\exp{(-\parallel x_i - x_j\parallel^2} / 2\sigma_i^2)}{\sum_{k\neq i}\exp(-\parallel x_i - x_k \parallel^2 / 2\sigma_i^2)}$$</p>
<p>其中，$j \neq i$，并且有$p_{i|i}=0$以及$\sum_j p_{j|i}=1, \forall i$。这个式子是指以$x_i$为中心点的概率分布中，点$x_j$的概率。由于距离是对称的，所以定义：</p>
<p>$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}$$</p>
<p>此时也就有$p_{ij}=p_{ji}, p_{ii}=0, \sum_{i,j}p_{ij}=1$了。</p>
<p>其中参数$\sigma_i$是由高斯分布的perplexity(困惑度)与二分法（bisection method）计算得到的困惑度相等来得到的。就体现在，越密集的数据空间中，$\sigma_i$越小，也就是低困惑度更关注局部数据点，高困惑度更关注全局结构。</p>
<p>这里需要注意的地方在于，$x_i$在高维空间中会面临维度灾难，此时欧氏距离没有区分性，也就是任何两个点之间看上去都不是那么相似了，毕竟高维空间里这些点还是非常稀疏的，也就导致计算的概率分布$p_{ij}$也区分性不大，针对这个问题，有一些方式是基于 <a href="https://en.wikipedia.org/wiki/Intrinsic_dimension">intrinsic dimension</a> 的 <a href="https://en.wikipedia.org/wiki/Power_transform">power transform</a> 来缓解这个问题。</p>
<p>回到上文，t-SNE的目标也是获得对应的到d维空间的映射，$y_1, \ldots, y_N, y \in \mathbb{R}^d$，同时可以反映出高维空间的概率分布$p_{ij}$。具体做法是，计算低维空间里点的概率分布，用$q_{ij}$表示。</p>
<p>$$q_{ij} = \frac{(1 + \parallel y_i - y_j \parallel^2)a^{-1}}{\sum_k \sum_{l\neq k}(1 + \parallel y_k - y_l \parallel^2)^{-1}}$$</p>
<p>这里也有$q_{ii}=0$。而且不再使用高斯分布计算概率了，而是选择student t-distribution来计算概率分布，好处是这个分布相比高斯分布，是胖尾的，也就可以将不相似的两个点用更远的低维映射点进行拟合。</p>
<p>优化映射过程是基于KL散度这个损失函数来实现的，高维、低维空间的分布分别用$P, Q$表示。</p>
<p>$$\mathrm{KL}(P\parallel Q) = \sum_{i\neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$</p>
<p>通过优化上述损失函数，利用梯度下降法来更新$y_i$的取值。优化的结果也就可以反映高维空间中点之间的相似度了。</p>
<p>下面是一个用在多模态预训练中文本 embedding 矩阵使用 t-SNE 可视化的结果，直观来看分布是比较均匀的。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/txt_embedding_0.png" alt="图 - 5 embedding 矩阵的t-SNE可视化">
    <figcaption>图 - 5 embedding 矩阵的t-SNE可视化</figcaption>
    </center>
</figure></p>
<p>对应PCA降维之后可视化的结果如下，就不是这么均匀了。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/txt_embedding_1.png" alt="图 - 6 embedding 矩阵的PCA可视化">
    <figcaption>图 - 6 embedding 矩阵的PCA可视化</figcaption>
    </center>
</figure></p>
        
            <p>Attention is all your need.</p>
<h2 id="基础">基础</h2>
<h3 id="attention">Attention</h3>
<p>Attention 定义上是一个映射函数，输入<code>Q,K,V</code>等向量，输出是一个新的向量。具体定义如下：</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.</p>
</blockquote>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>这里 <code>Dot-Product</code> 是一种计算Attention权重的方式（另一种常见的方式是 Additive Attention）。输入Q与K都是向量，维度为 $d_k$，输入 V 也是向量，维度为 $d_v$，然后权重计算过程是将 Q 与所有的 K 计算向量点乘（Dot-Product）。</p>
<p>所谓的 <code>Scaled</code> 体现在将上述点乘结果除以 $\sqrt{d_k}$。为什么是除以这个数？主要原因是，两个 $d_k$ 维的矩阵乘（矩阵元素是mean 0, variance 1 生成的随机数），结果矩阵的方差就是$d_k$。所以，如果这里不进行 Scale，那么得到的矩阵数值就会越来越大，导致后面的 Softmax 饱和。</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer0.png" alt="图-1 Scaled Dot-Product Attention示意图">
    <figcaption>图-1 Scaled Dot-Product Attention示意图</figcaption>
    </center>
</figure></p>
<p>补充一下 Additive Attention。具体实现是通过全连接映射然后按元素加得到，公式如下。这里为什么选择 Dot-Product 而不是 Additive Attention 呢？而且两者的理论计算复杂度差不多。论文里也给出了解释，就是Dot-Product在实际计算中其实是更快的，毕竟矩阵乘法被研究、优化的更多。</p>
<p>$$a(q, k) = w_v^T \tanh (W_q q + W_k k) \in \mathbb{R}$$</p>
<h3 id="multi-head">Multi-head</h3>
<p>作者发现，用不同的Linear Projection 来将 Q, K, V 进行映，然后对应的计算 Attention，最终将结果拼接起来的效果比使用一个单独的 Attention 效果更好。下面的公式与图2就可以很好的说明计算过程了，实际实现可以通过先合并 Linear Projection 的权重，然后在经过 Reshape 完成。</p>
<p>$$\begin{gather*}
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(head_1, \dots, head_n)W^O  \<br>
where, head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{gather*}$$</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer1.png" alt="图-2 Multi-Head Attention示意图">
    <figcaption>图-2 Multi-Head Attention示意图</figcaption>
    </center>
</figure></p>
<h3 id="position-wise-ffn">Position-wise FFN</h3>
<p>实现上来看是两层全连接，并且第一层一般会将输入Tensor的channel个数扩展expansion(=4)倍，然后第二层全连接在恢复原来的 channel 个数。</p>
<p>为什么叫 Position-wise 呢？按照论文的说法，我猜这里的Position是指 Depth 维度上的位置，体现在相同层的不同位置的 Token 公用相同的 Lienar Projection 权重矩阵，但是不同层上使用不同的 Linear Projection。</p>
<blockquote>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer</p>
</blockquote>
<p>不过 D2L 中李沐的说法是：</p>
<blockquote>
<p>The positionwise feed-forward network transforms the representation at all the sequence positions using the same MLP. This is why we call it positionwise.</p>
</blockquote>
<h2 id="encoder-decoder结构">Encoder-Decoder结构</h2>
<p>一个方面是如何将 Encoder 的信息传递给 Decoder，有两种做法，一种是指在Decoder的第一个输入位置上使用，另一种是在Decoder的每一次输入上都使用。</p>
<h3 id="seq2seq">Seq2Seq</h3>
<p>这里参考<a href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html">Sequence to sequence leanring - d2l</a>中的讲解。</p>
<p>具体的 Encoder - Decoder 部分这里基于 GRU 来实现。输入尺寸为：$(batch_size, num_steps, embed_size)$；GRU 的计算包含两个输出，一个是GRU 最后输出结果output，尺寸仍然是: $(num_steps, batch_size, embed_size)$，相当于每一步（共num_steps，可认为是 num_steps 个 Toke）都输出了一个新的 embed_size 大小的向量；另一个输出是隐空间变量 states，尺寸是 $(num_layers, batch_size, num_hiddens)$，相当于是当前输入Token与上一个Token对应的隐变量共同作用生成了当前Token对应的隐变量，这个隐变量就包含了前面所有 Token 的信息。 当前 token 的 output 与隐变量 state 之间的关系是：<code>output = Mlp(state)</code>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">rnn</span>(inputs, state, params):
    <span style="color:#998;font-style:italic"># Shape of `inputs`: (`num_steps`, `batch_size`, `vocab_size`)</span>
    W_xh, W_hh, b_h, W_hq, b_q <span style="color:#000;font-weight:bold">=</span> params
    H, <span style="color:#000;font-weight:bold">=</span> state
    outputs <span style="color:#000;font-weight:bold">=</span> []
    <span style="color:#998;font-style:italic"># Shape of `X`: (`batch_size`, `vocab_size`)</span>
    <span style="color:#000;font-weight:bold">for</span> X <span style="color:#000;font-weight:bold">in</span> inputs:
        H <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>tanh(np<span style="color:#000;font-weight:bold">.</span>dot(X, W_xh) <span style="color:#000;font-weight:bold">+</span> np<span style="color:#000;font-weight:bold">.</span>dot(H, W_hh) <span style="color:#000;font-weight:bold">+</span> b_h)
        Y <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>dot(H, W_hq) <span style="color:#000;font-weight:bold">+</span> b_q
        outputs<span style="color:#000;font-weight:bold">.</span>append(Y)
    <span style="color:#000;font-weight:bold">return</span> np<span style="color:#000;font-weight:bold">.</span>concatenate(outputs, axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>), (H,)
</code></pre></td></tr></table>
</div>
</div><p>上面说完 Encoder 部分，Decoder 部分结构相同，但重要的是 Decoder 部分的输出应该怎么决定。</p>
<p>输入主要包含两个部分，首先是起始Token，这里起始 Token 是一个特殊字符，<code>&lt;bos&gt;</code>；另一个部分就是隐变量的确定，这里使用 Encoder 输出的隐变量作为初始隐变量，注意这里 Encoder - Decoder 需要使用相同的层数，这样隐变量的尺寸才匹配，即：$(num_layers, batch size, embed_size)$。下面给出的示例代码中，还会将 Encoder 输出的最后一层的隐变量与输入 X 拼接起来进行计算。Decoder 的输出就是$(batch size, num steps, vocab size)$。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">Seq2SeqDecoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>Decoder):
    <span style="color:#d14">&#34;&#34;&#34;The RNN decoder for sequence to sequence learning.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(Seq2SeqDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, embed_size)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>rnn <span style="color:#000;font-weight:bold">=</span> rnn<span style="color:#000;font-weight:bold">.</span>GRU(num_hiddens, num_layers, dropout<span style="color:#000;font-weight:bold">=</span>dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dense(vocab_size, flatten<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">init_state</span>(<span style="color:#999">self</span>, enc_outputs, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#000;font-weight:bold">return</span> enc_outputs[<span style="color:#099">1</span>]

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        <span style="color:#998;font-style:italic"># The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)</span>
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X)<span style="color:#000;font-weight:bold">.</span>swapaxes(<span style="color:#099">0</span>, <span style="color:#099">1</span>)
        <span style="color:#998;font-style:italic"># `context` shape: (`batch_size`, `num_hiddens`)</span>
        context <span style="color:#000;font-weight:bold">=</span> state[<span style="color:#099">0</span>][<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]      <span style="color:#998;font-style:italic"># 最后一层对应的隐变量</span>
        <span style="color:#998;font-style:italic"># Broadcast `context` so it has the same `num_steps` as `X`</span>
        context <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>broadcast_to(
            context, (X<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>], context<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>], context<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">1</span>]))
        X_and_context <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate((X, context), <span style="color:#099">2</span>)
        output, state <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>rnn(X_and_context, state)
        output <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense(output)<span style="color:#000;font-weight:bold">.</span>swapaxes(<span style="color:#099">0</span>, <span style="color:#099">1</span>)
        <span style="color:#998;font-style:italic"># `output` shape: (`batch_size`, `num_steps`, `vocab_size`)</span>
        <span style="color:#998;font-style:italic"># `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)</span>
        <span style="color:#000;font-weight:bold">return</span> output, state
</code></pre></td></tr></table>
</div>
</div><h3 id="transformer-中的实现">Transformer 中的实现</h3>
<p>Encoder 部分简单的是 Transformer 层的堆叠。Transformer 层包含两个sublayer，分别是 MultiHead Self Attention 以及 Positionwise FFN，这两个 sublayer 都会通过残差连接并紧跟着计算一个 LayerNorm （这里不讨论pre-norm的实现）。</p>
<p>Decoder 部分相比于 Encoder 的两个 sublayer 构成，多了一个 cross-attention 的层。cross-attention的主要区别在于输入的 K, V 来自于对应的 Encoder 层，Query 来自于 Decoder 中上一层的 MultiHead Self Attention的输出。整体结构如下。</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer2.png" alt="图-3 Transformer Encoder-Decoder 示意图">
    <figcaption>图-3 Transformer Encoder-Decoder 示意图</figcaption>
    </center>
</figure></p>
<p>下面给出了MXNet实现代码，非常详细，但是解答了下面几个疑问。</p>
<ul>
<li>Decoder 最开始的输入是<code>&lt;bos&gt;</code>，在训练时，这个也是拼接在最前面的</li>
<li>Decoder 中每一层中 Cross Attention 的 K, V 都是相同的，都来自于 Encoder 的最后输出</li>
<li>在最大 num_steps 限制下，最后一个元素是 <code>&lt;eos&gt;</code> 时则退出 Decoder 部分</li>
</ul>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 74
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 78
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 79
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 80
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 81
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 82
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 83
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 84
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 85
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 86
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 87
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 88
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 89
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 90
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 91
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 92
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 93
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 94
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 95
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 96
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 97
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 98
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 99
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">100
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">101
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">102
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">103
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">104
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">105
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">106
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">107
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">108
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">109
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">110
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">111
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">112
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">113
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">114
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">115
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">116
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">117
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">118
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">119
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">120
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">121
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">122
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">123
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">124
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">125
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">126
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">127
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">128
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">129
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">130
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">131
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">132
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">133
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">134
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">135
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">136
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">137
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">138
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">139
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">140
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">141
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">142
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">143
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">144
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">145
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">146
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">147
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">148
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">149
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">150
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">151
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">152
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">153
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">154
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">155
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">156
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">157
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">158
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">159
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">160
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">161
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">162
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">163
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">164
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">165
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">166
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">167
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">168
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">169
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">170
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">171
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">172
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">173
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">174
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">175
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">176
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">177
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">178
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">179
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">180
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">181
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">182
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">183
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">184
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">185
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">186
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">187
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">188
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">189
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">190
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">191
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">192
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">193
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">194
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">195
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">196
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">197
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">198
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">199
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">200
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">201
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">202
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">203
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">204
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">205
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">206
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">207
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">208
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">209
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">210
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">211
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">212
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">213
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">214
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">215
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">216
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">EncoderDecoder</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#d14">&#34;&#34;&#34;The base class for the encoder-decoder architecture.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, encoder, decoder, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(EncoderDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>encoder <span style="color:#000;font-weight:bold">=</span> encoder
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder <span style="color:#000;font-weight:bold">=</span> decoder

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, enc_X, dec_X, <span style="color:#000;font-weight:bold">*</span>args):
        enc_outputs <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>encoder(enc_X, <span style="color:#000;font-weight:bold">*</span>args)
        dec_state <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>init_state(enc_outputs, <span style="color:#000;font-weight:bold">*</span>args)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder(dec_X, dec_state)

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">EncoderBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#d14">&#34;&#34;&#34;Transformer encoder block.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, num_hiddens, ffn_num_hiddens, num_heads, dropout,
                 use_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(EncoderBlock, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                dropout, use_bias)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn <span style="color:#000;font-weight:bold">=</span> PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, valid_lens):
        Y <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1(X, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention(X, X, X, valid_lens))
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2(Y, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn(Y))

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">TransformerEncoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>Encoder):
    <span style="color:#d14">&#34;&#34;&#34;Transformer encoder.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, use_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(TransformerEncoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens <span style="color:#000;font-weight:bold">=</span> num_hiddens
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>PositionalEncoding(num_hiddens, dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential()
        <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_layers):
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks<span style="color:#000;font-weight:bold">.</span>add(
                EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
                             use_bias))

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, valid_lens, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#998;font-style:italic"># Since positional encoding values are between -1 and 1, the embedding</span>
        <span style="color:#998;font-style:italic"># values are multiplied by the square root of the embedding dimension</span>
        <span style="color:#998;font-style:italic"># to rescale before they are summed up</span>
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X) <span style="color:#000;font-weight:bold">*</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention_weights <span style="color:#000;font-weight:bold">=</span> [<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#0086b3">len</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks)
        <span style="color:#000;font-weight:bold">for</span> i, blk <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks):
            X <span style="color:#000;font-weight:bold">=</span> blk(X, valid_lens)
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention_weights[
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
        <span style="color:#000;font-weight:bold">return</span> X

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">DecoderBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#998;font-style:italic"># The `i`-th block in the decoder</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, num_hiddens, ffn_num_hiddens, num_heads, dropout, i,
                 <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(DecoderBlock, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i <span style="color:#000;font-weight:bold">=</span> i
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention1 <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention2 <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn <span style="color:#000;font-weight:bold">=</span> PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm3 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        enc_outputs, enc_valid_lens <span style="color:#000;font-weight:bold">=</span> state[<span style="color:#099">0</span>], state[<span style="color:#099">1</span>]
        <span style="color:#998;font-style:italic"># During training, all the tokens of any output sequence are processed</span>
        <span style="color:#998;font-style:italic"># at the same time, so `state[2][self.i]` is `None` as initialized.</span>
        <span style="color:#998;font-style:italic"># When decoding any output sequence token by token during prediction,</span>
        <span style="color:#998;font-style:italic"># `state[2][self.i]` contains representations of the decoded output at</span>
        <span style="color:#998;font-style:italic"># the `i`-th block up to the current time step</span>
        <span style="color:#000;font-weight:bold">if</span> state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i] <span style="color:#000;font-weight:bold">is</span> <span style="color:#999">None</span>:
            key_values <span style="color:#000;font-weight:bold">=</span> X
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#998;font-style:italic"># 这里是只使用已预测的Token进行计算</span>
            key_values <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate((state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i], X), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)     
        state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i] <span style="color:#000;font-weight:bold">=</span> key_values

        <span style="color:#000;font-weight:bold">if</span> autograd<span style="color:#000;font-weight:bold">.</span>is_training():
            batch_size, num_steps, _ <span style="color:#000;font-weight:bold">=</span> X<span style="color:#000;font-weight:bold">.</span>shape
            <span style="color:#998;font-style:italic"># Shape of `dec_valid_lens`: (`batch_size`, `num_steps`), where</span>
            <span style="color:#998;font-style:italic"># every row is [1, 2, ..., `num_steps`]</span>
            dec_valid_lens <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>tile(np<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#099">1</span>, num_steps <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>, ctx<span style="color:#000;font-weight:bold">=</span>X<span style="color:#000;font-weight:bold">.</span>ctx),
                                     (batch_size, <span style="color:#099">1</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            dec_valid_lens <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>

        <span style="color:#998;font-style:italic"># Self-attention</span>
        X2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention1(X, key_values, key_values, dec_valid_lens)
        Y <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1(X, X2)
        <span style="color:#998;font-style:italic"># Encoder-decoder attention. Shape of `enc_outputs`:</span>
        <span style="color:#998;font-style:italic"># (`batch_size`, `num_steps`, `num_hiddens`)</span>
        <span style="color:#998;font-style:italic">## 这里使用 Encoder 最后的输出的 enc_outputs 当作 K, V 进行计算！！</span>
        Y2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2(Y, Y2)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm3(Z, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn(Z)), state

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">TransformerDecoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>AttentionDecoder):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(TransformerDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens <span style="color:#000;font-weight:bold">=</span> num_hiddens
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_layers <span style="color:#000;font-weight:bold">=</span> num_layers
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>PositionalEncoding(num_hiddens, dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential()
        <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_layers):
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks<span style="color:#000;font-weight:bold">.</span>add(
                DecoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
                             i))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dense(vocab_size, flatten<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">init_state</span>(<span style="color:#999">self</span>, enc_outputs, enc_valid_lens, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#000;font-weight:bold">return</span> [enc_outputs, enc_valid_lens, [<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_layers]

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X) <span style="color:#000;font-weight:bold">*</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights <span style="color:#000;font-weight:bold">=</span> [[<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#0086b3">len</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks) <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">2</span>)]
        <span style="color:#000;font-weight:bold">for</span> i, blk <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks):
            X, state <span style="color:#000;font-weight:bold">=</span> blk(X, state)
            <span style="color:#998;font-style:italic"># Decoder self-attention weights</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights[<span style="color:#099">0</span>][
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention1<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
            <span style="color:#998;font-style:italic"># Encoder-decoder attention weights</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights[<span style="color:#099">1</span>][
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention2<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense(X), state

    <span style="color:#3c5d5d;font-weight:bold">@property</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">attention_weights</span>(<span style="color:#999">self</span>):
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights

num_hiddens, num_layers, dropout, batch_size, num_steps <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">32</span>, <span style="color:#099">2</span>, <span style="color:#099">0.1</span>, <span style="color:#099">64</span>, <span style="color:#099">10</span>
lr, num_epochs, device <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0.005</span>, <span style="color:#099">200</span>, d2l<span style="color:#000;font-weight:bold">.</span>try_gpu()
ffn_num_hiddens, num_heads <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">64</span>, <span style="color:#099">4</span>

train_iter, src_vocab, tgt_vocab <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>load_data_nmt(batch_size, num_steps)

encoder <span style="color:#000;font-weight:bold">=</span> TransformerEncoder(<span style="color:#0086b3">len</span>(src_vocab), num_hiddens, ffn_num_hiddens,
                             num_heads, num_layers, dropout)
decoder <span style="color:#000;font-weight:bold">=</span> TransformerDecoder(<span style="color:#0086b3">len</span>(tgt_vocab), num_hiddens, ffn_num_hiddens,
                             num_heads, num_layers, dropout)
net <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>EncoderDecoder(encoder, decoder)

d2l<span style="color:#000;font-weight:bold">.</span>train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
engs <span style="color:#000;font-weight:bold">=</span> [<span style="color:#d14">&#39;go .&#39;</span>, <span style="color:#d14">&#34;i lost .&#34;</span>, <span style="color:#d14">&#39;he</span><span style="color:#d14">\&#39;</span><span style="color:#d14">s calm .&#39;</span>, <span style="color:#d14">&#39;i</span><span style="color:#d14">\&#39;</span><span style="color:#d14">m home .&#39;</span>]
fras <span style="color:#000;font-weight:bold">=</span> [<span style="color:#d14">&#39;va !&#39;</span>, <span style="color:#d14">&#39;j</span><span style="color:#d14">\&#39;</span><span style="color:#d14">ai perdu .&#39;</span>, <span style="color:#d14">&#39;il est calme .&#39;</span>, <span style="color:#d14">&#39;je suis chez moi .&#39;</span>]
<span style="color:#000;font-weight:bold">for</span> eng, fra <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">zip</span>(engs, fras):
    translation, dec_attention_weight_seq <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, <span style="color:#999">True</span>)
    <span style="color:#000;font-weight:bold">print</span>(f<span style="color:#d14">&#39;{eng} =&gt; {translation}, &#39;</span>,
          f<span style="color:#d14">&#39;bleu {d2l.bleu(translation, fra, k=2):.3f}&#39;</span>)

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">predict_seq2seq</span>(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>):
    <span style="color:#d14">&#34;&#34;&#34;Predict for sequence to sequence.&#34;&#34;&#34;</span>
    src_tokens <span style="color:#000;font-weight:bold">=</span> src_vocab[src_sentence<span style="color:#000;font-weight:bold">.</span>lower()<span style="color:#000;font-weight:bold">.</span>split(<span style="color:#d14">&#39; &#39;</span>)] <span style="color:#000;font-weight:bold">+</span> [
        src_vocab[<span style="color:#d14">&#39;&lt;eos&gt;&#39;</span>]]
    enc_valid_len <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([<span style="color:#0086b3">len</span>(src_tokens)], ctx<span style="color:#000;font-weight:bold">=</span>device)
    src_tokens <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>truncate_pad(src_tokens, num_steps, src_vocab[<span style="color:#d14">&#39;&lt;pad&gt;&#39;</span>])
    <span style="color:#998;font-style:italic"># Add the batch axis</span>
    enc_X <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>expand_dims(np<span style="color:#000;font-weight:bold">.</span>array(src_tokens, ctx<span style="color:#000;font-weight:bold">=</span>device), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    enc_outputs <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>encoder(enc_X, enc_valid_len)
    dec_state <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>init_state(enc_outputs, enc_valid_len)
    <span style="color:#998;font-style:italic"># Add the batch axis</span>
    <span style="color:#998;font-style:italic">## 最开始的是 &#39;&lt;bos&gt;&#39;</span>
    dec_X <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>expand_dims(np<span style="color:#000;font-weight:bold">.</span>array([tgt_vocab[<span style="color:#d14">&#39;&lt;bos&gt;&#39;</span>]], ctx<span style="color:#000;font-weight:bold">=</span>device), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    output_seq, attention_weight_seq <span style="color:#000;font-weight:bold">=</span> [], []
    <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_steps):
        Y, dec_state <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>decoder(dec_X, dec_state)
        <span style="color:#998;font-style:italic"># We use the token with the highest prediction likelihood as the input</span>
        <span style="color:#998;font-style:italic"># of the decoder at the next time step</span>
        dec_X <span style="color:#000;font-weight:bold">=</span> Y<span style="color:#000;font-weight:bold">.</span>argmax(axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>)
        pred <span style="color:#000;font-weight:bold">=</span> dec_X<span style="color:#000;font-weight:bold">.</span>squeeze(axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#d14">&#39;int32&#39;</span>)<span style="color:#000;font-weight:bold">.</span>item()
        <span style="color:#998;font-style:italic"># Save attention weights (to be covered later)</span>
        <span style="color:#000;font-weight:bold">if</span> save_attention_weights:
            attention_weight_seq<span style="color:#000;font-weight:bold">.</span>append(net<span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>attention_weights)
        <span style="color:#998;font-style:italic"># Once the end-of-sequence token is predicted, the generation of the</span>
        <span style="color:#998;font-style:italic"># output sequence is complete</span>
        <span style="color:#000;font-weight:bold">if</span> pred <span style="color:#000;font-weight:bold">==</span> tgt_vocab[<span style="color:#d14">&#39;&lt;eos&gt;&#39;</span>]:
            <span style="color:#000;font-weight:bold">break</span>
        output_seq<span style="color:#000;font-weight:bold">.</span>append(pred)
    <span style="color:#000;font-weight:bold">return</span> <span style="color:#d14">&#39; &#39;</span><span style="color:#000;font-weight:bold">.</span>join(tgt_vocab<span style="color:#000;font-weight:bold">.</span>to_tokens(output_seq)), attention_weight_seq

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">train_seq2seq</span>(net, data_iter, lr, num_epochs, tgt_vocab, device):
    <span style="color:#d14">&#34;&#34;&#34;Train a model for sequence to sequence.&#34;&#34;&#34;</span>
    net<span style="color:#000;font-weight:bold">.</span>initialize(init<span style="color:#000;font-weight:bold">.</span>Xavier(), force_reinit<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>, ctx<span style="color:#000;font-weight:bold">=</span>device)
    trainer <span style="color:#000;font-weight:bold">=</span> gluon<span style="color:#000;font-weight:bold">.</span>Trainer(net<span style="color:#000;font-weight:bold">.</span>collect_params(), <span style="color:#d14">&#39;adam&#39;</span>,
                            {<span style="color:#d14">&#39;learning_rate&#39;</span>: lr})
    loss <span style="color:#000;font-weight:bold">=</span> MaskedSoftmaxCELoss()
    animator <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Animator(xlabel<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;epoch&#39;</span>, ylabel<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;loss&#39;</span>,
                            xlim<span style="color:#000;font-weight:bold">=</span>[<span style="color:#099">10</span>, num_epochs])
    <span style="color:#000;font-weight:bold">for</span> epoch <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_epochs):
        timer <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Timer()
        metric <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Accumulator(<span style="color:#099">2</span>)  <span style="color:#998;font-style:italic"># Sum of training loss, no. of tokens</span>
        <span style="color:#000;font-weight:bold">for</span> batch <span style="color:#000;font-weight:bold">in</span> data_iter:
            X, X_valid_len, Y, Y_valid_len <span style="color:#000;font-weight:bold">=</span> [
                x<span style="color:#000;font-weight:bold">.</span>as_in_ctx(device) <span style="color:#000;font-weight:bold">for</span> x <span style="color:#000;font-weight:bold">in</span> batch]
            bos <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([tgt_vocab[<span style="color:#d14">&#39;&lt;bos&gt;&#39;</span>]] <span style="color:#000;font-weight:bold">*</span> Y<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>],
                           ctx<span style="color:#000;font-weight:bold">=</span>device)<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)
            dec_input <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>concat([bos, Y[:, :<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]], <span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># Teacher forcing</span>
            <span style="color:#000;font-weight:bold">with</span> autograd<span style="color:#000;font-weight:bold">.</span>record():
                Y_hat, _ <span style="color:#000;font-weight:bold">=</span> net(X, dec_input, X_valid_len)
                l <span style="color:#000;font-weight:bold">=</span> loss(Y_hat, Y, Y_valid_len)
            l<span style="color:#000;font-weight:bold">.</span>backward()
            d2l<span style="color:#000;font-weight:bold">.</span>grad_clipping(net, <span style="color:#099">1</span>)
            num_tokens <span style="color:#000;font-weight:bold">=</span> Y_valid_len<span style="color:#000;font-weight:bold">.</span>sum()
            trainer<span style="color:#000;font-weight:bold">.</span>step(num_tokens)
            metric<span style="color:#000;font-weight:bold">.</span>add(l<span style="color:#000;font-weight:bold">.</span>sum(), num_tokens)
        <span style="color:#000;font-weight:bold">if</span> (epoch <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">10</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:
            animator<span style="color:#000;font-weight:bold">.</span>add(epoch <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>, (metric[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">/</span> metric[<span style="color:#099">1</span>],))
    <span style="color:#000;font-weight:bold">print</span>(f<span style="color:#d14">&#39;loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} &#39;</span>

</code></pre></td></tr></table>
</div>
</div>
        
            <p>VOLO论文。</p>
<h2 id="outlook-attention">Outlook Attention</h2>
<p>关于论文里主要的 Outlook Attenion 模块的实现有下面几点理解。</p>
<ul>
<li>
<p>Unfold 计算</p>
<p>实现过程是将输入 Tensor 在划窗内的数据Flatten 成列，Flatten 过程中按照 Row-Major （第一行 -&gt; 第二行 -&gt; 第N行） 的方式完成；窗口滑动过程中得到新的列则拼接在后面。假设 unfold 操作的 kernel size 为 K，输入Tensor的尺寸为 $C \times H \times W$，则 unfold 得到的结果为 $(C \cdot K^2) \times (H \cdot W)$，最后一维表示考虑 padding 后，窗口一共滑动了 $H\cdot W$次。</p>
</li>
<li>
<p>Fold 计算</p>
<p>此计算是 Unfold 的反过程，并且将恢复过程中对应同一个位置(i, j)的$K^2$结果求和作为该位置上新的结果。</p>
</li>
<li>
<p>与 Convolution 的区别</p>
<p>卷积计算过程是将以 (i, j) 为中心的窗口内的元素进行加权求和，权重为对应的 kernel 数据；Outlook Attention 计算过程是将 (i, j) 位置的元素在不同的滑窗内计算的结果进行求和，在不同滑创内的结果是指，该窗口内的 $K \times K$ 个元素基于 $K^2 \times K^2$ 个权重得到 $K \times K$个位置上的向量，所以滑窗内每个位置上都有一个对应当前滑窗的结果，Outlook Attention 也就是将(i, j)位置上向量参与的所有的滑窗内的结果求和得到新的向量。</p>
<p>以$3 \times 3$的滑窗为例，则输入 Tensor 每个位置上的元素一共参与了 9 个滑窗，则结果就是这9个滑窗内的结果求和，而滑窗内的结果是根据该滑窗内的9个元素加权平均（权重经过 Softmax 了）得来的。</p>
</li>
<li>
<p>论文中的伪代码</p>
<p>其中比较奇怪的一个地方在于，<code>v=v_pj(x).permute(2, 1, 0)</code> 得到的尺寸是$(C, W, H)$，为什么 W / H 这两个维度还需要转置呢？</p>
<p>目前的想法是作者写错了，因为代码实现里还是 (C, H, W) 的顺序。伪代码里mul(a, v)计算相当于第 (i, j) 位置对应滑窗内的权重是由 (j, i) 位置上向量根据一个 Linear 层得到的，这肯定是不对的，毕竟图片肯定不是沿着对角线对称的，即使对称，Stem Block 计算的结果也不是。如果理解错了请告诉我。</p>
</li>
</ul>
<p>论文中给出的伪代码如下：</p>
<p><figure>
    <center>
    <img src="/imgs/volo/volo0.png" alt="图-1 Outlook Attention示例代码">
    <figcaption>图-1 Outlook Attention示例代码</figcaption>
    </center>
</figure></p>
<h2 id="引入多头注意力">引入多头注意力</h2>
<p>在这里，多头注意力简单来说就是得到 N (head num)个不同的权重分布，同时 Value 矩阵的 hidden status 维度分成 N 组，然后每组对应一个权重分布。多头注意力就是在 Value 的每个分组上求解加权平均，然后拼接起来恢复输入时候的尺寸。</p>
<p>对应 Outlook Attention 的实现，就是将得到权重的全连接层由$W^A \in R^{C \times K^4}$变为$W^A \in R^{C \times N \cdot K^{4}}$，同时将Value结果由$R^{H \times W \times C}$Reshape成$R^{H \times W \times N \times C_{in}}$即可。计算过程中，只需要将 $A$ 分成 $N$份，然后分别用于 $N$ 组 $V$ 最后将结果进行拼接完成计算。</p>
<h2 id="构建模型">构建模型</h2>
<p>模型部分主要分为三个部分：Stem, Volo Stages, Transformer Stages。</p>
<p>Stem 部分默认采用的是3层（Conv + BN + ReLU）结构，将输入数据下采样一倍，然后再经过一个 Patch Projection 层（Conv计算），继续下采样 4 倍，所以一共下采样8倍。这种 Stem 避免了 ViT 中的 $16 \times 16$这种大Kernel size / stride 的卷积计算，相关论文指出，这种大 ks / stride 的方式不利于训练稳定性。</p>
<p>中间是由 Multi-head Outlook Attention 构成的Stage。由于Stem Block将输入数据下采样了8倍，我们的目标是下采样16倍，作者选择在 Stage 1 结束后再做一次下采样，这里下采样通过<code>conv stride=2, ks=2</code>来实现。</p>
<p>最后是 Transformer 层构成的 Stages。</p>
<p>论文中对比了 Outlook Attention 层数与 Transformer 层数之间的比例，结论是 1 : 3 的时候效果比较好。</p>
<h2 id="模型精度">模型精度</h2>
<p>论文中采用了 Token Labeling，是 LV-ViT 论文中提出来的。基于 VOLO-D1，在不使用 Token Labeling 计算 Loss，同时引入 Random Augmentation 等增广方式后，自己训练精读是 82.2，使用论文里面的配置，确实可以达到 84.19 的准确率，总体来说效果还是不错的。</p>
        
            <p>Reformer论文。</p>
<h2 id="概览">概览</h2>
<p>Reformer论文的信息量还是有点大，主要创新点在于下面几个：</p>
<ul>
<li>引入了新的 LSH Attention</li>
</ul>
<h2 id="ann">ANN</h2>
<p>ANN (Aproximate Nearest Neighbor, 近似最近邻搜索)。常见的ANN算法可以分为三类：</p>
<ul>
<li>LSH(Locality Sensitive Hashing)</li>
<li>树方法（如HNSW）</li>
<li>Product Quantization</li>
</ul>
<h2 id="lsh-attention">LSH Attention</h2>
<p>对比普通的 Self-Attention (如下式)，新的LSH Attention改动如下。</p>
<p>$$Attention(Q, K, V) = Softmax(\frac{QK^T}{\sqrt{d_k}}) \times V$$</p>
        
            <p>Adam算法的实现以及一个主要改进AdamW的原理与实现。</p>
<p>突然觉着NCHW尺寸的张量比 NHWC Layout 的张量更容易理解，因为后者来看，就是N个样本，每个样本 H * W 的空间维度，然后每个空间元素点是一个 C 维的特征向量。NCHW Layout 的话就需要从后向前理解了&hellip;</p>
<h2 id="adam">Adam</h2>
<p>对于 Adam 的实现，参考下图即可。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adam0.png" alt="图-1 Adam的实现">
    <figcaption>图-1 Adam的实现</figcaption>
    </center>
</figure></p>
<p>算法里最后面三行可以通过用下面两个式子代替用来提高计算性能。</p>
<p>$$   \alpha_t = \alpha \cdot \frac{\sqrt{1 - \beta_2^t}}{ (1 - \beta_1^t)} $$
$$\theta_t \leftarrow \theta_{t-1} - \frac{\alpha_t \cdot m_t}{\sqrt{v_t} + \hat{\epsilon}} $$</p>
<p>其中，$\hat{\epsilon} = \frac{\epsilon}{\sqrt{1 - \beta_2^t}}$。此外，$\alpha$学习率设置了每次更新步长$\Delta_t$的置信区间，具体解释可以参考论文。二阶矩的计算中与普通方差的计算差别在于没有减去均值期望，所以被称为uncentered variacen。而最后上面第一个式子说明了两个超参对更新步长的控制，最后一个式子也说明一阶矩、二阶矩对更新步长的控制，如平地中，除式接近于1，陡峭部分除式小于1（因为方差大），更新会更保守一些。</p>
<p>算法的实现里，另一个重要的步骤是对一阶、二阶矩偏置的矫正。即为什么要除以$\sqrt{1 - \beta_{1,2}^2}$这个式子，证明如下。</p>
<p>以一阶矩$m_t$为例进行推导，首先假设$m_0 = 0$，即初始为一个零矩阵。则$t$时刻一阶矩的计算展开为：</p>
<p>$$m_t = (1 - \beta_t) \sum_{i}^{t}\beta_1^{t-i}g_i$$</p>
<p>我们的目标就是矫正$\mathbb{E}[m_t]$与$\mathbb{E}[g_t]$之间的差距。</p>
<p>$$\begin{aligned}
\mathbb{E}[m_t] = (1 - \beta_1) \mathbb{E}\left[ \sum_i^t \beta_1^{t-i} \cdot g_i \right] \<br>
= &amp; \mathbb{E}[g_t] \cdot (1 - \beta_1) \sum_{i}^t\beta_1^{t-i} + \zeta \<br>
= &amp; \mathbb{E}[g_t]\cdot (1 - \beta_1) + \zeta
\end{aligned}$$</p>
<p>其中，等比数列求和公式为：$\sum_i^t \beta_1^{t-i} = \frac{1 \cdot (1 - \beta_1^t)}{1 - \beta_1}$；另外一个地方是第一个等式，即$\mathbb{E}[g_i] = \mathbb{E}[g_t] + \zeta_i$，这里主要考虑使用等式右边近似表示等式左边的数值然后加上一个误差项，如果$g_i, g_t$属于独立同分布则这个误差项接近于0；论文里有提到，如果$\mathbb{E}[g_i]$比较稳定的时候，这里的误差项接近于0，或者当$\beta_1$比较小的时候，那么对于很久以前的梯度$g_i$赋予很小的权重。</p>
<p>当不使用这些矫正项的时候，Adam退化为RMSProp + Momentum的优化算法，实验表明，随着$\beta_2 \rightarrow 1$ 时，训练越来越不稳定。下图左侧开始的地方给出了一个示意图，其中绿色为 ground truth，紫色为预测的曲线，可以看到紫色部分在开始的地方偏小。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adam1.png" alt="图-2 偏置项矫正的作用">
    <figcaption>图-2 偏置项矫正的作用</figcaption>
    </center>
</figure></p>
<p>缺点是，像 Adam 这些 adaptive gradient optimization methods 在图像分类等任务上泛化性能不够高。常见的 adaptive gradient methods 包括：AdaGrad，RMSProp，Adam，AMSGrad等。可能的原因包括陡峭的局部最优解的出现或其他自身存在的缺点。</p>
<h2 id="adamw">AdamW</h2>
<p><a href="https://arxiv.org/pdf/1711.05101.pdf">Decoupled Weight Decay Regularization</a>作者发现，通过解偶weight decay以及基于loss的反向传播两个过程，可以让学习率、weight decay 两个超参的选取解偶，并且极大提高 adam 优化器在分类任务上的泛化性能，与 SGD + Momentum 取得类似的效果。</p>
<p>既然泛化性能不够，那说明正则化强度不够，所以作者就想到了对 L2 / weight decay 在 Adam 中的使用进行了研究。论文研究了 L2 正则项与 weight decay 在 SGD / Adam 中作用的异同，包括下面几点。</p>
<ul>
<li>L2 正则化项与 weight decay 的实现是不同的</li>
<li>Adam中L2正则项作用不明显</li>
<li>SGD中 L2 与 weight decay 效果类似</li>
<li>关于Adam中 Weight decay的选取，一般来说如果训练需要的迭代次数(iteration)越多，这个数值应该越小</li>
<li>Adam配合全局的学习率调整可以进一步提高性能，比如 cosine annealing等</li>
</ul>
<p>考虑<a href="#l2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8Eweight-decay%E7%9A%84%E5%8C%BA%E5%88%AB">L2与WD区别</a>，论文提出了 SGDW 优化算法，用于解耦weight decay实现中依赖于学习率来计算l2参数。主要改动在于：惩罚项从计算 Momentum 计算之前移动到之后了；在不考虑momentum的实现时，图-3中的算法那才时 weight decay 的真正实现，至于所说的解耦的好处，这也是 weight decay 本身自带的优势。(当红色起作用时，绿色不起作用；或者相反)</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/sgdw0.png" alt="图-3 SGDW的实现">
    <figcaption>图-3 SGDW的实现</figcaption>
    </center>
</figure></p>
<p>但是对于Adam的实现来说，带有weight decay的实现与 L2 Loss 计算梯度无法等价，这一点可以通过对 L2 loss 求导替换$\nabla f_t(\theta)$看出来，若需要两者等价，则L2的系数需要满足:</p>
<p>$$\lambda' = \frac{\lambda}{\alpha \mathbf{M}_t}$$</p>
<p>才行。在 SGD 中，L2 与 Weight Decay 的作用都是让权重接近于0，但是在 Adaptive gradient algorithms中，L2 却不会这样，下个式子给出了带有 L2 正则项的Loss函数求导然后更新权重的过程。</p>
<p>$$\theta_{t+1} \leftarrow \theta_t - \alpha \mathbf{M}_t \nabla f_t(\theta_t) - \alpha \mathbf{M}_t\lambda' \theta_t$$</p>
<p>也就是说，L2 惩罚项也会被用于计算梯度，然后这个计算的梯度被用于 Adam 更新算法中，惩罚项并不会直接作用于权重上，而是参与到自适应一阶、二阶矩$\mathbf{M}_t$的调整中。而在 weight decay 的实现中，没有这个问题，因为$\mathbf{M}_t$只依赖于梯度，而weight decay起作用的方式在于$(1 - \lambda)g_t$，具体公式见论文的Proposition2部分以及对应的附录部分。</p>
<p>为什么 L2 惩罚项这种使用方式不明显呢？首先明确一点是，weight decay / L2 都是为了获得更小的权重值，这样模型泛化会更好一些，为了实现这一点，就需要让权重较大的元素下降的更快一些，权重已经比较小的元素更新小一些。来看 L2 在 Adam 中的具体作用。</p>
<p><a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d">Why AdamW matters</a> 博客里最后一个公式，相当于对权重的exponential moving更新过程是：</p>
<p>$$（1 - \frac{\alpha ( 1 - \beta_1)w}{\sqrt{v_t} + \epsilon})x_{t-1}$$</p>
<p>所以当梯度较大的时候，$\sqrt{v_t}$也比较大，导致该权重的更新（exponential moving）比较小，且还不如那些梯度很小的权重变化大，所以L2作用被打折扣，这也是论文里提到的，拥有大梯度的权重x被惩罚的力度还不如其它权重。最后论文的prosition3 给出了weight decay 与 L2 作用等价时候的关系，但是实际不会在 adaptive gradient algorithms 中实现，而且按照公式关系就可以实现对大的权重进行惩罚，尤其是那些历史梯度都比较大的权重。</p>
<p>那么可不可以理论证明 Weight Decay 的效果确实优于 L2 呢？见<a href="#%E4%BB%8E-bayesian-filtering-%E7%9A%84%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85weight-decay">bayesian filtering</a></p>
<p>图-4给出了准确的 adam with weight decay 的实现。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adamw1.png" alt="图-4 AdamW的实现">
    <figcaption>图-4 AdamW的实现</figcaption>
    </center>
</figure></p>
<p>很明显的看出来，现在 weight decay 已经跟一阶、二阶矩没有关系了。论文中，作者实验证明，即使 Adam 会自适应的调整学习率，但是加上一个全局的乘积因子，如 Cosine Annealing 等，效果会更好。</p>
<h3 id="从-bayesian-filtering-的角度看待weight-decay">从 Bayesian Filtering 的角度看待Weight Decay</h3>
<p>论文引用<a href="https://openreview.net/forum?id=BygREjC9YQ">Aitchison 2018</a>论文的观点，将模型参数的优化过程看做是求解一系列权重参数最优分布的过程。并将训练过程看做是一系列根据训练数据求解模型权重最大似然的过程$P(\theta_t | y_{1..t})$，状态转移函数是一个训练数据无关的过程，即从$P(\theta_{t+1} | \theta_t)$；$\theta$被认为是state，$y_{1..t}$被认为是观测结果。这里还需要更详细的整理。</p>
<p>对于Bayesian Filtering的一个发展是大名鼎鼎的卡尔曼滤波，此外还有其它更复杂的非线性情况下的扩展等实现，这方面一个比较好的教材是：<a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf">State Estimation for Robotics</a>，上面提到的状态的预测、更新在这本书里都会有比较详细的支撑内容。</p>
<p>补充书上式3.3推导过程中第一个等式（下图）的来源，参考<a href="https://math.stackexchange.com/questions/408774/bayes-rule-with-multiple-conditions">Bayes rule with multiple conditions</a>。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adamw0.png" alt="图-5 Bayes推导公式">
    <figcaption>图-5 Bayes推导公式</figcaption>
    </center>
</figure></p>
<p>这可以基于chain rule 来思考：</p>
<p>$$P(a, z, b) = P(a, z | b)P(b) = P(a | z, b) P(z, b) = P(a | z, b)P(z|b)P(b)$$</p>
<p>所以就有了：</p>
<p>$$P(x | v, y) = \frac{P(y, x, v)}{P(v, y)} = \frac{P(y|x, v)P(x|v)P(v)}{P(y|v)P(v)}$$</p>
<p>多说一句，<a href="http://www.r-5.org/files/books/computers/algo-list/image-processing/vision/Richard_Hartley_Andrew_Zisserman-Multiple_View_Geometry_in_Computer_Vision-EN.pdf">Multiple View Gemometry in Computer Vision</a>与上面这本书是视觉SLAM领域两个非常重要的参考读物，一本讲解了从视觉图像上得到三维空间信息，另一本讲解了三维空间信息的使用、变换、误差优化等信息，完美相互配合。</p>
<h3 id="其它的一些改进">其它的一些改进</h3>
<p>论文里，作者用实验表明训练不同的迭代次数时，最优的 weight decay 也是不同的。作者提出了“Normalized Weight Decay”，计算如下：</p>
<p>$$\lambda = \lambda_{norm}\frac{b}{BT}$$</p>
<p>其中，$B$是 training point，T是total epochs数，b是batch size。training point 是单个 epoch 内迭代更新的次数（我的理解）。</p>
<p>其它的改进包括 AdamW with Warm Restart and Cosine Annealing等，一个例子如下。</p>
<p>$$\eta_t = 0.5 + 0.5\cos (\pi T_{cur} / T_i)$$</p>
<p>可以配合上面的 normalized weight decay 对$\lambda$进行调整。Warm Restart 的好处是可以保留前几次的信息，形成一个 momentum，具体可以参考论文。此外，附录B, C部分给出了一个例子，用来说明每次 Restart 时 T 增大两倍，以及 $\eta$ 的变化过程。</p>
<h2 id="l2正则化与weight-decay的区别">L2正则化与weight decay的区别</h2>
<p>注意，L2 正则化项与 Weight Decay 在 SGD 中的使用是两个不同的公式，虽然两者之间存在一个基于学习率的系数差别。结论就是，在 Adaptive Gradient methods 中 L2 的正则化能力弱于weight decay。</p>
<p>关于 weight decay，一般来说 CNN 模型会选取一个比较小的数值，如 10-4 / 10-5 这种量级，但是对于 transformer 模型来说，这个数值一般都会在10-2量级，直观的反映了模型容量对正则化强度的需求变化。</p>
<p>下面来看具体的区别。首先是定义，L2正则化项的定义是修改 Loss 函数，即：</p>
<p>$$f_t^{reg}(\mathbf{\theta}) = f_t(\theta) + \frac{\lambda'}{2}\parallel \theta \parallel^2$$</p>
<p>其中$f_t(\theta)$为 Loss 函数，$\theta$为模型的权重。而SGD中 weight decay 对应的实现是在参数更新的时候起作用，即：</p>
<p>$$\theta_{t+1} = (1 - \lambda) \theta_t + \alpha \nabla f_t(\theta_t) $$</p>
<p>可以看出来是在原来的权重基础上做了一个 expontional decay 的类似计算，$\alpha$为学习率。对 L2 正则项的Loss函数求导并更新权重的公式如下：</p>
<p>$$\theta_{t+1} = \theta - \alpha \nabla f_t(\theta_t) - \alpha \lambda' \theta$$</p>
<p>结合上面两个参数更新过程的式子，可以得出，当L2正则项的参数满足$\lambda' = \frac{\lambda}{\alpha}$条件时，这两个正则方式是等价的。另一方面，在 SGD 中，L2 与 weight decay 的参数（$\lambda'$与$\lambda$）是紧密关联的，前者是后者除以学习率得到。</p>
<h2 id="其它">其它</h2>
<ol>
<li>
<p>为什么weight decay正则化项不会包含 bias 项？</p>
<p>参考下面的<a href="https://stats.stackexchange.com/questions/153605/no-regularisation-term-for-bias-unit-in-neural-network">答案</a>。</p>
<blockquote>
<p>Overfitting usually requires the output of the model to be sensitive to small changes in the input data (i.e. to exactly interpolate the target values, you tend to need a lot of curvature in the fitted function). The bias parameters don&rsquo;t contribute to the curvature of the model, so there is usually little point in regularising them as well.</p>
</blockquote>
<p>事实上， bias项一般会用 mean = 1 的随机数进行初始化，而不是 mean = 0。</p>
</li>
<li>
<p>另有Adam的一个改进时：<a href="https://openreview.net/pdf?id=ryQu7f-RZ">AMSGrad</a>，用于改进Adam很多时候仅能收敛到局部最优点的问题。</p>
</li>
</ol>
        
            <p>Residual Connection以及后续发展。</p>
<p>主要是为了自己梳理一下，总不能最基础的残差网络也忘了吧。更多的信息可以参考：<a href="https://zhuanlan.zhihu.com/p/353185272">ResNet系列网络演绎过程</a></p>
<h2 id="基础">基础</h2>
<p>残差网络(ResNet)是2015年何凯明在<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a>提出的，旁路连接方便了梯度回传，可以帮助模型更好的训练。基础结构如下图1。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual0.png" alt="图1 - 残差块">
    <figcaption>图1 - 残差块</figcaption>
    </center>
</figure></p>
<p>我们知道，VGG / ResNet / Mobilenet 等论文里已经说明现在网络结构设计可以通过简单的 Block 堆叠来构建，并且Blocks可以分组为若干个 Stage，每个 Stage 包含若干层 Block。为了提高计算性能以及提高感受野等，不同 Stage 之间会下采样降低空间分辨率同时提高 channel 个数（神经元）来保证模型容量。对于每个 Stage 的第一层 Block 需要完成下采样、channel翻倍的任务，为了保证输入数据与这两步处理后的输出数据尺寸相同，需要修改旁路，不再是 Indentity，而需要通过卷积完成映射。论文里在每个Block的第一层卷积里使用<code>stride=2</code>来完成下采样。 有论文表明，使用 avg pooling 进行下采样会更好，避免丢失很多的信息。</p>
<p>另外，一般配合 BN 时，CNN 的 bias 作用不明显可去掉。对于 bias 的作用可简单参考：<a href="https://www.pico.net/kb/the-role-of-bias-in-neural-networks/">The role of bias in Neural Networks</a>，猜测是在 BN 之前用于修正 <code>W * x</code> 的偏置，<strong>防止方差过大导致训练困难</strong>，即学习一个参数来降低输出值的方差，类似于降噪。</p>
<p>上述过程的示例代码如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">torch</span> <span style="color:#000;font-weight:bold">import</span> nn

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">ResBasicBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    expansion <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, in_c, out_c, stride, downsample<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, reduce_first<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, ks<span style="color:#000;font-weight:bold">=</span><span style="color:#099">3</span>, padding<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        first_planes <span style="color:#000;font-weight:bold">=</span> out_c <span style="color:#000;font-weight:bold">//</span> reduce_first

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, first_planes, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(first_planes)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(first_planes, out_c, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#998;font-style:italic">## for downsample &amp; double channel number</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
        <span style="color:#000;font-weight:bold">if</span> downsample:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential(
                nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_c, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>),        <span style="color:#998;font-style:italic"># 通常这里的 ks = 1, senet 里 ks = 3</span>
                nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
            )
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1(x)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2(feat)
        <span style="color:#998;font-style:italic">## add se block here</span>
        <span style="color:#998;font-style:italic"># feat = self.se(feat)</span>

        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample(x)
        feat <span style="color:#000;font-weight:bold">+=</span> x
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2(feat)

        <span style="color:#000;font-weight:bold">return</span> feat
</code></pre></td></tr></table>
</div>
</div><p>注意在Stem模块中，不使用残差模块，并且通过 <code>stride=2</code> 以及一个 <code>MaxPooling(stride=2)</code> 来将输入图片下采样4倍。</p>
<h2 id="bottleneck-结构">Bottleneck 结构</h2>
<p><a href="#%E5%9F%BA%E7%A1%80">第一小节</a>里提到的结构更多的是用于 resnet-18/34等浅层网络，为了构建深层网络（resnet-50/101-152）等，作者提出了 Bottleneck 模块。Bottleneck 模块包含三层卷机，分别是 <code>conv1x1, conv3x3, conv1x1</code>，并且第一个 <code>conv1x1</code>将输入数据的channel根据一个因子（通常是4）进行缩小，最后一个<code>conv1x1</code>在缩放回原来大小，这样既可以完成残差计算，也降低了中间<code>conv3x3</code>的计算。实验表明，这里即使不降低 channel 个数也不会影响性能，所以Bottleneck 完全为了实际中提高计算效率，至于 Mobilenetv2 里提到的 Inverted Residual Block，不会展开。网络结构示意图如图2右边部分。</p>
<p>实现的时候需要注意的是，每个 Stage 里Block内的Channel变化过程，最后一个<code>conv1x1</code>的是第一个<code>conv1x1</code><strong>输入</strong>的expansion倍。下图展示的其实是Stage内非第一个Block的结构，相较于输入，第一个<code>conv1x1</code>将channel数降低了4倍；而第一个Block的输入channel数时上一Stage输出的channel数，配合channel double的过程，第一个<code>conv1x1</code>只是将channel下降了2；此外，下采样部分是在 <code>conv3x3, stride=2</code> 部分完成的，如果放在第一个<code>conv1x1</code>里，会导致3/4的信息丢失。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual1.png" alt="图-2 Bottleneck 结构">
    <figcaption>图-2 Bottleneck 结构</figcaption>
    </center>
</figure></p>
<p>具体实现代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">torch</span> <span style="color:#000;font-weight:bold">import</span> nn
<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">BottleneckBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, in_c<span style="color:#000;font-weight:bold">=</span><span style="color:#099">256</span>, out_c<span style="color:#000;font-weight:bold">=</span><span style="color:#099">64</span>, expansion<span style="color:#000;font-weight:bold">=</span><span style="color:#099">4</span>, stride<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, downsample<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        out_planes <span style="color:#000;font-weight:bold">=</span> out_c <span style="color:#000;font-weight:bold">*</span> expansion

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_c, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(out_c, out_c, <span style="color:#099">3</span>, stride, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)      <span style="color:#998;font-style:italic"># stride = 2 时进行下采样</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(out_c, out_planes, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)         <span style="color:#998;font-style:italic"># 注意输出 channel 的个数</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#000;font-weight:bold">if</span> downsample:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential(
                nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_planes, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>),
                nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_planes)
            )
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1(x)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1(feat)

        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2(feat)
        <span style="color:#998;font-style:italic">## use avg pooling to downsample here</span>

        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv3(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn3(feat)
        <span style="color:#998;font-style:italic">## add se here</span>
        <span style="color:#998;font-style:italic"># feat = self.se(feat)</span>
        <span style="color:#998;font-style:italic">## drop path here, i.e. random drop some samples along batch axis</span>
        <span style="color:#998;font-style:italic">## downsample projection path here</span>
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample(x)
        feat <span style="color:#000;font-weight:bold">+=</span> x
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act3(feat)
        <span style="color:#000;font-weight:bold">return</span> feat
</code></pre></td></tr></table>
</div>
</div><p>总结一下SE的位置，SE Block 均是在每一个 Block 最后一层卷积 BN 之后的特征上进行。</p>
<h2 id="resnet-v2">ResNet v2</h2>
<p>论文地址：<a href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep Residual Networks</a></p>
<p>论文里其实是对 BN / ReLU 两个函数的位置进行了挪动。作者测试了下面几种排列组合，发现最后一种实现效果最好。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual2.png" alt="图-3 ResNet v2改进">
    <figcaption>图-3 ResNet v2改进</figcaption>
    </center>
</figure></p>
<p>分析一下，(b)里 BN 在 Identity (左侧)分支里，会改变Identity分支的分布，影响信息传递，在训练开始的时候会阻碍Loss的下降。这一点可以通过论文里的梯度反向传播推导过程看出来。</p>
<p>(c)里residual(右侧)分支是 ReLU 的输出，导致这个分支对结果只有正向影响，毕竟非负，但我们希望有两个方向的影响，所以非最优。关于(d, e)，实验表明都不如(f)，毕竟 BN 在Residual分支上可以对输入就起到正则化的作用。</p>
<h2 id="resnext">ResNeXt</h2>
<p>网络结构如图4。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual3.png" alt="图-4 ResNeXt网络结构">
    <figcaption>图-4 ResNeXt网络结构</figcaption>
    </center>
</figure></p>
<p>(a)为最开始的思想，(c)为等价形式。也就是说，中间的<code>conv3x3</code>替换为分组卷积计算。
主要改动就是将普通残差结构中的 Residual 分支用 Inception 思想进行修改，用多路并行卷积代替原来的一支卷积，与Inception论文不同的是，这里每个分支采用相同的参数配置，如kernel size等。</p>
<h2 id="其它">其它</h2>
<ul>
<li>
<p>ResNeSt</p>
<p>与<a href="https://arxiv.org/abs/1903.06586">SKNet</a>类似。</p>
</li>
<li>
<p>Res2Net</p>
<p>在单个残差块内引入Inception思想，感受野逐步增大，最后concatenate 起来送入 <code>conv1x1</code> 计算。</p>
</li>
<li>
<p>SKNet</p>
</li>
</ul>
        
            <p>二叉搜索树相关笔记</p>
<h2 id="定义">定义</h2>
<p>每个节点有两个子节点：左节点、右节点，即是一个二叉树；同时有顺序关系：左子树小于父节点，右子树大于父节点。</p>
<p><figure>
    <center>
    <img src="/imgs/binary-search-tree/BSTSearch01.png" alt="binary-search-image structure example">
    <figcaption>binary-search-image structure example</figcaption>
    </center>
</figure></p>
<p>查找复杂度：$\sim 2\ln(N)$，其中 N 是节点个数。查找未命中也是这个复杂度。</p>
<p>作为一种数据结构，主要功能就是：增删查改。floor() 也可以认为是一种“查”操作。</p>
<p>目前来看，相关应用可以分为两大类：遍历，排序。遍历就是包括前序、中序、后序进行遍历然后处理；排序一般就是按照中序进行处理。</p>
<h2 id="floor-函数">floor() 函数</h2>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">Key <span style="color:#900;font-weight:bold">floor</span>(Key key)
{
    Node x <span style="color:#000;font-weight:bold">=</span> floor(root, key);
    <span style="color:#000;font-weight:bold">if</span> (x <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    <span style="color:#000;font-weight:bold">else</span>
        <span style="color:#000;font-weight:bold">return</span> x;
}

Node <span style="color:#900;font-weight:bold">floor</span>(Node root, Key key)
{
    <span style="color:#000;font-weight:bold">if</span> (root <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    
    <span style="color:#000;font-weight:bold">if</span> (root<span style="color:#000;font-weight:bold">-&gt;</span>key <span style="color:#000;font-weight:bold">==</span> key)
        <span style="color:#000;font-weight:bold">return</span> root;
    <span style="color:#000;font-weight:bold">if</span> (root<span style="color:#000;font-weight:bold">-&gt;</span>key <span style="color:#000;font-weight:bold">&gt;</span> key)    <span style="color:#998;font-style:italic">// left branch
</span><span style="color:#998;font-style:italic"></span>        <span style="color:#000;font-weight:bold">return</span> floor(root<span style="color:#000;font-weight:bold">-&gt;</span>left, key);
    <span style="color:#998;font-style:italic">// right branch: key &gt; root-&gt;key
</span><span style="color:#998;font-style:italic"></span>    Node t <span style="color:#000;font-weight:bold">=</span> floor(root<span style="color:#000;font-weight:bold">-&gt;</span>right, key);
    <span style="color:#000;font-weight:bold">if</span> (t <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> root;
    <span style="color:#000;font-weight:bold">else</span>
        <span style="color:#000;font-weight:bold">return</span> t;
}
</code></pre></td></tr></table>
</div>
</div><p>为什么这里相当于是一个前序排序，其实本质上需要的是 <code>root-&gt;left</code> 分支的递归要在 <code>root-&gt;right</code> 的前面，这样才能保证走右子树的时候，可以最先判断这个子树上最小的数值。</p>
<p>这个函数关键是分清楚什么时候向左走，什么时候向右走，然后才能明白应该返回什么数值。向左走的时候，说明当前的 <code>root-&gt;key</code> 大于目标值，向右走的时候，说明当前的 <code>root-&gt;key</code> 小于目标值，而小于目标值则说明结果必定存在，因为大不了返回当前的 root 即可。因此，在递归过程向上走的时候，如果当前属于右子路的下一层(代码第19行)并且返回的是 nullptr，那么就返回当前的节点就行了(第20行)，否则返回递归返回的结果即可。总而言之，如果走上右分支，就必定有解；走上左分支，则原样返回即可。</p>
<p><strong>递归过程可以看成两个步骤：首先是沿着左子树或者右子树向下走，然后就是沿着树向上爬。</strong> 向上爬的时候，需要注意返回结果，对树的状态进行更新，比如链接、具体成员变量等。另一方面，递归过程其实就是一个先进后出的过程，所以对应的非递归实现可以借助Stack来实现。向上走的时候基于向下走的时候的判断来保证正确。</p>
<h2 id="delete函数">Delete()函数</h2>
<p>首先是删除最小、最大值节点。</p>
<p>主要在于递归返回结果，<strong>只需要将返回的链接赋给作为参数的链接</strong>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#998;font-style:italic">// ...
</span><span style="color:#998;font-style:italic"></span>x.left <span style="color:#000;font-weight:bold">=</span> deleteMin(x.left);     <span style="color:#998;font-style:italic">// delete minimum
</span><span style="color:#998;font-style:italic">// ...
</span></code></pre></td></tr></table>
</div>
</div><p>其次是删除中间某个节点，与删除最小、最大节点的区别在于，这个节点包含两个字节点，而且还需要保持二叉搜索树之间的顺序。这个问题常用的解法是：<code>Hibbard算法</code>。该算法表示在删除节点 x 之后，用它的后继节点填充它的位置，并且这个后继节点就是右子树中的最小节点。</p>
<p>对应的4个步骤如下：</p>
<ol>
<li>找到需要被删除的节点，用 t 表示</li>
<li>然后找到被删除节点右（左，随机选择）分支的最小（大）值，表示为 x，即 <code>x = min(t.right)</code></li>
<li>然后更新 x 节点的左右分支，<code>x.right</code> 更新为 <code>deleteMin(t.right)</code>，保证右子树都大于 <code>x</code></li>
<li><code>x.left</code> 更新为 <code>t.left</code>，最后返回 x 作为原来 t 的父节点的子节点，即代替 t</li>
</ol>
<p>实现示例如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">Node <span style="color:#900;font-weight:bold">delete</span>(Key key)
{
    <span style="color:#000;font-weight:bold">delete</span>(root, key);
}

Node <span style="color:#900;font-weight:bold">delete</span>(Node t, key key)
{
    <span style="color:#000;font-weight:bold">if</span> (t <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)       <span style="color:#998;font-style:italic">// not exists
</span><span style="color:#998;font-style:italic"></span>        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    <span style="color:#000;font-weight:bold">if</span> (key <span style="color:#000;font-weight:bold">&lt;</span> t<span style="color:#000;font-weight:bold">-&gt;</span>key)       <span style="color:#998;font-style:italic">// go left
</span><span style="color:#998;font-style:italic"></span>    {
        t<span style="color:#000;font-weight:bold">-&gt;</span>left <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">delete</span>(t<span style="color:#000;font-weight:bold">-&gt;</span>left, key);
        <span style="color:#000;font-weight:bold">return</span> t;
    }
    elif (key <span style="color:#000;font-weight:bold">&gt;</span> t<span style="color:#000;font-weight:bold">-&gt;</span>key)
    {
        t<span style="color:#000;font-weight:bold">-&gt;</span>right <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">delete</span>(t<span style="color:#000;font-weight:bold">-&gt;</span>right, key);
        <span style="color:#000;font-weight:bold">return</span> t;
    }
    <span style="color:#000;font-weight:bold">else</span>                    <span style="color:#998;font-style:italic">// equal
</span><span style="color:#998;font-style:italic"></span>    {
        <span style="color:#000;font-weight:bold">if</span> (t.right <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
            <span style="color:#000;font-weight:bold">return</span> t.left;
        <span style="color:#000;font-weight:bold">if</span> (t.left <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
            <span style="color:#000;font-weight:bold">return</span> t.right;
        Node x <span style="color:#000;font-weight:bold">=</span> min(t.right);
        x.right <span style="color:#000;font-weight:bold">=</span> deleteMin(t.right);
        x.left <span style="color:#000;font-weight:bold">=</span> t.left;
        <span style="color:#000;font-weight:bold">delete</span> t;
        <span style="color:#000;font-weight:bold">return</span> x;
    }
    <span style="color:#998;font-style:italic">// other update
</span><span style="color:#998;font-style:italic"></span>}
</code></pre></td></tr></table>
</div>
</div><h2 id="其他支持的函数">其他支持的函数</h2>
<ul>
<li>
<p>Select()</p>
<p>即返回第k小的节点。</p>
<p>可以通过中序进行遍历，然后每次递归返回的时候，通过引用更新计数，当计数为0的时侯返回该节点即可。</p>
</li>
<li>
<p>Rank()</p>
<p>即给定一个key，返回该key在树中的位置。</p>
<p>如果给定 key 小于当前节点，则正确的位置必定在右分支中，则最终的排序就是左分支的节点个数 + 1（当前节点） + 在右子树中的排序，而在右子树中的排序直接递归即可。</p>
</li>
<li>
<p>范围查找</p>
<p>典型的中序遍历思路，即判断当前节点是否在制定的范围内，如果在范围内，则保存当前节点到一个队列即可。</p>
</li>
</ul>
        
            <p>经典自监督模型，包括MoCo / SimCLR / SwAV / BYOL / SimSiam 等。</p>
<p>主要关注无监督策略的研究，模型结构不是本文重点，所以主要包括 MoCo 系列、SimCLR 系列、SwAV、BYOL 等几篇论文。无监督训练模型的一点在于要避免模型坍塌，通过 Contrastive Loss, Clustering Constraints, Predictor(Stop Gradient), Batch Normalization等。</p>
<p>整体来说，图像自监督学习方法按照自监督实现思想可以分为下面几类。</p>
<ul>
<li>基于contrastive loss</li>
<li>基于蒸馏的方式，一般设计 momentum</li>
<li>基于聚类的方法</li>
</ul>
<p>关于预训练任务也存在多种选择。</p>
<ul>
<li>预测图像选装方向。图像经过 0/90/180/270 等几个角度的随机旋转，然后训练模型进行4分类</li>
<li>预测图片不定位置相对关系。图像被分割成 3 * 3 的表格，然后选取中心小图与另外8个子图中的随机一个进行位置分类，分类类别为8（两个子图的输出拼接起来送入分类层），一些技巧是图像分割成子图时可以增加缝隙或者抖动等</li>
<li>补丁拼图。将图片分割成 3 * 3 的子图，然后随机打乱，将子图的所有输出特征拼接起来送入分类层，正常来说，类别说应该是 9!，但是作者对这些排列类别做了合并，因为很多排列比较相似，合并过程基于汉明距离进行</li>
<li>图片上色。灰度图片输入 Encoder，然后Decoder输出彩色图片，可以使用 L2 Loss，或者 LAB 颜色空间等</li>
<li>自编码器系列。</li>
<li>GAN系列。</li>
<li>对比学习。需要构造丰富的负样本，比如大的 Batch Size 或者借助 Memory Bank等</li>
</ul>
<h2 id="moco-系列">MoCo 系列</h2>
<h2 id="simclr-系列">SimCLR 系列</h2>
<h2 id="swav">SwAV</h2>
<h2 id="byol">BYOL</h2>
<p>分析了怎么防止模型坍塌（也就是所有的输入的模型输出都是相同的），关键是要让模型的输出部分层学习到新的知识。在这里，一方面是借助 Mean Teacher，一方面是在 Student Network 上面增加了一层 Predictor，这两个因素可以让 Prediction 层不断学习新的知识，从而避免模型坍塌。BYOL包含两个模型，一个称为 Online，一个称为 Target。</p>
<p>按理来说，没有负样本，那么优化损失函数的梯度$\nabla_{\theta}(\mathcal{L}_{\theta, \epsilon}^{\mathrm{BYOL}})$应该很快导致模型坍塌啊，也就是损失降为0，但实际没有发生，作者认为这是因为这个损失的梯度下降方向与 Target 模型参数的变化方向是不一致的，也就是梯度下降方向 与 Target 模型让 Online 模型参数更新的放向不一样，所以避免了模型坍塌。另一方面，这也就意味着不存在一个Loss可以同时优化 Target / Online 模型的权重，类似于 GAN 模型的G / D的参数无法同时优化一样。作者也用消融实验表明，保持 prediction 足够好貌似是防止坍塌的关键。</p>
<p>为啥 SimCLR 依赖于 color jitter 这个变换，因为如果去掉这个变换的话，两次 crop 的图像的颜色直方图分布其实是非常接近的，导致模型非常容易学习。</p>
<p>发现去掉 Weight Decay 后，模型发散，说明 WD 对自监督模型的重要性，但是增加模型初始化时的初始值范围对模型性能影响不大。</p>
<h2 id="simsiam">SimSiam</h2>
<p>SimSiam 的 Prediction Head 需要固定 Learning Rate，也就是不随 Scheduler 变化。</p>
<h2 id="消融实验">消融实验</h2>
<h2 id="一些-tricks">一些 Tricks</h2>
<ul>
<li>
<p>Rethinking Image Mixture for Unsupervised Visual Representation Learning</p>
<p>在无监督训练中引入了Image Mixture &amp; Label Smooth。</p>
</li>
<li>
<p>Whitening for Self-Supervised Representation Learning</p>
</li>
<li>
<p>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</p>
</li>
<li>
<p>Contrastive Multiview Coding</p>
</li>
</ul>
        
            <p>一些为了适应大的Batch Size训练的优化算法。</p>
<p>偶尔就有时间看到有标题：训练 XX 模型只用 ** 秒 / 分钟，当然除了钱多卡多，在算法实现方面也看看有什么不同。</p>
<p>有个概念是 cross-view prediction (Hinto92年的一篇论文)，现在很多自监督模型都是在 representation space 完成这个预测任务，但是这容易导致模型坍塌，所以 BYOL 增加了一个 Prediction Layer，这也是相对于 Mean Teacher 这篇论文最大的改进了。对比学习的做法相当于学习当前图片的一个augmentation与其他图片的augmentation之间的区分。</p>
<p>主要包括：LARS, LAMB 等。</p>
<h2 id="基础adam">基础：Adam</h2>
<h2 id="lars算法">LARS算法</h2>
<h2 id="lamb算法">LAMB算法</h2>
        
	
		<span>2</span>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2021-11-23 14:12:01.895392 &#43;0800 CST m=&#43;0.118987582">2021</time> triloon. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
