<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Posts &middot; Triloon</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
        <link rel="stylesheet" href="/css/custom.css">
		
		<link rel="icon" href="favicon.ico" />
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Triloon" />
	</head>

    <body>
        
		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h1 class="nav-title">Triloon</h1>
					
				</a>
				<ul>
    
    
        <li>
            <a href="/about/about">
                
                <span>About</span>
                
            </a>
        </li>
    
        <li>
            <a href="/posts/">
                
                <span>Posts</span>
                
            </a>
        </li>
    
</ul>
			</div>
		</nav>

        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">

        

<main>
	<div class="catalogue">
		
			<a href="https://triloon.space/posts/img-transform-ssl/" class="catalogue-item">
    <div>
        <time datetime="2021-09-28 14:07:50 &#43;0800 CST" class="catalogue-time">September 28, 2021</time>
        <h2 class="catalogue-title">基于Transformer结构的图像自监督模型及训练</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>一些基于Transformer结构的图像自监督模型以及训练过程中遇到的问题。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/torch-impl-0/" class="catalogue-item">
    <div>
        <time datetime="2021-09-18 11:08:11 &#43;0800 CST" class="catalogue-time">September 18, 2021</time>
        <h2 class="catalogue-title">Torch实现原理分析积累</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Pytorch 实现学习积累。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/model-visualization/" class="catalogue-item">
    <div>
        <time datetime="2021-09-11 15:16:07 &#43;0800 CST" class="catalogue-time">September 11, 2021</time>
        <h2 class="catalogue-title">Model Visualization</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>深度学习中的一些可视化技术。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/origin-transformer/" class="catalogue-item">
    <div>
        <time datetime="2021-09-06 20:04:32 &#43;0800 CST" class="catalogue-time">September 6, 2021</time>
        <h2 class="catalogue-title">Origin Transformer</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Attention is all your need.</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/cnn-transformer-volo/" class="catalogue-item">
    <div>
        <time datetime="2021-09-06 16:26:11 &#43;0800 CST" class="catalogue-time">September 6, 2021</time>
        <h2 class="catalogue-title">Cnn Transformer 系列之 Volo</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>VOLO论文。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/adam-adamw/" class="catalogue-item">
    <div>
        <time datetime="2021-09-03 20:30:59 &#43;0800 CST" class="catalogue-time">September 3, 2021</time>
        <h2 class="catalogue-title">从Adam到AdamW</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Adam算法的实现以及一个主要改进AdamW的原理与实现。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/resnet-series/" class="catalogue-item">
    <div>
        <time datetime="2021-09-02 14:12:59 &#43;0800 CST" class="catalogue-time">September 2, 2021</time>
        <h2 class="catalogue-title">Resnet Series</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>Residual Connection以及后续发展。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/binary-search-tree/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 15:00:22 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">Binary Search Tree</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>二叉搜索树相关笔记</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/ssl-image-representation/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 14:15:48 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">图像表征算法中的自监督学习方法</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>经典自监督模型，包括MoCo / SimCLR / SwAV / BYOL / SimSiam 等。</p>
        </p>
    </div>
</a>

		
			<a href="https://triloon.space/posts/efficient-transformer/" class="catalogue-item">
    <div>
        <time datetime="2021-08-31 14:02:36 &#43;0800 CST" class="catalogue-time">August 31, 2021</time>
        <h2 class="catalogue-title">Efficient Transformer系列</h2>
        <div class="catalogue-line"></div>

        <p>
            <p>一些以提高 Transformer 计算性能为目的的 Xformer 方案整理。</p>
        </p>
    </div>
</a>

		
	</div>
	
	<div class="pagination">
		
			<a href="/posts/" class="left arrow">&#8592;</a>
		
		
			<a href="/posts/page/3/" class="right arrow">&#8594;</a>
		
        
            <p>一些基于Transformer结构的图像自监督模型以及训练过程中遇到的问题。</p>
<p>这篇文章里除了给出 DINO / MoCoV3 的具体结构以及对应的自监督训练思想，还给出了两种position embedding的具体实现。</p>
<h2 id="概览">概览</h2>
<p>目前出现了一系列的图像表征模型，包括 MoCo 系列、SimCLR系列以及BYOL/SwAV/SimSiam等，本文主要关注基于 Transformer 结构的一些无监督训练模型的细节，主要包括 MoCo v3, DINO, MoBY等。</p>
<p>性能对比主要分为：Linear Acc 以及 End-2-End Fine Tuning Acc 两种方式。前者是无监督预训练之后，仅对最有一层分类输出层进行微调，后者是微调整个网络。</p>
<p>几个模型的主要指标对比如下，ViT-S 与 ResNet50 参数量（21M vs 23M(resnet50)）以及吞吐率（1007 vs 1237 img/sec）还有有监督训练精度（79.8(v) VS 79.3(r)）都类似。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Pretrain Epochs</th>
<th>Linear Acc</th>
<th>E2E Acc</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>SimSiam, ResNet-50</td>
<td>100</td>
<td>68.1</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>SimSiam, ResNet-50</td>
<td>200</td>
<td>70.0</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>SimSiam, ResNet-50</td>
<td>400</td>
<td>70.8</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>SimSiam, ResNet-50</td>
<td>800</td>
<td>71.3</td>
<td>-</td>
<td>与 SimCLR/MoCoV2/SwAV 几乎差不多</td>
</tr>
<tr>
<td>MoCo V3, ResNet-50</td>
<td>100</td>
<td>68.9</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoCo V3, ResNet-50</td>
<td>300</td>
<td>72.8</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoCo V3, ViT Small</td>
<td>300</td>
<td>73.2</td>
<td>81.4</td>
<td></td>
</tr>
<tr>
<td>MoCo V3, ViT Base</td>
<td>300</td>
<td>76.7</td>
<td>83.2</td>
<td></td>
</tr>
<tr>
<td>MoCo V3, ViT Large</td>
<td>300</td>
<td>77.6</td>
<td>84.1</td>
<td></td>
</tr>
<tr>
<td>DINO, ResNet-50</td>
<td>300</td>
<td>74.5</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DINO, ResNet-50</td>
<td>-</td>
<td>75.3</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DINO, ViT Small</td>
<td>300</td>
<td>76.1</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DINO, ViT Small</td>
<td>-</td>
<td>77.0</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DINO, ViT Base/16</td>
<td>-</td>
<td>78.2</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DINO, ViT Base/8</td>
<td>-</td>
<td>80.1</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoBY, DEIT-S</td>
<td>300</td>
<td>72.8</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoBY, DEIT-S(multi crop)</td>
<td>300</td>
<td>75.9</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoBY, Swin-T</td>
<td>100</td>
<td>70.9</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MoBY, Swin-T</td>
<td>300</td>
<td>75.0</td>
<td>-</td>
<td></td>
</tr>
</tbody>
</table>
<p>所以，在 epoch = 300 这个门槛上，最好的模型以及对应无监督训练策略应该是 DINO + ViT Base/8。在 ResNet50 / 300 epochs 上，最好的无监督训练模型也是DINO的74.5；在 ViT-S / 300 epochs 上，最好的当属 DINO 的76.1。而且 ViT B/8 比 ViT B/16 效果更好。</p>
<p>MoCo V3 以及 DINO 以及 MoBY 都借助了 Momentum Update 的技巧来防治模型坍塌。</p>
<h2 id="moco-v3">MoCo v3</h2>
<p>主要思想是基于 Contrastive Loss 进行训练。</p>
<p>数据增广方式由两种Augmentation List构成(采取BYOL论文的做法)，第一种与 SimSiam 类似，第二种Aug List 里新增了BYOL中的 Solarization 增广方式。效果体现在对输入图片的两个Crop分别进行增广，因为<code>RandomResizedCrop()</code>函数的实现会先 Crop，然后在Resize 到指定大小。具体的增广参考<a href="https://github.com/facebookresearch/moco-v3">moco-v3</a>里的代码。</p>
<p>MoCo V3 丢弃了 MoCo V1/V2 里采用的 Memory Queue 来构造负样本，与 SimCLR 的观察类似，也是通过大的 Batch Size 来保证负样本的数量；另一方面，模型采用了EMA思路，也就是有两个 Encoder 网络$f_q, f_k$，其中 $f_k = m * f_k + (1 - m) * f_q$。采用的 Loss 是 InfoNCE，即：</p>
<p>$$\mathcal{L}_q = - \log \frac{\exp(q \cdot k^+ / \tau)}{\exp(q \cdot k^+ / \tau) + \sum_k \exp(q \cdot k^- / \tau) }$$</p>
<p>完整的算法实现见图-1。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/mocov3-0.png" alt="图 - 1 MoCo v3 Algorithm">
    <figcaption>图 - 1 MoCo v3 Algorithm</figcaption>
    </center>
</figure></p>
<p>值得注意的地方在于，Predictior 仅用与计算 Q，K 是直接 $f_k$ 模型的输出结果；Loss的最终计算会乘上一个因子 $2 * \tau$，默认的$\tau=0.2$；在训练过程中，算法中的$m$也随着 epoch 的增加而 cosine 下降；类似 SimCLR 新增了 3 层的 Mlp 层，具体维度的变化可以参考代码；其它具体实现可以参考上面链接中的代码。两个细节，首先是计算 K 的时候使用的更新后的 $f_k$，其次是没有像 SimSiam 中那样使用 fix lr 技巧。</p>
<p>对于参数 $m$ 的选取，作者对比了 0, 0.9, 0.99, 0.999 等值，发现 m = 0.99 时效果最好，但是实际实现是按照下面一行代码进行更新，并且初始值是0.99。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">adjust_moco_momentum</span>(epoch, args):
    <span style="color:#d14">&#34;&#34;&#34;Adjust moco momentum based on current epoch&#34;&#34;&#34;</span>
    m <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1.</span> <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">0.5</span> <span style="color:#000;font-weight:bold">*</span> (<span style="color:#099">1.</span> <span style="color:#000;font-weight:bold">+</span> math<span style="color:#000;font-weight:bold">.</span>cos(math<span style="color:#000;font-weight:bold">.</span>pi <span style="color:#000;font-weight:bold">*</span> epoch <span style="color:#000;font-weight:bold">/</span> args<span style="color:#000;font-weight:bold">.</span>epochs)) 
                        <span style="color:#000;font-weight:bold">*</span> (<span style="color:#099">1.</span> <span style="color:#000;font-weight:bold">-</span> args<span style="color:#000;font-weight:bold">.</span>moco_m)
    <span style="color:#000;font-weight:bold">return</span> m
</code></pre></td></tr></table>
</div>
</div><p>对应的曲线如图-2，其中横轴为epoch的取值，纵轴是对应 epoch 的 momentum 参数。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/mocov3-3.png" alt="图 - 2 MoCo V3 中 Momentum 参数的变化趋势">
    <figcaption>图 - 2 MoCo V3 中 Momentum 参数的变化趋势</figcaption>
    </center>
</figure></p>
<p>与参数量差不多的 ResNet 模型相比，ViT 占有优势！</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/mocov3-2.png" alt="图 - 3 不同 Backbone 在 MoCo V3 上的性能表现">
    <figcaption>图 - 3 不同 Backbone 在 MoCo V3 上的性能表现</figcaption>
    </center>
</figure></p>
<h3 id="sin-cos位置编码">Sin-Cos位置编码</h3>
<p>Sin-Cos位置编码基于下面的公式计算每个位置上的 position embedding。</p>
<p>$$PE(pos, 2i) = \sin \left( \frac{pos}{10000^{2i / d_{model}}} \right)$$
$$PE(pos, 2i + 1) = \cos \left( \frac{pos}{10000^{2i / d_{model}}} \right)$$</p>
<h3 id="使用-vit-时的训练稳定性">使用 ViT 时的训练稳定性</h3>
<p>一般来说，可以直接将 ResNet50 替换成 ViT 模型进行训练，但是作者发现 Transformer 网络不太稳定。论文里作者给出了几个导致训练不稳定的原因以及可能的改正方法。</p>
<p>首先是不稳定因素。</p>
<ul>
<li>
<p>Batch Size</p>
<p>实验发现，Batch Size 由 1K -&gt; 2K 的时候，Linear Acc 是有提高的（71.5 -&gt; 72.6）。但是当 Batch Size 继续增加到 4K 的时候，按理说可用负样本更多了，效果应该更好，但是实际是 Acc 下降到了 72.2，而且Trainging Curve 上也出现了很多的 Dips。当 Batch Size = 6K 的时候，现象更严重。可能的原因在于大Batch Size 训练时模型会跳出当前局部最优解，然后重新进行优化。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/mocov3-1.png" alt="图 - 4 训练稳定性与Batch Size的关系">
    <figcaption>图 - 4 训练稳定性与Batch Size的关系</figcaption>
    </center>
</figure></p>
</li>
<li>
<p>Learning Rate</p>
<p>学习率也是一个因素，当学习率太小的时候，模型没法学到最优导致ACC降低，当学习率太大的时候，也会出现 Dip，从而训练不稳定导致ACC下降。文章里使用的 learning rate scale 规则是：$lr * \mathrm{BatchSize} / 256$，其中基础的 lr = 1.5e-4。</p>
</li>
<li>
<p>Optimizer</p>
<p>一般来说，大 Batch Size 训练需要使用专门的优化器，比如 LARS，以及 LAMB等。作者对比了 LAMB，发现效果与 AdamW 效果类似，但是对 lr 会更敏感，导致不好调参，所以还是使用 AdamW。</p>
</li>
</ul>
<p>然后作者给出了一种解决办法：</p>
<ul>
<li>Random Patch Projection
也就是将 Patch Embedding 层固定为随机初始化权重；并且发现使用 BN / Weight Norm 等方法也不如使用 Random Patch Projection训练更好。</li>
<li>Long Warm-Up
使用 40 epochs 的Warm Up，也有助于增加训练稳定性。</li>
</ul>
<h2 id="dino">DINO</h2>
<p>主要思路是基于 Self Distillation 来实现。既然是类似于 Knowledge Distillation 的实现，那就必须要有一个 Teacher 模型、一个 Student 模型，如图-5所示。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/dino-0.png" alt="图 - 5 DINO网络结构示意图">
    <figcaption>图 - 5 DINO网络结构示意图</figcaption>
    </center>
</figure></p>
<p>在SSL中，Teacher模型的实现包含两个方面，首先是输入数据，然后是模型参数怎么更新。输入数据涉及到了同一幅图片的 Multi-Crops(Views)的实现。在 DINO 中，给定一幅图片 x，产生一个 view 集合，集合中包含两个 global view 以及多个（如6个）Local Views，Local View 具有更小的分辨率。所有的 Views 都会进行 student 网络的前向计算，但是只有两个 Global Views 才会进行 Teacher 网络的前向计算，所以在基于上述 Student / Teancher 模型进行知识蒸馏的时候可以获得 &ldquo;Local-to-Global&rdquo; 的对应。然后用下式对 Student 网络参数进行更新。</p>
<p>$$\min_{\theta_s} \sum_{x \in { x_1^g, x_2^g }} \sum_{x'\in V, x' \neq x} H(P_t(x), P_s(x'))$$</p>
<p>其中，$H(a, b) = -a \log b$。Teacher / Student 两个网络具有相同的结构但是不同的参数。为了处理不同的输入分辨率（这里为224，96两种），代码中借助 XCiT 的思路进行实现。</p>
<p>与 Knowledge Distillation 不同的是，SSL 中没有Label 来训练 Teacher 模型，那么Teacher 模型的权重怎么更新呢？直观的方案有以下两种方案。</p>
<ul>
<li>
<p>直接拷贝Student网络的参数</p>
<ul>
<li>直接将最新的Student网络的权重拷贝过来 - 不收敛</li>
<li>拷贝上一次 Iteration 的权重 - 不收敛</li>
<li>拷贝上一次 Epoch 的权重 - 66.6</li>
</ul>
</li>
<li>
<p>利用 EMA 机制来更新 Teacher 模型的权重，也就是 Momentum Encoder - 72.8</p>
<p>$$\theta_t \leftarrow \lambda \theta_t + (1 - \lambda)\theta_s$$</p>
<p>其中，权重$\lambda$ 会按照 Cosine 的方式从0.996 上升到 1.0。这种方式的 Teacher 实现原理类似于model ensembling中的Polyak-Ruppert Averaging算法。</p>
</li>
</ul>
<p>实际实验发现，直接拷贝最新的Student权重或者上一次 Iteration 的权重，都会导致模型坍塌；相比之下，拷贝上一Epoch 的 Student 的权重 或者基于 EMA 进行更新的方式得到的 Teacher 模型不会导致坍塌。</p>
<p>接下来就是具体的网络结构了。</p>
<p>DINO 框架下的模型包含两部分，一个是backbone $f$，一个是 projection head $h$，所以得到的特征提取函数是$g = h \circ f$，没有使用BYOL中的 Prediction Head，因为加上之后效果会下降(76.1 vs 75.6)。Projection Head 部分包含3 层的MLP，hidden size为2048 + l2 normalization，最后 3-layer MLP 的输出在接一个<strong>weight normalized fully connected layer</strong>，如图-6所示，最终输出维度是 K，如果backbone 是 ViT，那么MLP中也会去掉 BN，毕竟ViT中没有使用 BN。实验表明，如果不实用 l2 normalization 的话，MLP 中层数大于 2 层后就会出现模型坍塌，但是 层数为1、2层时不会出现这个问题，但是1 - 4 层来看，层数越多效果越好；对于输出维度 K 来说，小于等于65536时，越大模型效果越好；MLP中使用的激活函数GELU效果比 ReLU更好。</p>
<p>对应的 Projection Head 的实现示意图如图-6。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/dino-2.png" alt="图 - 6 DINO Projection Head 实现示意图">
    <figcaption>图 - 6 DINO Projection Head 实现示意图</figcaption>
    </center>
</figure></p>
<p>DINO模型使用Centering &amp; Sharping 两种操作相互配合来防止模型坍塌。Sharping 的实现就是类似 Knowledge Distillation 中计算 Softmax 时，激活数值除以$\tau_s$（student），或者 Teacher 模型的 $\tau_t$，一般来说，这个参数越小，Softmax 的输出就越 sharp，但是这里只针对Teacher模型而言。即：</p>
<p>$$P_t(x) = \frac{\exp (g_{\theta_t} (x)^{(i)}/ \tau_t)}{\sum_{k=1}^K \exp (g_{\theta_t} (x)^{(k)} / \tau_t)}$$</p>
<p>其中$K$就是Student / Teacher Projection Head 部分的输出维度。对应的 Centering 的实现是引入了新的偏置项用于Teacher 模型的输出：</p>
<p>$$g_t (x) \leftarrow g_t(x) + c$$</p>
<p>然后这个偏置项会根据Teacher模型的输出进行移动平均更新。</p>
<p>$$c \leftarrow m * c + (1 - m) \frac{1}{B} \sum_{i=1}^B g_{\theta_t} (x_i)$$</p>
<p>对应的伪代码实现如下。</p>
<p><figure>
    <center>
    <img src="/imgs/img-transform-ssl/dino-1.png" alt="图 - 7 DINO伪代码实现">
    <figcaption>图 - 7 DINO伪代码实现</figcaption>
    </center>
</figure></p>
<h3 id="position-embedding-的生成">Position Embedding 的生成</h3>
<p>与 MoCoV3 中使用 sin-cos 的位置编码不同，DINO 针对不同的分辨率采用的是对 position embedding 利用 bicubic 的方式进行上/下采样。</p>
<p>具体实现代码如下，可以看出主要是将 Patch 恢复成 $(B, C, H, W)$ 格式，然后借助<code>torch.nn.functional.interpolate()</code>函数进行下采样。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">interpolate_pos_encoding</span>(<span style="color:#999">self</span>, x, w, h):
    npatch <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
    N <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_embed<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>
    <span style="color:#000;font-weight:bold">if</span> npatch <span style="color:#000;font-weight:bold">==</span> N <span style="color:#000;font-weight:bold">and</span> w <span style="color:#000;font-weight:bold">==</span> h:
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_embed
    class_pos_embed <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_embed[:, <span style="color:#099">0</span>]
    patch_pos_embed <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_embed[:, <span style="color:#099">1</span>:]
    dim <span style="color:#000;font-weight:bold">=</span> x<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]
    w0 <span style="color:#000;font-weight:bold">=</span> w <span style="color:#000;font-weight:bold">//</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>patch_embed<span style="color:#000;font-weight:bold">.</span>patch_size
    h0 <span style="color:#000;font-weight:bold">=</span> h <span style="color:#000;font-weight:bold">//</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>patch_embed<span style="color:#000;font-weight:bold">.</span>patch_size
    <span style="color:#998;font-style:italic"># we add a small number to avoid floating point error in the interpolation</span>
    <span style="color:#998;font-style:italic"># see discussion at https://github.com/facebookresearch/dino/issues/8</span>
    w0, h0 <span style="color:#000;font-weight:bold">=</span> w0 <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">0.1</span>, h0 <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">0.1</span>
    patch_pos_embed <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>functional<span style="color:#000;font-weight:bold">.</span>interpolate(
        patch_pos_embed<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#099">1</span>, <span style="color:#0086b3">int</span>(math<span style="color:#000;font-weight:bold">.</span>sqrt(N)), <span style="color:#0086b3">int</span>(math<span style="color:#000;font-weight:bold">.</span>sqrt(N)), dim)<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">0</span>, <span style="color:#099">3</span>, <span style="color:#099">1</span>, <span style="color:#099">2</span>),
        scale_factor<span style="color:#000;font-weight:bold">=</span>(w0 <span style="color:#000;font-weight:bold">/</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(N), h0 <span style="color:#000;font-weight:bold">/</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(N)),
        mode<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;bicubic&#39;</span>,
    )
    <span style="color:#000;font-weight:bold">assert</span> <span style="color:#0086b3">int</span>(w0) <span style="color:#000;font-weight:bold">==</span> patch_pos_embed<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">2</span>] <span style="color:#000;font-weight:bold">and</span> <span style="color:#0086b3">int</span>(h0) <span style="color:#000;font-weight:bold">==</span> patch_pos_embed<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]
    patch_pos_embed <span style="color:#000;font-weight:bold">=</span> patch_pos_embed<span style="color:#000;font-weight:bold">.</span>permute(<span style="color:#099">0</span>, <span style="color:#099">2</span>, <span style="color:#099">3</span>, <span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>view(<span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, dim)
    <span style="color:#000;font-weight:bold">return</span> torch<span style="color:#000;font-weight:bold">.</span>cat((class_pos_embed<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">0</span>), patch_pos_embed), dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)
</code></pre></td></tr></table>
</div>
</div><p>其中，$-1$是为了去掉增加的 <code>[CLS]</code> 这个 Token，然后将 <code>patch_pos_embed</code> 恢复空间维度，利用<code>interpolate()</code>函数进行下采样，最后恢复成$(B, SeqLen, Dim)$的格式与CLS的embedding拼接起来做为返回。</p>
<h3 id="最后一层的处理">最后一层的处理</h3>
<p>DINO模型对最后一层全连接的处理，包括两个方面，首先是使用了 weighted normalized 的全连阶层，其次是在第一个 epoch 的时候，会固定 last layer 的参数，不进行更新。</p>
<p>Weighted Normalized的全连接的实现。对应的函数是<code>torch.nn.utils.weight_norm()</code>，对应的论文是<a href="https://arxiv.org/abs/1602.07868">Weight Normalization</a>，中文博客可以参考：<a href="https://zhuanlan.zhihu.com/p/55102378">模型优化之Weight Normalization - 知乎</a>。</p>
<p>主要是思想是将全连阶层的权重分离为大小、方向两个部分，然后使用 SGD 分别优化这两个部分。</p>
<p>$$w = g \frac{\mathbf{v}}{\parallel \mathbf{v} \parallel}$$</p>
<p>其中$\mathbf{v}$为表示方向的向量。然后SGD优化的时候，分别优化$g$与$\mathbf{v}$。</p>
<p>另一个处理是第一个 epoch 内取消最后一层的梯度更新，实现代码其实就是设置<code>grad=None</code>，即。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">cancel_gradients_last_layer</span>(epoch, model, freeze_last_layer<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>):
    <span style="color:#000;font-weight:bold">if</span> epoch <span style="color:#000;font-weight:bold">&gt;=</span> freeze_last_layer:
        <span style="color:#000;font-weight:bold">return</span>
    <span style="color:#000;font-weight:bold">for</span> n, p <span style="color:#000;font-weight:bold">in</span> model<span style="color:#000;font-weight:bold">.</span>named_parameters():
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#d14">&#34;last_layer&#34;</span> <span style="color:#000;font-weight:bold">in</span> n:
            p<span style="color:#000;font-weight:bold">.</span>grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="dinoloss的实现">DINOLoss的实现</h3>
<p>论文中提到的 Centering &amp; Sharping 两个做法都体现在这个类里；而且包括对应 Sharping 的参数的调整（前30个epoch由0.04 -&gt; 0.07，即<code>warmup_teacher_temp_epochs=30</code>）提高训练稳定性。</p>
<p>对应的实现代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">58
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">DINOLoss</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, out_dim, ncrops, warmup_teacher_temp, teacher_temp,
                 warmup_teacher_temp_epochs, nepochs, student_temp<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.1</span>,
                 center_momentum<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0.9</span>):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__()
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>student_temp <span style="color:#000;font-weight:bold">=</span> student_temp
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center_momentum <span style="color:#000;font-weight:bold">=</span> center_momentum
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ncrops <span style="color:#000;font-weight:bold">=</span> ncrops        <span style="color:#998;font-style:italic"># 2 (global crops) + local crop nums</span>
        <span style="color:#998;font-style:italic"># 注意 Center 尺寸，用于Centering</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>register_buffer(<span style="color:#d14">&#34;center&#34;</span>, torch<span style="color:#000;font-weight:bold">.</span>zeros(<span style="color:#099">1</span>, out_dim))
        <span style="color:#998;font-style:italic"># we apply a warm up for the teacher temperature because</span>
        <span style="color:#998;font-style:italic"># a too high temperature makes the training instable at the beginning</span>
        <span style="color:#998;font-style:italic"># Teacher 模型的 Sharping 参数的更新</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>teacher_temp_schedule <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate((
            np<span style="color:#000;font-weight:bold">.</span>linspace(warmup_teacher_temp,
                        teacher_temp, warmup_teacher_temp_epochs),
            np<span style="color:#000;font-weight:bold">.</span>ones(nepochs <span style="color:#000;font-weight:bold">-</span> warmup_teacher_temp_epochs) <span style="color:#000;font-weight:bold">*</span> teacher_temp
        ))

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, student_output, teacher_output, epoch):
        <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">        Cross-entropy between softmax outputs of the teacher and student networks.
</span><span style="color:#d14">        &#34;&#34;&#34;</span>
        student_out <span style="color:#000;font-weight:bold">=</span> student_output <span style="color:#000;font-weight:bold">/</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>student_temp
        <span style="color:#998;font-style:italic"># 分成不同的chunk，一共 ncrops 份，每份对应一个 crops</span>
        student_out <span style="color:#000;font-weight:bold">=</span> student_out<span style="color:#000;font-weight:bold">.</span>chunk(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ncrops)

        <span style="color:#998;font-style:italic"># teacher centering and sharpening</span>
        <span style="color:#998;font-style:italic">## 论文里提到的思路在这里</span>
        temp <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>teacher_temp_schedule[epoch]
        teacher_out <span style="color:#000;font-weight:bold">=</span> F<span style="color:#000;font-weight:bold">.</span>softmax((teacher_output <span style="color:#000;font-weight:bold">-</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center) <span style="color:#000;font-weight:bold">/</span> temp, dim<span style="color:#000;font-weight:bold">=-</span><span style="color:#099">1</span>)
        teacher_out <span style="color:#000;font-weight:bold">=</span> teacher_out<span style="color:#000;font-weight:bold">.</span>detach()<span style="color:#000;font-weight:bold">.</span>chunk(<span style="color:#099">2</span>)

        total_loss <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
        n_loss_terms <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>
        <span style="color:#000;font-weight:bold">for</span> iq, q <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(teacher_out):
            <span style="color:#000;font-weight:bold">for</span> v <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#0086b3">len</span>(student_out)):
                <span style="color:#000;font-weight:bold">if</span> v <span style="color:#000;font-weight:bold">==</span> iq:
                    <span style="color:#998;font-style:italic"># we skip cases where student and teacher operate on the same view</span>
                    <span style="color:#000;font-weight:bold">continue</span>
                loss <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#000;font-weight:bold">-</span>q <span style="color:#000;font-weight:bold">*</span> F<span style="color:#000;font-weight:bold">.</span>log_softmax(student_out[v], dim<span style="color:#000;font-weight:bold">=-</span><span style="color:#099">1</span>), dim<span style="color:#000;font-weight:bold">=-</span><span style="color:#099">1</span>)
                total_loss <span style="color:#000;font-weight:bold">+=</span> loss<span style="color:#000;font-weight:bold">.</span>mean()
                n_loss_terms <span style="color:#000;font-weight:bold">+=</span> <span style="color:#099">1</span>
        total_loss <span style="color:#000;font-weight:bold">/=</span> n_loss_terms
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>update_center(teacher_output)
        <span style="color:#000;font-weight:bold">return</span> total_loss

    <span style="color:#3c5d5d;font-weight:bold">@torch.no_grad</span>()
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">update_center</span>(<span style="color:#999">self</span>, teacher_output):
        <span style="color:#d14">&#34;&#34;&#34;
</span><span style="color:#d14">        Update center used for teacher output.
</span><span style="color:#d14">        &#34;&#34;&#34;</span>
        batch_center <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(teacher_output, dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>, keepdim<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)
        <span style="color:#998;font-style:italic">## 收集teacher 模型的输出并计算平均值</span>
        dist<span style="color:#000;font-weight:bold">.</span>all_reduce(batch_center)
        batch_center <span style="color:#000;font-weight:bold">=</span> batch_center <span style="color:#000;font-weight:bold">/</span> (<span style="color:#0086b3">len</span>(teacher_output) <span style="color:#000;font-weight:bold">*</span> dist<span style="color:#000;font-weight:bold">.</span>get_world_size())

        <span style="color:#998;font-style:italic"># ema update</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center_momentum <span style="color:#000;font-weight:bold">+</span> batch_center <span style="color:#000;font-weight:bold">*</span> (<span style="color:#099">1</span> <span style="color:#000;font-weight:bold">-</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>center_momentum)
</code></pre></td></tr></table>
</div>
</div><h3 id="dino实现中的其它一些细节">DINO实现中的其它一些细节</h3>
<ul>
<li>
<p>使用Pre-norm的实现。由于输入的 Crops 之间尺寸不一致（224 与 96），所以需要注意<code>MultiCropWrapper</code>类的实现以及<code>DINOLoss</code>损失函数的实现</p>
</li>
<li>
<p>learning rate scaling rule: $lr = 0.0005 * batchsize / 256$；然后前10 epoch 进行线性warmup，之后按照 cosine annealing进行下降</p>
</li>
<li>
<p>wd参数会按照 cosine 过程从0.04 增加到0.4</p>
</li>
<li>
<p>Centering 中的$m=0.9$时效果最好，太大如0.999的话就会坍塌</p>
</li>
<li>
<p>$\tau_s=0.1$，但是Sharping中的$\tau_t$在前30个epoch 会由0.04线性增加到0.07；这个设置时对比了几种$\tau_t$取值的结果得到的</p>
</li>
<li>
<p>数据增广方式与 BYOL 相同，也就是 Asymmetric Data Augmentation的方式</p>
</li>
<li>
<p>随着 patch 大小的下降，模型效果会上升，如5 * 5 &gt; 8 * 8 &gt; 16 * 16，但是模型计算耗时会增加</p>
</li>
<li>
<p>对Batch Size不是非常敏感，1024时效果最好(59.9)，512时为59.6，256时为59.1，128时为57.9</p>
</li>
<li>
<p>作者对 Teacher 模型的输出对比了三种输出处理方式</p>
<ul>
<li>Centering（本文默认做法）</li>
<li>Sinkhorn-Knopp</li>
<li>Softmax</li>
<li>发现，当使用 Centering 配合 Momentum Teacher Update from Student 时效果最好；Momentum 的效果都大幅优于不使用 Momentum 的训练效果</li>
</ul>
</li>
<li>
<p>Multi-Crop的数据增广方式对DINO模型的帮助最大</p>
</li>
<li>
<p>作者尝试了集中 <code>[CLS]</code> 的使用方式</p>
<ul>
<li>取后面$l$层的<code>[CLS]</code>拼接起来，ViT-S模型取最后4层的<code>[CLS]</code>时效果最好(77.0 VS 76.1)；但是对 ViT-B模型而言，这种拼接方式没有帮助，而取<code>[CLS]</code>的输出与其它Tokens的平均拼接起来效果最好（78.2 vs 78.0）</li>
</ul>
</li>
<li>
<p>在第一个 Epoch 训练过程中，固定 Last Layer 的参数，具体参考<code>cancel_gradients_last_layer()</code>函数</p>
</li>
<li>
<p>BYOL中基于Predictor &amp; BN来对抗模型坍塌，但是 DINO 使用的是Teacher Output Centering来实现，而且与 Sharping 结合起来效果才是最好。这里采用Momentum的方式来更新 Teacher 模型，如果不采用这种方式，另一种常见的方式是拷贝 Student 的权重 + Stop Gradient来避免模型坍塌。</p>
</li>
<li>
<p>与 MoCo V3中的发现类似，ViT-S 相比于 ResNet50 具有更大的潜力，尤其是 k-NN 指标下，两者差距达到 14%。</p>
</li>
<li>
<p>类似于 Mean Teacher，不论是 ViT / ResNet50，在训练过程中，Teacher 模型的效果都优于 Student 模型的效果。</p>
</li>
<li>
<p>论文里贴出来了一些 Attention Map，具体实现是取出最后一层的 Attention 矩阵，尺寸是 $(B, HeadNum, 1 + SeqLen, 1 + SeqLen)$，其中 1 表示新增加的<code>[CLS]</code> Token，然后就是</p>
</li>
</ul>
<h3 id="与-moco-v3-的异同">与 MoCo V3 的异同</h3>
<p>相同点。</p>
<ul>
<li>都包含两个Encoder，MoCo V3中称为Base/Target Encoder，DINO中称为 Student / Teacher 模型</li>
<li>两个 Encoder 之间都是通过 Momentum 的方式由一个 Encoder 的权重来更新另一个Encoder 的权重，并且这个 m 参数都按照 cosine 的方式增加到 1.0</li>
</ul>
<p>不同点。</p>
<ul>
<li>MoCo V3中，两个 Encoder 结构不一样，主要在于 Base Encoder 多了一个 Prediction Head；而DINO中两个模型的结构是完全相同的，都不包含 Prediction Head</li>
<li>DINO 计算 Softmax 的$\tau$在两个 Encoder 中是不同的，使用的Loss也不同</li>
<li>DINO 使用了 Multi Crop 的数据增广方式实现 local-to-global 内容的学习，也就是 DINO 不仅要学习对 Transformers 的不变性，还要学习到 local-to-global 的一致性!</li>
<li>其它实现细节上的不同，如 $\tau_t$ 的调整等</li>
</ul>
<h2 id="moby">MoBY</h2>
<p>本文没啥新的技巧，主要就是将 MoCo V2 与 BYOL 进行了组合创新，然后 Backbone 替换为 Swin Transformer 模型，以及验证预训练对下游如 Object Detection &amp; Semantic Segmentation等任务的收益。</p>
<p>BYOL 的Asymmetric Encoder 结构是指模型包含两个 Encoder，分别是online encoder 以及 target encoding，两个 Encoder 都包含一个Backbone 以及一个 Projection Head，但是 Online Encoder 会多包含一个 Prediction Head，这也就是 Asymmetric Encoder 名字的来源。Online Encoder的参数基于梯度进行更新；Target Encoder 的参数基于 Online Encoder 的参数基于momentum的方式进行更新，并且momentum的参数会从0.99逐步更新到1.0。</p>
<p>Loss的实现与MoCo V3 的实现类似，不赘述。</p>
<p>作者新引入了Aynmmetric Drop Path 来提高性能。Drop Path 在基于 Transformer 的有监督训练过程中已经被证明是一个非常有用的正则化手段。这里非对称 Drop Path 的意思是，Drop Path 仅应用在 Online Encoder 上，如果也用在 Target Encoder 上会导致性能下降（70.9 vs 69.0）。这一技巧其实在 DINO 的代码里也有被使用的体现。</p>
<p>其它的结果对比以及数据细节可参考论文。</p>
<h2 id="消融实验">消融实验</h2>
<h3 id="position-embedding">Position Embedding</h3>
<p>MoCo V3 里基于 Linear Acc 对Position Embedding进行了对比，发现 Sin-Cos 方式略好于 Learned 的方式，Sin-Cos 的实现可以参考下 MoCo V3 的源码；另一方面，与不使用 Pos Embedding 相比的结果来说，目前的 Position Embedding 的使用还有待进一步挖掘。</p>
<table>
<thead>
<tr>
<th>ViT-B, 300 ep</th>
<th>sin-cos</th>
<th>learned</th>
<th>None</th>
</tr>
</thead>
<tbody>
<tr>
<td>linear acc.</td>
<td>76.5</td>
<td>76.1</td>
<td>74.9</td>
</tr>
</tbody>
</table>
<h3 id="class-token">Class Token</h3>
<p>MoCo v3 的作者测试了使用 Global Avg Pooling 代替 <code>[CLS]</code> Token的特征，发现Acc没有明显变化(76.5 VS 76.3)；其中 LN + Pool 表示在 Pooling 层之前先计算一次 LN。</p>
<table>
<thead>
<tr>
<th>ViT-B, 300 ep -</th>
<th>- w / CLS -</th>
<th>- w/o CLS; LN + Pool -</th>
<th>- w/o CLS; Pool</th>
</tr>
</thead>
<tbody>
<tr>
<td>lienar acc.</td>
<td>76.5</td>
<td>69.7</td>
<td>76.3</td>
</tr>
</tbody>
</table>
<h3 id="traning-length">Traning Length</h3>
<p>MoCo V3 作者发现，模型越小(ViT/S, ResNet50)，训练越长的增益越大。</p>
<table>
<thead>
<tr>
<th></th>
<th>300 ep</th>
<th>600 ep</th>
</tr>
</thead>
<tbody>
<tr>
<td>ViT-S/16</td>
<td>72.5</td>
<td>73.4 (+0.9)</td>
</tr>
<tr>
<td>ViT-B/16</td>
<td>76.5</td>
<td>76.7 (+0.2)</td>
</tr>
</tbody>
</table>
<p>对于 DINO也有同样的发现。</p>
<table>
<thead>
<tr>
<th>DINO ViT-S</th>
<th>100 ep</th>
<th>300 ep</th>
<th>800 ep</th>
</tr>
</thead>
<tbody>
<tr>
<td>k-NN top1</td>
<td>70.9</td>
<td>72.8</td>
<td>74.5</td>
</tr>
</tbody>
</table>
<h2 id="其它">其它</h2>
<p>实际实验中，按照 SimSiam 的方式， Backbone 换成 VOLO，优化器使用 AdamW，然后模型就很容易塌缩了，表现在 Loss 数值快速到达 -1.0（SimSiam 使用的 Loss），表明已经达到最优了，无法继续优化。更换成 SGD 优化器之后，情况有所缓解，但是收敛速度还是非常快，Epoch 1 之后也开始坍塌。模型替换为 ResNet50 之后，模型就可以训练。所以暂定原因为 Transformer 模型不适用于 SimSiam 无监督训练框架。</p>
<p>将自监督训练框架切换为 MoCo V3 的方式，可以正常训练。针对参数$T$，当取值为1.0时收敛变慢，取值为默认的0.2之后收敛变快。然后使用预训练后的参数初始化Transformer模型，可以让模型收敛的更好，达到与使用RandomAug等数据增广训练的模型非常接近的精度水平，而且前30个epoch的结果就基本稳定了；但是如果也加上这些数据增广，那么精度怎么变化呢？基于有限的实验，前60epoch的结果来看，使用自监督训练模型进行初始化时，精度比从头训练的情况增加了不到1个百分点（0.8左右）最终的精度对比时：80.63 vs 80.58，这里有一个因素是模型结构发生了变化，也就是 CNN Stem 的结构由(conv7x2 -&gt; conv3x1 -&gt; conv3x1 -&gt; conv4x4)变为(conv7x2 -&gt; conv3x2 -&gt; conv3x2 -&gt; conv3x1)。主要问题在于前者训练过程非常不稳定，精度增加非常缓慢，后者的精度则稳步上升。经过进一步验证，同一种架构同一个模型，不同初始化方式，精度对比是：80.63 vs 80.52，只有0.1个百分点的提高而已。至于温度参数的作用，可以参考<a href="https://arxiv.org/abs/2012.09740">Understanding the Behaviour of Contrastive Loss</a>这篇论文。</p>
<p>将自监督训练框架切换为 DINO 的方式，也可以正常训练，但是训练速度会变慢比较多，毕竟 crop 的个数增加了。warmup epochs 占总的 epoch 的比例由 40% -&gt; 10% 之后，收敛速度变快。</p>
        
            <p>Pytorch 实现学习积累。</p>
<h2 id="基础">基础</h2>
<ul>
<li>All objects in pytorch are passed by reference in python. But doing <code>a=</code> does not try to change <code>a</code> in-place, it only give the name <code>a</code> to the object returned by the right hand side.</li>
<li>矩阵乘：@， Matmul，mm（后两者的区别在于 mm 仅适用于二维Tensor，matmul适合高维Tensor）；*，mul 实现的是element-wise乘</li>
<li><code>_</code> suffix ops 是in-place操作</li>
<li>Tensor 与 Numpy 之间可以共享底层存储空间，所以修改一个也会导致另一个变量发生变化。如<code>.numpy()</code>操作，<code>from_numpy()</code>等</li>
<li>自定义Dataset，需要自己实现<code>__init__</code>、<code>__len__</code>、<code>__getitem__</code>等函数；<code>ToTensor</code>会将PIL Image、NumPy ndarry转换成<code>FloatTensor</code>，并且将像素上的数值范围缩放到(0.0, 1.0)之间。</li>
<li>继承<code>nn.Module</code>创建模型的时候，会自动收集定义在models内的fields，并且让所有的 parameters 都可以被<code>parameters()</code>以及<code>named_parameters()</code>等方法获取到</li>
</ul>
<h2 id="module">Module</h2>
<p>Module 在调用的时候实际会调用<code>Module._call_impl()</code>函数，这个函数里调用顺序如下。</p>
<ol>
<li>调用<code>_global_forward_pre_hooks</code>或者<code>self._forward_pre_hooks</code>里面所有的hook，对当前的Module以及输入数据进行处理，hook 函数的格式是：<code>hook(module, input) -&gt; None or modified input</code>，如果 hook 函数会返回数据，那么这个返回的数据才是真正的输入 forward() 函数进行计算的数据</li>
<li>调用<code>forward_call()</code>函数完成前向计算</li>
<li>调用<code>_global_forward_hooks</code>或者<code>self._forward_hooks</code>里面的所有hook，hook函数签名是<code>hook(module, input, output) -&gt; None or modified output</code>，函数的输出是最终的输出</li>
<li><code>full_backward_hooks</code>里的 hooks</li>
</ol>
<h2 id="autograd">Autograd</h2>
<p>通过设置Tensor的<code>requires_grad</code>来决定是否需要计算 Loss 对该 Tensor 的梯度。</p>
<ul>
<li>
<p>torch.autograd.Function</p>
<p>记录对Tensor的操作，是一个类，包含<code>forward()</code>、<code>backward()</code>两个静态成员函数。每个Function完成对 Tensor 的一个操作，并记录发生的事情。所有的 Function 被组织成有向无环图（DAG），边表示数据依赖(input &lt;&ndash; output)。当反向传播时，按照拓扑顺序依次调用Function的<code>backward()</code>函数。</p>
<p>实际使用的时候就是继承Function类并实现这两个静态成员函数。一个具体例子如下，所以都是静态成员函数进行操作，无需创建具体实例。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">Exp</span>(Function):
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(ctx, i):
        result <span style="color:#000;font-weight:bold">=</span> i<span style="color:#000;font-weight:bold">.</span>exp()
        ctx<span style="color:#000;font-weight:bold">.</span>save_for_backward(result)
        <span style="color:#000;font-weight:bold">return</span> result
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">backward</span>(ctx, grad_output):
        result, <span style="color:#000;font-weight:bold">=</span> ctx<span style="color:#000;font-weight:bold">.</span>saved_tensors
        <span style="color:#000;font-weight:bold">return</span> grad_output <span style="color:#000;font-weight:bold">*</span> result

  output <span style="color:#000;font-weight:bold">=</span> Exp<span style="color:#000;font-weight:bold">.</span>apply(<span style="color:#0086b3">input</span>)
</code></pre></td></tr></table>
</div>
</div><p>注意，Function知道Tensor的前向计算，也支持后向传播，后向传播函数保存在<code>tensor.grad_fn</code>属性中。也就是说Function 是计算图中的节点，边才是 Tensor。</p>
</li>
<li>
<p>is_leaf</p>
<p>这个函数用来判断Tensor是否保存了grad。</p>
<ul>
<li>如果Tensor的<code>requires_grad=False</code>，则通常是 Leaf</li>
<li>如果 Tensor 是用户创建的，那么即使<code>requires_grad=True</code>也是Leaf，意味着这些Tensor不是一个Op的结果，并且<code>grad_fn=None</code></li>
<li>只有Leaf Tensor 才会在<code>backward()</code>过程中保存梯度结果；如果需要获取那些non-leaf节点的grad，可以使用<code>Tensor.retain_grad()</code>来修改</li>
<li>第三条与第一条貌似冲突，其实不冲突，因为 <code>requires_grad=False</code>的含义是指这个 Tensor 的梯度不需要向后传播了，而不是不会计算该 Tensor 的梯度，也就是实际是指<code>grad_fn=None</code>。</li>
<li>从CPU拷贝到 GPU 上也算是一个 Op 操作，具体例子可以查看：<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html?highlight=is_leaf#torch.Tensor.is_leaf">torch.tensor.is_leaf</a></li>
</ul>
</li>
<li>
<p>Disabling Gradient Tracking</p>
<p>有时候需要停止一些 Tensor 的梯度后向传播，那些<code>requires_grad=True</code>的 Tensor 都会跟踪该Tensor 的计算历史，并支持梯度计算。所以要想阻止后向传播，有两种方式：</p>
<ul>
<li>使用 <code>torch.no_grad()</code> block 进行封装</li>
<li>使用 <code>detach()</code>，相当于新建了一个Tensor返回的，所以计算梯度更新这个新的 Tensor，之前旧的 Tensor 数值也会保持不变。</li>
</ul>
<p>下面的方式适合单个 Parameter 的梯度更新。</p>
<ul>
<li>设置<code>parameter.requires_grad=False</code></li>
<li>设置<code>parameter.grad=None</code>，优化器在根据梯度更新这个参数时，如果发现 <code>grad=None</code>，则略过当前参数，从而实现防止梯度反向传播的目的</li>
</ul>
<p>经过上述两种方式处理后的 Tensor 直接影响是，不会向后传播 Gradient，也不会发生数值变化。</p>
</li>
<li>
<p>Tensor Gradients and Jacobian Products</p>
<p>大部分情况下，Loss函数计算得到的是一个Scalar数值，计算梯度容易理解。但是当 Loss 是一个多维的Tensor时，反向传播计算的就是<code>Jacobian product</code>，而不是真正的梯度了。</p>
<p>一般来说，输入、输出都是 Tensor 时，反向传播得到的是一个<code>Jacobian matrix</code>，但是 pytorch 支持<code>Jacobian product</code>的计算，此时需要一个与输出Loss同等尺寸的Tensor作为<code>backward()</code>函数的输入。</p>
<p>下式中，<code>x, y</code>为输入输出，计算<code>y</code>对<code>x</code>的梯度时，引入的 <code>v</code> 就是上面提到的需要跟 <code>y</code> 尺寸相同的新引入的 Tensor，具体例子可参考<a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">Automatic Diff</a>下方。</p>
<p>$$y=f(x), J = \frac{\partial y}{\partial x}, v^T \cdot J$$</p>
</li>
<li>
<p>optimize steps</p>
<ol>
<li>call <code>optimizer.zero_grad()</code></li>
<li>call <code>loss.backward()</code></li>
<li>call <code>optimizer.step()</code></li>
</ol>
</li>
<li>
<p>其它</p>
<ul>
<li>每次<code>backward()</code>之后，创建的计算图都会被重置，从而支持每次 iter 之间修改数据的尺寸、条件判断修改计算图等，也就是对动态计算图的支持；如果想保留当前的计算图，可以在 <code>backward()</code>函数中设置<code>retain_graph=True</code></li>
<li>但是连续两次<code>backward()</code>时，同一个 Tensor 的梯度会被累加。</li>
</ul>
</li>
</ul>
<h2 id="extending-pytorch">Extending Pytorch</h2>
<p>主要参考：<a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">Extending Pytorch</a></p>
<h3 id="extending-autograd">Extending Autograd</h3>
<p>这里是新增一个支持前向、后向计算的方法，也就是说，当前 Torch 内所有支持训练的计算（支持后向传播梯度）本质上都是来自<code>torch.autograd</code>命名空间下的<code>Function</code>。所以新增一个计算方法，需要作为派生自<code>torch.autograd.Function</code>类的子类来完成。</p>
<p>存在这种extending方法的主要原因是，希望新增一个自定义操作，可以用在模型训练中，而这个新增的操作要么不可求导、要么是一个非Torch的变量（比如Numpy Array），但是还是希望模型中新增了这个计算之后，梯度仍然可以沿着模型传递，从而支持 autograd engine 的模型参数更新。换句话说，新增的 Function 子类，可以隐藏不支持求导的计算，将断开的梯度传播链路 chain 起来。另一种情况是，新增自定义 Function 可以Wrap C++实现的操作，或者进行一些类似Op融合的操作来提高运算效率。</p>
<p>新增 Autograd Function 的步骤主要分为四步，具体写代码是实现两个Function子类的静态函数。下面是四个实现步骤:</p>
<ol>
<li>派生<code>torch.autograd.Function</code>子类并且实现两个静态函数</li>
</ol>
<ul>
<li>
<p>forward 函数</p>
<p>用于前向计算的函数，可以接收任意数目的参数，如果有默认值，则对应的参数是可选的。输出参数的类型可以是单个 Tensor 输出，或者 Tuple 形式的多个输出。</p>
</li>
<li>
<p>backward 函数</p>
<p>定义梯度计算函数。输入的参数是对应 <code>forward()</code> 函数输出参数的梯度，也就是前向过程中有几个输出，这里就有几个输入，然后就可以根据这些输入的梯度参数计算输出梯度了，而返回变量个的个数与<code>forward()</code>函数的输入参数的个数一致。当<code>foward()</code>有可选参数的时候，这些参数对应的返回的梯度应该是None。</p>
</li>
</ul>
<ol start="2">
<li>使用<code>ctx</code>参数提供的一些操作来保证新增的Function可以适应autograd engine中的计算</li>
</ol>
<p>ctx 提供了一些有用的参数可以帮助新 Function 的实现，并且支持 autograd engine 的计算。</p>
<ul>
<li>
<p><code>save_for_backward()</code>函数</p>
<p>前面提到，<code>backward()</code>函数的输入参数都是梯度值，有些计算过程还需要模型对应计算的状态参数，比如 CNN 中的权重/偏置项等。这个函数的作用就是为了在前向计算函数中保存这些参数的，然后在后向过程中取出来用于计算梯度。</p>
</li>
<li>
<p><code>make_dirty()</code>函数</p>
<p>前向计算中，如果参数使用了in-place操作，那么就需要用这个函数来指示。</p>
</li>
<li>
<p><code>mark_non_differentiable()</code>函数</p>
<p>告诉 autograd engine，对应的输出不可求导。</p>
</li>
<li>
<p><code>set_materialize_grad()</code>函数</p>
<p>我的理解是，如果有些参数的梯度是None，但是如果设置了<code>set_materialize_grad(True)</code>，那么这些梯度会用合适大小的全零的 Tensor 代替；如如果设置为 False，则这些参数传入 <code>backward()</code> 函数中对应的梯度就会保持 None。</p>
</li>
</ul>
<ol start="3">
<li>必要的时候使新增的<code>Function</code>支持高阶求导</li>
</ol>
<p>为了支持高阶求导，需要在 <code>backward()</code> 的修饰器中使用 <code>once_differentiable()</code> 来设置该后向传播函数只能求导一次。</p>
<ol start="4">
<li>建议使用<code>torch.autograd.gradcheck()</code>函数对结果进行验证</li>
</ol>
<p>使用<code>torch.autograd.gradcheck()</code>函数来验证实现的后向传播函数是否正确。</p>
<p>一个具体的例子如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># Inherit from Function</span>
<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">LinearFunction</span>(Function):

    <span style="color:#998;font-style:italic"># Note that both forward and backward are @staticmethods</span>
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#998;font-style:italic"># bias is an optional argument</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(ctx, <span style="color:#0086b3">input</span>, weight, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">None</span>):
        ctx<span style="color:#000;font-weight:bold">.</span>save_for_backward(<span style="color:#0086b3">input</span>, weight, bias)
        output <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">input</span><span style="color:#000;font-weight:bold">.</span>mm(weight<span style="color:#000;font-weight:bold">.</span>t())
        <span style="color:#000;font-weight:bold">if</span> bias <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            output <span style="color:#000;font-weight:bold">+=</span> bias<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>expand_as(output)
        <span style="color:#000;font-weight:bold">return</span> output

    <span style="color:#998;font-style:italic"># This function has only a single output, so it gets only one gradient</span>
    <span style="color:#3c5d5d;font-weight:bold">@staticmethod</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">backward</span>(ctx, grad_output):
        <span style="color:#998;font-style:italic"># This is a pattern that is very convenient - at the top of backward</span>
        <span style="color:#998;font-style:italic"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>
        <span style="color:#998;font-style:italic"># None. Thanks to the fact that additional trailing Nones are</span>
        <span style="color:#998;font-style:italic"># ignored, the return statement is simple even when the function has</span>
        <span style="color:#998;font-style:italic"># optional inputs.</span>
        <span style="color:#0086b3">input</span>, weight, bias <span style="color:#000;font-weight:bold">=</span> ctx<span style="color:#000;font-weight:bold">.</span>saved_tensors
        grad_input <span style="color:#000;font-weight:bold">=</span> grad_weight <span style="color:#000;font-weight:bold">=</span> grad_bias <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>

        <span style="color:#998;font-style:italic"># These needs_input_grad checks are optional and there only to</span>
        <span style="color:#998;font-style:italic"># improve efficiency. If you want to make your code simpler, you can</span>
        <span style="color:#998;font-style:italic"># skip them. Returning gradients for inputs that don&#39;t require it is</span>
        <span style="color:#998;font-style:italic"># not an error.</span>
        <span style="color:#000;font-weight:bold">if</span> ctx<span style="color:#000;font-weight:bold">.</span>needs_input_grad[<span style="color:#099">0</span>]:
            grad_input <span style="color:#000;font-weight:bold">=</span> grad_output<span style="color:#000;font-weight:bold">.</span>mm(weight)
        <span style="color:#000;font-weight:bold">if</span> ctx<span style="color:#000;font-weight:bold">.</span>needs_input_grad[<span style="color:#099">1</span>]:
            grad_weight <span style="color:#000;font-weight:bold">=</span> grad_output<span style="color:#000;font-weight:bold">.</span>t()<span style="color:#000;font-weight:bold">.</span>mm(<span style="color:#0086b3">input</span>)
        <span style="color:#000;font-weight:bold">if</span> bias <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span> <span style="color:#000;font-weight:bold">and</span> ctx<span style="color:#000;font-weight:bold">.</span>needs_input_grad[<span style="color:#099">2</span>]:
            grad_bias <span style="color:#000;font-weight:bold">=</span> grad_output<span style="color:#000;font-weight:bold">.</span>sum(<span style="color:#099">0</span>)

        <span style="color:#000;font-weight:bold">return</span> grad_input, grad_weight, grad_bias
</code></pre></td></tr></table>
</div>
</div><p>在实际使用时，为了方便，一般会有下面的一条赋值：</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">linear <span style="color:#000;font-weight:bold">=</span> LinearFunction<span style="color:#000;font-weight:bold">.</span>apply
</code></pre></div><h3 id="extending-nn">Extending nn</h3>
<p>一般来说，扩展 nn 有两种方式，一种是上面提到的 Function 方式，一般适用于那些没有自身计算状态参数（如卷积权重）的操作，另一种是定义 Module 子类的方式，后者需要自定义<code>__init__()</code>以及<code>forward()</code>两个成员函数，<code>forward()</code>成员函数内一般就会调用上面提到的 Function 来实现操作。</p>
<h2 id="optimizer">Optimizer</h2>
<p>实现自己的 Optimizer 的时候，需要继承<code>torch.optim.Optimizer</code>类。需要实现<code>__init__、__setstate__、step</code>等函数；然后将新 Optimizer 的参数，比如lr, eps, betas等参数保存到<code>defaults</code>字典中，并跟parameters一起传给Base类的<code>__init__</code>函数。<code>__setstate__</code>函数主要是为了比如在pickle等序列化中使用，并做必要的更新，比如<code>self.param_groups</code>里的成员。在 <code>step()</code>函数里，会更新<code>self.state</code>成员变量，然后后面更新的时候就可以直接从 <code>state</code> 里面取出来进行更新就可以了。</p>
<p>此外，defaults 字典里面的信息在<code>add_param_group()</code>函数里面被放入<code>self.param_groups</code>里面了，如lr, eps, betas等；特定Optimizer的相关数据放在<code>self.states</code>里面了，如Adam里面的 m / v 等。</p>
<p>具体例子可以参考 TIMM 库里的AdamW算法实现。</p>
        
            <p>深度学习中的一些可视化技术。</p>
<p>主要包括CAM、t-SNE两个方面，CAM又包括Gradient Free 以及 Gradient Based 两种实现思路；t-SNE更多的是用于高维空间在低维空间的可视化。</p>
<h2 id="cam">CAM</h2>
<p>再记录GradCam之前，可以先看下Cam算法的实现。参考博客是：<a href="https://zhuanlan.zhihu.com/p/269702192">万字长文：特征可视化技术(CAM)</a>。</p>
<h3 id="cam基础">CAM基础</h3>
<p>全称 Class Activation Mapping。也就是获取每个类别在Feature Map上关注点的分布，比如利用最后一层CNN的Feature Map，将所有的Channel加权融合为一个二维图片，然后这个二维图片就被认作激活图。以ResNet18为例，最后一层CNN的Feature Map包含512个Channel，如果单独可视化每个通道，则比较难理解，所以CAM会根据每个通道不同的贡献大小对所有的通道进行加权融合获取一张CAM。</p>
<p>效果如图-1。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam0.png" alt="图 - 1 CAM结果示意图">
    <figcaption>图 - 1 CAM结果示意图</figcaption>
    </center>
</figure></p>
<p>CAM实现的步骤如下：</p>
<ol>
<li>提取需要可视化的特征层，例如尺寸为7 * 7 * 512的张量</li>
<li>获取该张量的每个channel的权重，即长度为512的向量</li>
<li>对 Step1 中的张量按照 Step2 中的权重进行加权，获取尺寸为 7 * 7 的Map</li>
<li>对该Map进行归一化，并通过插值的方式</li>
</ol>
<p>上面提到，CAM可以分为Gradient Free / Gradient Based两种方式，两者的主要区别在于Step 2中计算Channel的权重方式不同，后者会利用梯度信息，前者不需要。</p>
<p>Gradient Based常见的算法包括</p>
<ul>
<li>Grad CAM (2016.10)</li>
<li>Grad CAM ++ (2017.10)</li>
<li>Smooth Grad-CAM++ (2019.08)</li>
</ul>
<p>Gradient Free常见的算法包括</p>
<ul>
<li>CAM (2015.12)</li>
<li>score-CAM (2019.10)</li>
<li>ss-CAM (2020.06)</li>
<li>Ablation-CAM (2020)</li>
</ul>
<h3 id="利用gap获取cam">利用GAP获取CAM</h3>
<p>属于Gradient Free类算法。</p>
<p>CAM的实现依赖于CNN卷积之后使用Global Average Pooling (GAP) 来实现；也就是说网络结构具有如下特征，经过若干层CNN得到Feature Map，然后利用GAP来压缩空间维度，然后压缩后的向量利用一个线性变换得到对应的类别预测。然后CAM就利用最后一个线性变换的权重作为每个类别的Channel的权重进行加权。示意图如图2所示，对于Australian terrier类的全连接权重为 $w_1, \ldots, w_n$。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam1.png" alt="图 - 2 CAM实现示意图">
    <figcaption>图 - 2 CAM实现示意图</figcaption>
    </center>
</figure></p>
<blockquote>
<p>Global average pooling outputs the spatial average of the feature map of each unit at the last convolutional layer. A weighted sum of these values is used to generate the final output. Similarly, we compute a weighted sum of the feature maps of the last convolutional layer to obtain our class activation maps.</p>
</blockquote>
<p>利用数学公式说明就是，设$f_k(x, y)$表示最后一层CNN的Feature Map中第 k 个unit (channel)在空间位置(x, y)处的数值，则GAP的计算就是$\sum_{x, y}f_k(x, y)$。然后对于类别 c，线性变换得到输入 Softmax 的数值，也就是 $S_c = \sum_k w_k^c F_k$，其中$w_k^c$也就是第 k channel 对类别 c 分类的重要度。</p>
<p>然后，论文定义class activation map ($M_c$) 为：</p>
<p>$$M_c(x, y) = \sum_k w_k^c f_k(x, y)$$</p>
<p>CAM这种Gradient Free算法的定义就是上面公示了。</p>
<p>作者分析了为啥不使用Global Max Pooling (GMP)，而是使用GAP。简单来说，GAP会让模型学习Object边界内的所有点的信息，而GMP则可以让模型只关注Object内最有区分性的空间位置即可，有很大可能 Object 内的其它位置对结果没有影响，毕竟对Max Pooling的结果没有影响。</p>
<p>CAM的缺点就是要求模型输出层之前需要使用 GAP，如果模型默认不是这一计算，则还需要替换成 GAP 重新进行训练。实现代码示例如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># 获取全连接层的权重</span>
<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_fc_weights <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>_modules<span style="color:#000;font-weight:bold">.</span>get(fc_layer)<span style="color:#000;font-weight:bold">.</span>weight<span style="color:#000;font-weight:bold">.</span>data
<span style="color:#998;font-style:italic"># 获取目标类别的权重作为特征权重</span>
weights<span style="color:#000;font-weight:bold">=</span><span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_fc_weights[class_idx, :]
<span style="color:#998;font-style:italic"># 这里self.hook_a为最后一层特征图的输出</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> (weights<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>)<span style="color:#000;font-weight:bold">.</span>unsqueeze(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> 
                <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>hook_a<span style="color:#000;font-weight:bold">.</span>squeeze(<span style="color:#099">0</span>))<span style="color:#000;font-weight:bold">.</span>sum(dim<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
<span style="color:#998;font-style:italic"># relu操作,去除负值</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> F<span style="color:#000;font-weight:bold">.</span>relu(batch_cams, inplace<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>)
<span style="color:#998;font-style:italic"># 归一化操作</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_normalize(batch_cams)
</code></pre></td></tr></table>
</div>
</div><h3 id="grad-cam">Grad CAM</h3>
<p>此算法可以克服上面CAM算法中要求模型必须包含GAP的存在才行。基本思路是用梯度来计算Channel加权的权重，而且用ReLU去除权重为负数的那些channel。Grad CAM支持对任意一层CNN的 Feature Map 进行CAM可视化。</p>
<p>首先计算类别 c 的得分(score, before the softmax)对CNN层$A^k$的梯度，也就是$\frac{\partial y^c}{\partial A^k}$，然后这些题都在 $A^k$ 的空间维度上进行求平均，也就是 GAP（作者发现 GAP 好于 GMP），然后作为权重对$A^k$的channel进行加权，也就是</p>
<p>$$\alpha_k^c = \overbrace{\frac{1}{Z}\sum_i \sum_j}^\text{global average pooling} \underbrace{\frac{\partial y^c}{\partial A_{ij}^k}}_\text{gradients via backprop}$$</p>
<p>实验表明，越是浅层的CNN，Grad CAM的效果越差，因为这些层的感受野也越小。后就是ReLU的使用保留那些只起正向作用的空间点：</p>
<p>$$L_{\text{Grad-CAM}}^c = ReLU(\sum_k \alpha_k^c A^k)$$</p>
<p>注意，$y^c$在这里不一定必须是分类模型的class score，可以是任何可微分的激活函数的输出。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam2.png" alt="图 - 3 Grad-CAM实现示意图">
    <figcaption>图 - 3 Grad-CAM实现示意图</figcaption>
    </center>
</figure></p>
<p>示意图可以看出，Grad CAM支持多种模型的输出层结构以及对应的任务。为了实现更高分辨率的可视化，作者提出首先将$L_{\text{Grad-CAM}}$使用双线性插值进行上采样，然后与 Guided Backpropagation 的结果进行按元素乘的结果作为可视化结果。</p>
<p>另外一个方面，论文中提到使用Softmax之前的输入作为 $y^c$，但是某些代码中也使用了 Softmax 的输出作为 $y^c$，知乎文章里有给出为啥可能采用Softmax之前（论文中的做法）会效果更好。实现代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#998;font-style:italic"># 利用onehot的形式锁定目标类别</span>
one_hot <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>zeros((<span style="color:#099">1</span>, output<span style="color:#000;font-weight:bold">.</span>size()[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]), dtype<span style="color:#000;font-weight:bold">=</span>np<span style="color:#000;font-weight:bold">.</span>float32)
one_hot[<span style="color:#099">0</span>][index] <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>
one_hot <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>from_numpy(one_hot)<span style="color:#000;font-weight:bold">.</span>requires_grad_(<span style="color:#999">True</span>) 
<span style="color:#998;font-style:italic"># 获取目标类别的输出,该值带有梯度链接关系,可进行求导操作</span>
one_hot <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>sum(one_hot <span style="color:#000;font-weight:bold">*</span> output)
<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>model<span style="color:#000;font-weight:bold">.</span>zero_grad()
one_hot<span style="color:#000;font-weight:bold">.</span>backward(retain_graph<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>) <span style="color:#998;font-style:italic"># backward 求导</span>
<span style="color:#998;font-style:italic"># 获取对应特征层的梯度map</span>
grads_val <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>extractor<span style="color:#000;font-weight:bold">.</span>get_gradients()[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]<span style="color:#000;font-weight:bold">.</span>cpu()<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>numpy()
target <span style="color:#000;font-weight:bold">=</span> features[<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]<span style="color:#000;font-weight:bold">.</span>cpu()<span style="color:#000;font-weight:bold">.</span>data<span style="color:#000;font-weight:bold">.</span>numpy()[<span style="color:#099">0</span>, :] <span style="color:#998;font-style:italic"># 获取目标特征输出</span>
weights <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>mean(grads_val, axis<span style="color:#000;font-weight:bold">=</span>(<span style="color:#099">2</span>, <span style="color:#099">3</span>))[<span style="color:#099">0</span>, :] <span style="color:#998;font-style:italic"># 利用GAP操作, 获取特征权重</span>
cam <span style="color:#000;font-weight:bold">=</span> weights<span style="color:#000;font-weight:bold">.</span>dot(target<span style="color:#000;font-weight:bold">.</span>reshape((nc, h <span style="color:#000;font-weight:bold">*</span> w)))
<span style="color:#998;font-style:italic"># relu操作,去除负值, 并缩放到原图尺寸</span>
cam <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>maximum(cam, <span style="color:#099">0</span>)
cam <span style="color:#000;font-weight:bold">=</span> cv2<span style="color:#000;font-weight:bold">.</span>resize(cam, <span style="color:#0086b3">input</span><span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">2</span>:])
<span style="color:#998;font-style:italic"># 归一化操作</span>
batch_cams <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_normalize(batch_cams)
</code></pre></td></tr></table>
</div>
</div><h3 id="grad-cam-1">Grad CAM++</h3>
<p>Grad CAM++ 是对 Grad CAM 的进一步改进，优势在于定位更加精准，也更适用于图像中包含不止一个目标类别物体的情况。</p>
<p>具体来说，Grad CAM认为 Feature Map上每一个点的重要度是一样的（使用GAP得到），而Grad CAM++认为每个位置上点的贡献度不同，因此额外增加了一个权重用来表示Feature Map上每个元素的重要度。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/cam3.png" alt="图 - 4 Grad-CAM&#43;&#43;实现示意图">
    <figcaption>图 - 4 Grad-CAM&#43;&#43;实现示意图</figcaption>
    </center>
</figure></p>
<p>这里的重点也是权重$\alpha_{ij}^{kc}$的计算。</p>
<p>$$\alpha_{ij}^{kc} = \frac{(\frac{\partial S^c}{\partial A_{ij}^k})^2}{2(\frac{\partial S^c}{\partial A^k_{ij}})^2 + \sum_a \sum_b A_{ab}^k (\frac{\partial S^c}{\partial A^k_{ij}})^3}$$</p>
<p>知乎参考博客里还有其他一些比较新的Feature Map的可视化方法，可以去参考一下，这里略过。</p>
<h2 id="t-sne">t-SNE</h2>
<p>主要的参考是 <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding</a>。</p>
<p>这个算法其实适用于展示高维数据的，也就是将高维数据降维到低维数据，然后方便查看两个数据点之间的距离。与其他降维算法（如PCA）相比，t-SNE创建一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。最后一句话可以通过下面的例子进行说明。</p>
<p>整体来说，t-SNE是将高维数据映射到低维空间，所以就涉及到几个问题：这个映射过程怎么确定，等价于低维空间的点应该怎么选择；然后就是维度灾难，也就是原始高维空间的数据计算距离的时候会受维度灾难的影响。</p>
<p>t-SNE其实更是为了更好的可视化相似性，而不是为了降维。整体思路也是保证映射前后，也就是高维空间、低维空间中，相似的点继续保持相似，也就是距离比较近。采取的方法是对两个空间中的点的距离进行概率建模，也就是一个点与其它所有点的距离映射成一个概率分布。首先是高维空间，将欧氏距离映射成高斯分布。</p>
<p>N个高维空间点，$x_1, \ldots, x_N$，计算概率$p_{ij}$为与点$x_i, x_j$之间的欧式距离成正比。</p>
<p>$$p_{j|i} = \frac{\exp{(-\parallel x_i - x_j\parallel^2} / 2\sigma_i^2)}{\sum_{k\neq i}\exp(-\parallel x_i - x_k \parallel^2 / 2\sigma_i^2)}$$</p>
<p>其中，$j \neq i$，并且有$p_{i|i}=0$以及$\sum_j p_{j|i}=1, \forall i$。这个式子是指以$x_i$为中心点的概率分布中，点$x_j$的概率。由于距离是对称的，所以定义：</p>
<p>$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}$$</p>
<p>此时也就有$p_{ij}=p_{ji}, p_{ii}=0, \sum_{i,j}p_{ij}=1$了。</p>
<p>其中参数$\sigma_i$是由高斯分布的perplexity(困惑度)与二分法（bisection method）计算得到的困惑度相等来得到的。就体现在，越密集的数据空间中，$\sigma_i$越小，也就是低困惑度更关注局部数据点，高困惑度更关注全局结构。</p>
<p>这里需要注意的地方在于，$x_i$在高维空间中会面临维度灾难，此时欧氏距离没有区分性，也就是任何两个点之间看上去都不是那么相似了，毕竟高维空间里这些点还是非常稀疏的，也就导致计算的概率分布$p_{ij}$也区分性不大，针对这个问题，有一些方式是基于 <a href="https://en.wikipedia.org/wiki/Intrinsic_dimension">intrinsic dimension</a> 的 <a href="https://en.wikipedia.org/wiki/Power_transform">power transform</a> 来缓解这个问题。</p>
<p>回到上文，t-SNE的目标也是获得对应的到d维空间的映射，$y_1, \ldots, y_N, y \in \mathbb{R}^d$，同时可以反映出高维空间的概率分布$p_{ij}$。具体做法是，计算低维空间里点的概率分布，用$q_{ij}$表示。</p>
<p>$$q_{ij} = \frac{(1 + \parallel y_i - y_j \parallel^2)a^{-1}}{\sum_k \sum_{l\neq k}(1 + \parallel y_k - y_l \parallel^2)^{-1}}$$</p>
<p>这里也有$q_{ii}=0$。而且不再使用高斯分布计算概率了，而是选择student t-distribution来计算概率分布，好处是这个分布相比高斯分布，是胖尾的，也就可以将不相似的两个点用更远的低维映射点进行拟合。</p>
<p>优化映射过程是基于KL散度这个损失函数来实现的，高维、低维空间的分布分别用$P, Q$表示。</p>
<p>$$\mathrm{KL}(P\parallel Q) = \sum_{i\neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$</p>
<p>通过优化上述损失函数，利用梯度下降法来更新$y_i$的取值。优化的结果也就可以反映高维空间中点之间的相似度了。</p>
<p>下面是一个用在多模态预训练中文本 embedding 矩阵使用 t-SNE 可视化的结果，直观来看分布是比较均匀的。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/txt_embedding_0.png" alt="图 - 5 embedding 矩阵的t-SNE可视化">
    <figcaption>图 - 5 embedding 矩阵的t-SNE可视化</figcaption>
    </center>
</figure></p>
<p>对应PCA降维之后可视化的结果如下，就不是这么均匀了。</p>
<p><figure>
    <center>
    <img src="/imgs/model-visualization/txt_embedding_1.png" alt="图 - 6 embedding 矩阵的PCA可视化">
    <figcaption>图 - 6 embedding 矩阵的PCA可视化</figcaption>
    </center>
</figure></p>
        
            <p>Attention is all your need.</p>
<h2 id="基础">基础</h2>
<h3 id="attention">Attention</h3>
<p>Attention 定义上是一个映射函数，输入<code>Q,K,V</code>等向量，输出是一个新的向量。具体定义如下：</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.</p>
</blockquote>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>这里 <code>Dot-Product</code> 是一种计算Attention权重的方式（另一种常见的方式是 Additive Attention）。输入Q与K都是向量，维度为 $d_k$，输入 V 也是向量，维度为 $d_v$，然后权重计算过程是将 Q 与所有的 K 计算向量点乘（Dot-Product）。</p>
<p>所谓的 <code>Scaled</code> 体现在将上述点乘结果除以 $\sqrt{d_k}$。为什么是除以这个数？主要原因是，两个 $d_k$ 维的矩阵乘（矩阵元素是mean 0, variance 1 生成的随机数），结果矩阵的方差就是$d_k$。所以，如果这里不进行 Scale，那么得到的矩阵数值就会越来越大，导致后面的 Softmax 饱和。</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer0.png" alt="图-1 Scaled Dot-Product Attention示意图">
    <figcaption>图-1 Scaled Dot-Product Attention示意图</figcaption>
    </center>
</figure></p>
<p>补充一下 Additive Attention。具体实现是通过全连接映射然后按元素加得到，公式如下。这里为什么选择 Dot-Product 而不是 Additive Attention 呢？而且两者的理论计算复杂度差不多。论文里也给出了解释，就是Dot-Product在实际计算中其实是更快的，毕竟矩阵乘法被研究、优化的更多。</p>
<p>$$a(q, k) = w_v^T \tanh (W_q q + W_k k) \in \mathbb{R}$$</p>
<h3 id="multi-head">Multi-head</h3>
<p>作者发现，用不同的Linear Projection 来将 Q, K, V 进行映，然后对应的计算 Attention，最终将结果拼接起来的效果比使用一个单独的 Attention 效果更好。下面的公式与图2就可以很好的说明计算过程了，实际实现可以通过先合并 Linear Projection 的权重，然后在经过 Reshape 完成。</p>
<p>$$\begin{gather*}
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(head_1, \dots, head_n)W^O  \<br>
where, head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{gather*}$$</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer1.png" alt="图-2 Multi-Head Attention示意图">
    <figcaption>图-2 Multi-Head Attention示意图</figcaption>
    </center>
</figure></p>
<h3 id="position-wise-ffn">Position-wise FFN</h3>
<p>实现上来看是两层全连接，并且第一层一般会将输入Tensor的channel个数扩展expansion(=4)倍，然后第二层全连接在恢复原来的 channel 个数。</p>
<p>为什么叫 Position-wise 呢？按照论文的说法，我猜这里的Position是指 Depth 维度上的位置，体现在相同层的不同位置的 Token 公用相同的 Lienar Projection 权重矩阵，但是不同层上使用不同的 Linear Projection。</p>
<blockquote>
<p>While the linear transformations are the same across different positions, they use different parameters from layer to layer</p>
</blockquote>
<p>不过 D2L 中李沐的说法是：</p>
<blockquote>
<p>The positionwise feed-forward network transforms the representation at all the sequence positions using the same MLP. This is why we call it positionwise.</p>
</blockquote>
<h2 id="encoder-decoder结构">Encoder-Decoder结构</h2>
<p>一个方面是如何将 Encoder 的信息传递给 Decoder，有两种做法，一种是指在Decoder的第一个输入位置上使用，另一种是在Decoder的每一次输入上都使用。</p>
<h3 id="seq2seq">Seq2Seq</h3>
<p>这里参考<a href="https://d2l.ai/chapter_recurrent-modern/seq2seq.html">Sequence to sequence leanring - d2l</a>中的讲解。</p>
<p>具体的 Encoder - Decoder 部分这里基于 GRU 来实现。输入尺寸为：$(batch_size, num_steps, embed_size)$；GRU 的计算包含两个输出，一个是GRU 最后输出结果output，尺寸仍然是: $(num_steps, batch_size, embed_size)$，相当于每一步（共num_steps，可认为是 num_steps 个 Toke）都输出了一个新的 embed_size 大小的向量；另一个输出是隐空间变量 states，尺寸是 $(num_layers, batch_size, num_hiddens)$，相当于是当前输入Token与上一个Token对应的隐变量共同作用生成了当前Token对应的隐变量，这个隐变量就包含了前面所有 Token 的信息。 当前 token 的 output 与隐变量 state 之间的关系是：<code>output = Mlp(state)</code>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">rnn</span>(inputs, state, params):
    <span style="color:#998;font-style:italic"># Shape of `inputs`: (`num_steps`, `batch_size`, `vocab_size`)</span>
    W_xh, W_hh, b_h, W_hq, b_q <span style="color:#000;font-weight:bold">=</span> params
    H, <span style="color:#000;font-weight:bold">=</span> state
    outputs <span style="color:#000;font-weight:bold">=</span> []
    <span style="color:#998;font-style:italic"># Shape of `X`: (`batch_size`, `vocab_size`)</span>
    <span style="color:#000;font-weight:bold">for</span> X <span style="color:#000;font-weight:bold">in</span> inputs:
        H <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>tanh(np<span style="color:#000;font-weight:bold">.</span>dot(X, W_xh) <span style="color:#000;font-weight:bold">+</span> np<span style="color:#000;font-weight:bold">.</span>dot(H, W_hh) <span style="color:#000;font-weight:bold">+</span> b_h)
        Y <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>dot(H, W_hq) <span style="color:#000;font-weight:bold">+</span> b_q
        outputs<span style="color:#000;font-weight:bold">.</span>append(Y)
    <span style="color:#000;font-weight:bold">return</span> np<span style="color:#000;font-weight:bold">.</span>concatenate(outputs, axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>), (H,)
</code></pre></td></tr></table>
</div>
</div><p>上面说完 Encoder 部分，Decoder 部分结构相同，但重要的是 Decoder 部分的输出应该怎么决定。</p>
<p>输入主要包含两个部分，首先是起始Token，这里起始 Token 是一个特殊字符，<code>&lt;bos&gt;</code>；另一个部分就是隐变量的确定，这里使用 Encoder 输出的隐变量作为初始隐变量，注意这里 Encoder - Decoder 需要使用相同的层数，这样隐变量的尺寸才匹配，即：$(num_layers, batch size, embed_size)$。下面给出的示例代码中，还会将 Encoder 输出的最后一层的隐变量与输入 X 拼接起来进行计算。Decoder 的输出就是$(batch size, num steps, vocab size)$。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">Seq2SeqDecoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>Decoder):
    <span style="color:#d14">&#34;&#34;&#34;The RNN decoder for sequence to sequence learning.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(Seq2SeqDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, embed_size)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>rnn <span style="color:#000;font-weight:bold">=</span> rnn<span style="color:#000;font-weight:bold">.</span>GRU(num_hiddens, num_layers, dropout<span style="color:#000;font-weight:bold">=</span>dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dense(vocab_size, flatten<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">init_state</span>(<span style="color:#999">self</span>, enc_outputs, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#000;font-weight:bold">return</span> enc_outputs[<span style="color:#099">1</span>]

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        <span style="color:#998;font-style:italic"># The output `X` shape: (`num_steps`, `batch_size`, `embed_size`)</span>
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X)<span style="color:#000;font-weight:bold">.</span>swapaxes(<span style="color:#099">0</span>, <span style="color:#099">1</span>)
        <span style="color:#998;font-style:italic"># `context` shape: (`batch_size`, `num_hiddens`)</span>
        context <span style="color:#000;font-weight:bold">=</span> state[<span style="color:#099">0</span>][<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]      <span style="color:#998;font-style:italic"># 最后一层对应的隐变量</span>
        <span style="color:#998;font-style:italic"># Broadcast `context` so it has the same `num_steps` as `X`</span>
        context <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>broadcast_to(
            context, (X<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>], context<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>], context<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">1</span>]))
        X_and_context <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate((X, context), <span style="color:#099">2</span>)
        output, state <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>rnn(X_and_context, state)
        output <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense(output)<span style="color:#000;font-weight:bold">.</span>swapaxes(<span style="color:#099">0</span>, <span style="color:#099">1</span>)
        <span style="color:#998;font-style:italic"># `output` shape: (`batch_size`, `num_steps`, `vocab_size`)</span>
        <span style="color:#998;font-style:italic"># `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)</span>
        <span style="color:#000;font-weight:bold">return</span> output, state
</code></pre></td></tr></table>
</div>
</div><h3 id="transformer-中的实现">Transformer 中的实现</h3>
<p>Encoder 部分简单的是 Transformer 层的堆叠。Transformer 层包含两个sublayer，分别是 MultiHead Self Attention 以及 Positionwise FFN，这两个 sublayer 都会通过残差连接并紧跟着计算一个 LayerNorm （这里不讨论pre-norm的实现）。</p>
<p>Decoder 部分相比于 Encoder 的两个 sublayer 构成，多了一个 cross-attention 的层。cross-attention的主要区别在于输入的 K, V 来自于对应的 Encoder 层，Query 来自于 Decoder 中上一层的 MultiHead Self Attention的输出。整体结构如下。</p>
<p><figure>
    <center>
    <img src="/imgs/origin-transformer/transformer2.png" alt="图-3 Transformer Encoder-Decoder 示意图">
    <figcaption>图-3 Transformer Encoder-Decoder 示意图</figcaption>
    </center>
</figure></p>
<p>下面给出了MXNet实现代码，非常详细，但是解答了下面几个疑问。</p>
<ul>
<li>Decoder 最开始的输入是<code>&lt;bos&gt;</code>，在训练时，这个也是拼接在最前面的</li>
<li>Decoder 中每一层中 Cross Attention 的 K, V 都是相同的，都来自于 Encoder 的最后输出</li>
<li>在最大 num_steps 限制下，最后一个元素是 <code>&lt;eos&gt;</code> 时则退出 Decoder 部分</li>
</ul>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">  9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 46
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 47
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 48
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 49
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 50
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 51
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 52
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 53
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 54
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 55
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 56
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 57
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 58
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 59
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 60
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 61
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 62
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 63
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 64
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 65
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 66
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 67
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 68
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 69
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 70
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 71
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 72
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 73
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 74
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 75
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 76
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 77
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 78
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 79
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 80
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 81
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 82
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 83
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 84
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 85
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 86
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 87
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 88
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 89
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 90
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 91
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 92
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 93
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 94
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 95
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 96
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 97
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 98
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 99
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">100
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">101
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">102
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">103
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">104
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">105
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">106
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">107
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">108
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">109
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">110
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">111
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">112
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">113
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">114
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">115
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">116
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">117
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">118
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">119
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">120
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">121
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">122
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">123
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">124
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">125
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">126
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">127
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">128
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">129
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">130
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">131
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">132
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">133
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">134
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">135
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">136
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">137
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">138
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">139
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">140
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">141
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">142
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">143
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">144
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">145
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">146
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">147
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">148
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">149
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">150
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">151
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">152
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">153
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">154
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">155
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">156
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">157
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">158
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">159
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">160
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">161
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">162
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">163
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">164
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">165
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">166
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">167
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">168
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">169
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">170
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">171
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">172
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">173
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">174
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">175
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">176
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">177
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">178
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">179
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">180
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">181
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">182
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">183
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">184
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">185
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">186
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">187
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">188
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">189
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">190
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">191
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">192
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">193
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">194
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">195
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">196
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">197
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">198
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">199
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">200
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">201
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">202
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">203
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">204
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">205
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">206
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">207
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">208
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">209
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">210
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">211
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">212
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">213
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">214
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">215
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">216
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">EncoderDecoder</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#d14">&#34;&#34;&#34;The base class for the encoder-decoder architecture.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, encoder, decoder, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(EncoderDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>encoder <span style="color:#000;font-weight:bold">=</span> encoder
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder <span style="color:#000;font-weight:bold">=</span> decoder

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, enc_X, dec_X, <span style="color:#000;font-weight:bold">*</span>args):
        enc_outputs <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>encoder(enc_X, <span style="color:#000;font-weight:bold">*</span>args)
        dec_state <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>init_state(enc_outputs, <span style="color:#000;font-weight:bold">*</span>args)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>decoder(dec_X, dec_state)

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">EncoderBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#d14">&#34;&#34;&#34;Transformer encoder block.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, num_hiddens, ffn_num_hiddens, num_heads, dropout,
                 use_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(EncoderBlock, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                dropout, use_bias)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn <span style="color:#000;font-weight:bold">=</span> PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, valid_lens):
        Y <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1(X, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention(X, X, X, valid_lens))
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2(Y, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn(Y))

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">TransformerEncoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>Encoder):
    <span style="color:#d14">&#34;&#34;&#34;Transformer encoder.&#34;&#34;&#34;</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, use_bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(TransformerEncoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens <span style="color:#000;font-weight:bold">=</span> num_hiddens
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>PositionalEncoding(num_hiddens, dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential()
        <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_layers):
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks<span style="color:#000;font-weight:bold">.</span>add(
                EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
                             use_bias))

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, valid_lens, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#998;font-style:italic"># Since positional encoding values are between -1 and 1, the embedding</span>
        <span style="color:#998;font-style:italic"># values are multiplied by the square root of the embedding dimension</span>
        <span style="color:#998;font-style:italic"># to rescale before they are summed up</span>
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X) <span style="color:#000;font-weight:bold">*</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention_weights <span style="color:#000;font-weight:bold">=</span> [<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#0086b3">len</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks)
        <span style="color:#000;font-weight:bold">for</span> i, blk <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks):
            X <span style="color:#000;font-weight:bold">=</span> blk(X, valid_lens)
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention_weights[
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
        <span style="color:#000;font-weight:bold">return</span> X

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">DecoderBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Block):
    <span style="color:#998;font-style:italic"># The `i`-th block in the decoder</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, num_hiddens, ffn_num_hiddens, num_heads, dropout, i,
                 <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(DecoderBlock, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i <span style="color:#000;font-weight:bold">=</span> i
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention1 <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention2 <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>MultiHeadAttention(num_hiddens, num_heads,
                                                 dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn <span style="color:#000;font-weight:bold">=</span> PositionWiseFFN(ffn_num_hiddens, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm3 <span style="color:#000;font-weight:bold">=</span> AddNorm(dropout)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        enc_outputs, enc_valid_lens <span style="color:#000;font-weight:bold">=</span> state[<span style="color:#099">0</span>], state[<span style="color:#099">1</span>]
        <span style="color:#998;font-style:italic"># During training, all the tokens of any output sequence are processed</span>
        <span style="color:#998;font-style:italic"># at the same time, so `state[2][self.i]` is `None` as initialized.</span>
        <span style="color:#998;font-style:italic"># When decoding any output sequence token by token during prediction,</span>
        <span style="color:#998;font-style:italic"># `state[2][self.i]` contains representations of the decoded output at</span>
        <span style="color:#998;font-style:italic"># the `i`-th block up to the current time step</span>
        <span style="color:#000;font-weight:bold">if</span> state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i] <span style="color:#000;font-weight:bold">is</span> <span style="color:#999">None</span>:
            key_values <span style="color:#000;font-weight:bold">=</span> X
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#998;font-style:italic"># 这里是只使用已预测的Token进行计算</span>
            key_values <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>concatenate((state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i], X), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>)     
        state[<span style="color:#099">2</span>][<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>i] <span style="color:#000;font-weight:bold">=</span> key_values

        <span style="color:#000;font-weight:bold">if</span> autograd<span style="color:#000;font-weight:bold">.</span>is_training():
            batch_size, num_steps, _ <span style="color:#000;font-weight:bold">=</span> X<span style="color:#000;font-weight:bold">.</span>shape
            <span style="color:#998;font-style:italic"># Shape of `dec_valid_lens`: (`batch_size`, `num_steps`), where</span>
            <span style="color:#998;font-style:italic"># every row is [1, 2, ..., `num_steps`]</span>
            dec_valid_lens <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>tile(np<span style="color:#000;font-weight:bold">.</span>arange(<span style="color:#099">1</span>, num_steps <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>, ctx<span style="color:#000;font-weight:bold">=</span>X<span style="color:#000;font-weight:bold">.</span>ctx),
                                     (batch_size, <span style="color:#099">1</span>))
        <span style="color:#000;font-weight:bold">else</span>:
            dec_valid_lens <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>

        <span style="color:#998;font-style:italic"># Self-attention</span>
        X2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention1(X, key_values, key_values, dec_valid_lens)
        Y <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm1(X, X2)
        <span style="color:#998;font-style:italic"># Encoder-decoder attention. Shape of `enc_outputs`:</span>
        <span style="color:#998;font-style:italic"># (`batch_size`, `num_steps`, `num_hiddens`)</span>
        <span style="color:#998;font-style:italic">## 这里使用 Encoder 最后的输出的 enc_outputs 当作 K, V 进行计算！！</span>
        Y2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm2(Y, Y2)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>addnorm3(Z, <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>ffn(Z)), state

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">TransformerDecoder</span>(d2l<span style="color:#000;font-weight:bold">.</span>AttentionDecoder):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,
                 num_layers, dropout, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>(TransformerDecoder, <span style="color:#999">self</span>)<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens <span style="color:#000;font-weight:bold">=</span> num_hiddens
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_layers <span style="color:#000;font-weight:bold">=</span> num_layers
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Embedding(vocab_size, num_hiddens)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>PositionalEncoding(num_hiddens, dropout)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential()
        <span style="color:#000;font-weight:bold">for</span> i <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_layers):
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks<span style="color:#000;font-weight:bold">.</span>add(
                DecoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout,
                             i))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Dense(vocab_size, flatten<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">init_state</span>(<span style="color:#999">self</span>, enc_outputs, enc_valid_lens, <span style="color:#000;font-weight:bold">*</span>args):
        <span style="color:#000;font-weight:bold">return</span> [enc_outputs, enc_valid_lens, [<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_layers]

    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, X, state):
        X <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>pos_encoding(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>embedding(X) <span style="color:#000;font-weight:bold">*</span> math<span style="color:#000;font-weight:bold">.</span>sqrt(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>num_hiddens))
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights <span style="color:#000;font-weight:bold">=</span> [[<span style="color:#999">None</span>] <span style="color:#000;font-weight:bold">*</span> <span style="color:#0086b3">len</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks) <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(<span style="color:#099">2</span>)]
        <span style="color:#000;font-weight:bold">for</span> i, blk <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">enumerate</span>(<span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>blks):
            X, state <span style="color:#000;font-weight:bold">=</span> blk(X, state)
            <span style="color:#998;font-style:italic"># Decoder self-attention weights</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights[<span style="color:#099">0</span>][
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention1<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
            <span style="color:#998;font-style:italic"># Encoder-decoder attention weights</span>
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights[<span style="color:#099">1</span>][
                i] <span style="color:#000;font-weight:bold">=</span> blk<span style="color:#000;font-weight:bold">.</span>attention2<span style="color:#000;font-weight:bold">.</span>attention<span style="color:#000;font-weight:bold">.</span>attention_weights
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>dense(X), state

    <span style="color:#3c5d5d;font-weight:bold">@property</span>
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">attention_weights</span>(<span style="color:#999">self</span>):
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>_attention_weights

num_hiddens, num_layers, dropout, batch_size, num_steps <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">32</span>, <span style="color:#099">2</span>, <span style="color:#099">0.1</span>, <span style="color:#099">64</span>, <span style="color:#099">10</span>
lr, num_epochs, device <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0.005</span>, <span style="color:#099">200</span>, d2l<span style="color:#000;font-weight:bold">.</span>try_gpu()
ffn_num_hiddens, num_heads <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">64</span>, <span style="color:#099">4</span>

train_iter, src_vocab, tgt_vocab <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>load_data_nmt(batch_size, num_steps)

encoder <span style="color:#000;font-weight:bold">=</span> TransformerEncoder(<span style="color:#0086b3">len</span>(src_vocab), num_hiddens, ffn_num_hiddens,
                             num_heads, num_layers, dropout)
decoder <span style="color:#000;font-weight:bold">=</span> TransformerDecoder(<span style="color:#0086b3">len</span>(tgt_vocab), num_hiddens, ffn_num_hiddens,
                             num_heads, num_layers, dropout)
net <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>EncoderDecoder(encoder, decoder)

d2l<span style="color:#000;font-weight:bold">.</span>train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
engs <span style="color:#000;font-weight:bold">=</span> [<span style="color:#d14">&#39;go .&#39;</span>, <span style="color:#d14">&#34;i lost .&#34;</span>, <span style="color:#d14">&#39;he</span><span style="color:#d14">\&#39;</span><span style="color:#d14">s calm .&#39;</span>, <span style="color:#d14">&#39;i</span><span style="color:#d14">\&#39;</span><span style="color:#d14">m home .&#39;</span>]
fras <span style="color:#000;font-weight:bold">=</span> [<span style="color:#d14">&#39;va !&#39;</span>, <span style="color:#d14">&#39;j</span><span style="color:#d14">\&#39;</span><span style="color:#d14">ai perdu .&#39;</span>, <span style="color:#d14">&#39;il est calme .&#39;</span>, <span style="color:#d14">&#39;je suis chez moi .&#39;</span>]
<span style="color:#000;font-weight:bold">for</span> eng, fra <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">zip</span>(engs, fras):
    translation, dec_attention_weight_seq <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, <span style="color:#999">True</span>)
    <span style="color:#000;font-weight:bold">print</span>(f<span style="color:#d14">&#39;{eng} =&gt; {translation}, &#39;</span>,
          f<span style="color:#d14">&#39;bleu {d2l.bleu(translation, fra, k=2):.3f}&#39;</span>)

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">predict_seq2seq</span>(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>):
    <span style="color:#d14">&#34;&#34;&#34;Predict for sequence to sequence.&#34;&#34;&#34;</span>
    src_tokens <span style="color:#000;font-weight:bold">=</span> src_vocab[src_sentence<span style="color:#000;font-weight:bold">.</span>lower()<span style="color:#000;font-weight:bold">.</span>split(<span style="color:#d14">&#39; &#39;</span>)] <span style="color:#000;font-weight:bold">+</span> [
        src_vocab[<span style="color:#d14">&#39;&lt;eos&gt;&#39;</span>]]
    enc_valid_len <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([<span style="color:#0086b3">len</span>(src_tokens)], ctx<span style="color:#000;font-weight:bold">=</span>device)
    src_tokens <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>truncate_pad(src_tokens, num_steps, src_vocab[<span style="color:#d14">&#39;&lt;pad&gt;&#39;</span>])
    <span style="color:#998;font-style:italic"># Add the batch axis</span>
    enc_X <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>expand_dims(np<span style="color:#000;font-weight:bold">.</span>array(src_tokens, ctx<span style="color:#000;font-weight:bold">=</span>device), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    enc_outputs <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>encoder(enc_X, enc_valid_len)
    dec_state <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>init_state(enc_outputs, enc_valid_len)
    <span style="color:#998;font-style:italic"># Add the batch axis</span>
    <span style="color:#998;font-style:italic">## 最开始的是 &#39;&lt;bos&gt;&#39;</span>
    dec_X <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>expand_dims(np<span style="color:#000;font-weight:bold">.</span>array([tgt_vocab[<span style="color:#d14">&#39;&lt;bos&gt;&#39;</span>]], ctx<span style="color:#000;font-weight:bold">=</span>device), axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)
    output_seq, attention_weight_seq <span style="color:#000;font-weight:bold">=</span> [], []
    <span style="color:#000;font-weight:bold">for</span> _ <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_steps):
        Y, dec_state <span style="color:#000;font-weight:bold">=</span> net<span style="color:#000;font-weight:bold">.</span>decoder(dec_X, dec_state)
        <span style="color:#998;font-style:italic"># We use the token with the highest prediction likelihood as the input</span>
        <span style="color:#998;font-style:italic"># of the decoder at the next time step</span>
        dec_X <span style="color:#000;font-weight:bold">=</span> Y<span style="color:#000;font-weight:bold">.</span>argmax(axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">2</span>)
        pred <span style="color:#000;font-weight:bold">=</span> dec_X<span style="color:#000;font-weight:bold">.</span>squeeze(axis<span style="color:#000;font-weight:bold">=</span><span style="color:#099">0</span>)<span style="color:#000;font-weight:bold">.</span>astype(<span style="color:#d14">&#39;int32&#39;</span>)<span style="color:#000;font-weight:bold">.</span>item()
        <span style="color:#998;font-style:italic"># Save attention weights (to be covered later)</span>
        <span style="color:#000;font-weight:bold">if</span> save_attention_weights:
            attention_weight_seq<span style="color:#000;font-weight:bold">.</span>append(net<span style="color:#000;font-weight:bold">.</span>decoder<span style="color:#000;font-weight:bold">.</span>attention_weights)
        <span style="color:#998;font-style:italic"># Once the end-of-sequence token is predicted, the generation of the</span>
        <span style="color:#998;font-style:italic"># output sequence is complete</span>
        <span style="color:#000;font-weight:bold">if</span> pred <span style="color:#000;font-weight:bold">==</span> tgt_vocab[<span style="color:#d14">&#39;&lt;eos&gt;&#39;</span>]:
            <span style="color:#000;font-weight:bold">break</span>
        output_seq<span style="color:#000;font-weight:bold">.</span>append(pred)
    <span style="color:#000;font-weight:bold">return</span> <span style="color:#d14">&#39; &#39;</span><span style="color:#000;font-weight:bold">.</span>join(tgt_vocab<span style="color:#000;font-weight:bold">.</span>to_tokens(output_seq)), attention_weight_seq

<span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">train_seq2seq</span>(net, data_iter, lr, num_epochs, tgt_vocab, device):
    <span style="color:#d14">&#34;&#34;&#34;Train a model for sequence to sequence.&#34;&#34;&#34;</span>
    net<span style="color:#000;font-weight:bold">.</span>initialize(init<span style="color:#000;font-weight:bold">.</span>Xavier(), force_reinit<span style="color:#000;font-weight:bold">=</span><span style="color:#999">True</span>, ctx<span style="color:#000;font-weight:bold">=</span>device)
    trainer <span style="color:#000;font-weight:bold">=</span> gluon<span style="color:#000;font-weight:bold">.</span>Trainer(net<span style="color:#000;font-weight:bold">.</span>collect_params(), <span style="color:#d14">&#39;adam&#39;</span>,
                            {<span style="color:#d14">&#39;learning_rate&#39;</span>: lr})
    loss <span style="color:#000;font-weight:bold">=</span> MaskedSoftmaxCELoss()
    animator <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Animator(xlabel<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;epoch&#39;</span>, ylabel<span style="color:#000;font-weight:bold">=</span><span style="color:#d14">&#39;loss&#39;</span>,
                            xlim<span style="color:#000;font-weight:bold">=</span>[<span style="color:#099">10</span>, num_epochs])
    <span style="color:#000;font-weight:bold">for</span> epoch <span style="color:#000;font-weight:bold">in</span> <span style="color:#0086b3">range</span>(num_epochs):
        timer <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Timer()
        metric <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>Accumulator(<span style="color:#099">2</span>)  <span style="color:#998;font-style:italic"># Sum of training loss, no. of tokens</span>
        <span style="color:#000;font-weight:bold">for</span> batch <span style="color:#000;font-weight:bold">in</span> data_iter:
            X, X_valid_len, Y, Y_valid_len <span style="color:#000;font-weight:bold">=</span> [
                x<span style="color:#000;font-weight:bold">.</span>as_in_ctx(device) <span style="color:#000;font-weight:bold">for</span> x <span style="color:#000;font-weight:bold">in</span> batch]
            bos <span style="color:#000;font-weight:bold">=</span> np<span style="color:#000;font-weight:bold">.</span>array([tgt_vocab[<span style="color:#d14">&#39;&lt;bos&gt;&#39;</span>]] <span style="color:#000;font-weight:bold">*</span> Y<span style="color:#000;font-weight:bold">.</span>shape[<span style="color:#099">0</span>],
                           ctx<span style="color:#000;font-weight:bold">=</span>device)<span style="color:#000;font-weight:bold">.</span>reshape(<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>, <span style="color:#099">1</span>)
            dec_input <span style="color:#000;font-weight:bold">=</span> d2l<span style="color:#000;font-weight:bold">.</span>concat([bos, Y[:, :<span style="color:#000;font-weight:bold">-</span><span style="color:#099">1</span>]], <span style="color:#099">1</span>)  <span style="color:#998;font-style:italic"># Teacher forcing</span>
            <span style="color:#000;font-weight:bold">with</span> autograd<span style="color:#000;font-weight:bold">.</span>record():
                Y_hat, _ <span style="color:#000;font-weight:bold">=</span> net(X, dec_input, X_valid_len)
                l <span style="color:#000;font-weight:bold">=</span> loss(Y_hat, Y, Y_valid_len)
            l<span style="color:#000;font-weight:bold">.</span>backward()
            d2l<span style="color:#000;font-weight:bold">.</span>grad_clipping(net, <span style="color:#099">1</span>)
            num_tokens <span style="color:#000;font-weight:bold">=</span> Y_valid_len<span style="color:#000;font-weight:bold">.</span>sum()
            trainer<span style="color:#000;font-weight:bold">.</span>step(num_tokens)
            metric<span style="color:#000;font-weight:bold">.</span>add(l<span style="color:#000;font-weight:bold">.</span>sum(), num_tokens)
        <span style="color:#000;font-weight:bold">if</span> (epoch <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">10</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>:
            animator<span style="color:#000;font-weight:bold">.</span>add(epoch <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>, (metric[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">/</span> metric[<span style="color:#099">1</span>],))
    <span style="color:#000;font-weight:bold">print</span>(f<span style="color:#d14">&#39;loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} &#39;</span>

</code></pre></td></tr></table>
</div>
</div>
        
            <p>VOLO论文。</p>
<h2 id="outlook-attention">Outlook Attention</h2>
<p>关于论文里主要的 Outlook Attenion 模块的实现有下面几点理解。</p>
<ul>
<li>
<p>Unfold 计算</p>
<p>实现过程是将输入 Tensor 在划窗内的数据Flatten 成列，Flatten 过程中按照 Row-Major （第一行 -&gt; 第二行 -&gt; 第N行） 的方式完成；窗口滑动过程中得到新的列则拼接在后面。假设 unfold 操作的 kernel size 为 K，输入Tensor的尺寸为 $C \times H \times W$，则 unfold 得到的结果为 $(C \cdot K^2) \times (H \cdot W)$，最后一维表示考虑 padding 后，窗口一共滑动了 $H\cdot W$次。</p>
</li>
<li>
<p>Fold 计算</p>
<p>此计算是 Unfold 的反过程，并且将恢复过程中对应同一个位置(i, j)的$K^2$结果求和作为该位置上新的结果。</p>
</li>
<li>
<p>与 Convolution 的区别</p>
<p>卷积计算过程是将以 (i, j) 为中心的窗口内的元素进行加权求和，权重为对应的 kernel 数据；Outlook Attention 计算过程是将 (i, j) 位置的元素在不同的滑窗内计算的结果进行求和，在不同滑创内的结果是指，该窗口内的 $K \times K$ 个元素基于 $K^2 \times K^2$ 个权重得到 $K \times K$个位置上的向量，所以滑窗内每个位置上都有一个对应当前滑窗的结果，Outlook Attention 也就是将(i, j)位置上向量参与的所有的滑窗内的结果求和得到新的向量。</p>
<p>以$3 \times 3$的滑窗为例，则输入 Tensor 每个位置上的元素一共参与了 9 个滑窗，则结果就是这9个滑窗内的结果求和，而滑窗内的结果是根据该滑窗内的9个元素加权平均（权重经过 Softmax 了）得来的。</p>
</li>
<li>
<p>论文中的伪代码</p>
<p>其中比较奇怪的一个地方在于，<code>v=v_pj(x).permute(2, 1, 0)</code> 得到的尺寸是$(C, W, H)$，为什么 W / H 这两个维度还需要转置呢？</p>
<p>目前的想法是作者写错了，因为代码实现里还是 (C, H, W) 的顺序。伪代码里mul(a, v)计算相当于第 (i, j) 位置对应滑窗内的权重是由 (j, i) 位置上向量根据一个 Linear 层得到的，这肯定是不对的，毕竟图片肯定不是沿着对角线对称的，即使对称，Stem Block 计算的结果也不是。如果理解错了请告诉我。</p>
</li>
</ul>
<p>论文中给出的伪代码如下：</p>
<p><figure>
    <center>
    <img src="/imgs/volo/volo0.png" alt="图-1 Outlook Attention示例代码">
    <figcaption>图-1 Outlook Attention示例代码</figcaption>
    </center>
</figure></p>
<h2 id="引入多头注意力">引入多头注意力</h2>
<p>在这里，多头注意力简单来说就是得到 N (head num)个不同的权重分布，同时 Value 矩阵的 hidden status 维度分成 N 组，然后每组对应一个权重分布。多头注意力就是在 Value 的每个分组上求解加权平均，然后拼接起来恢复输入时候的尺寸。</p>
<p>对应 Outlook Attention 的实现，就是将得到权重的全连接层由$W^A \in R^{C \times K^4}$变为$W^A \in R^{C \times N \cdot K^{4}}$，同时将Value结果由$R^{H \times W \times C}$Reshape成$R^{H \times W \times N \times C_{in}}$即可。计算过程中，只需要将 $A$ 分成 $N$份，然后分别用于 $N$ 组 $V$ 最后将结果进行拼接完成计算。</p>
<h2 id="构建模型">构建模型</h2>
<p>模型部分主要分为三个部分：Stem, Volo Stages, Transformer Stages。</p>
<p>Stem 部分默认采用的是3层（Conv + BN + ReLU）结构，将输入数据下采样一倍，然后再经过一个 Patch Projection 层（Conv计算），继续下采样 4 倍，所以一共下采样8倍。这种 Stem 避免了 ViT 中的 $16 \times 16$这种大Kernel size / stride 的卷积计算，相关论文指出，这种大 ks / stride 的方式不利于训练稳定性。</p>
<p>中间是由 Multi-head Outlook Attention 构成的Stage。由于Stem Block将输入数据下采样了8倍，我们的目标是下采样16倍，作者选择在 Stage 1 结束后再做一次下采样，这里下采样通过<code>conv stride=2, ks=2</code>来实现。</p>
<p>最后是 Transformer 层构成的 Stages。</p>
<p>论文中对比了 Outlook Attention 层数与 Transformer 层数之间的比例，结论是 1 : 3 的时候效果比较好。</p>
<h2 id="模型精度">模型精度</h2>
<p>论文中采用了 Token Labeling，是 LV-ViT 论文中提出来的。基于 VOLO-D1，在不使用 Token Labeling 计算 Loss，同时引入 Random Augmentation 等增广方式后，自己训练精读是 82.2，使用论文里面的配置，确实可以达到 84.19 的准确率，总体来说效果还是不错的。</p>
        
            <p>Adam算法的实现以及一个主要改进AdamW的原理与实现。</p>
<p>突然觉着NCHW尺寸的张量比 NHWC Layout 的张量更容易理解，因为后者来看，就是N个样本，每个样本 H * W 的空间维度，然后每个空间元素点是一个 C 维的特征向量。NCHW Layout 的话就需要从后向前理解了&hellip;</p>
<h2 id="adam">Adam</h2>
<p>对于 Adam 的实现，参考下图即可。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adam0.png" alt="图-1 Adam的实现">
    <figcaption>图-1 Adam的实现</figcaption>
    </center>
</figure></p>
<p>算法里最后面三行可以通过用下面两个式子代替用来提高计算性能。</p>
<p>$$   \alpha_t = \alpha \cdot \frac{\sqrt{1 - \beta_2^t}}{ (1 - \beta_1^t)} $$
$$\theta_t \leftarrow \theta_{t-1} - \frac{\alpha_t \cdot m_t}{\sqrt{v_t} + \hat{\epsilon}} $$</p>
<p>其中，$\hat{\epsilon} = \frac{\epsilon}{\sqrt{1 - \beta_2^t}}$。此外，$\alpha$学习率设置了每次更新步长$\Delta_t$的置信区间，具体解释可以参考论文。二阶矩的计算中与普通方差的计算差别在于没有减去均值期望，所以被称为uncentered variacen。而最后上面第一个式子说明了两个超参对更新步长的控制，最后一个式子也说明一阶矩、二阶矩对更新步长的控制，如平地中，除式接近于1，陡峭部分除式小于1（因为方差大），更新会更保守一些。</p>
<p>算法的实现里，另一个重要的步骤是对一阶、二阶矩偏置的矫正。即为什么要除以$\sqrt{1 - \beta_{1,2}^2}$这个式子，证明如下。</p>
<p>以一阶矩$m_t$为例进行推导，首先假设$m_0 = 0$，即初始为一个零矩阵。则$t$时刻一阶矩的计算展开为：</p>
<p>$$m_t = (1 - \beta_t) \sum_{i}^{t}\beta_1^{t-i}g_i$$</p>
<p>我们的目标就是矫正$\mathbb{E}[m_t]$与$\mathbb{E}[g_t]$之间的差距。</p>
<p>$$\begin{aligned}
\mathbb{E}[m_t] = (1 - \beta_1) \mathbb{E}\left[ \sum_i^t \beta_1^{t-i} \cdot g_i \right] \<br>
= &amp; \mathbb{E}[g_t] \cdot (1 - \beta_1) \sum_{i}^t\beta_1^{t-i} + \zeta \<br>
= &amp; \mathbb{E}[g_t]\cdot (1 - \beta_1) + \zeta
\end{aligned}$$</p>
<p>其中，等比数列求和公式为：$\sum_i^t \beta_1^{t-i} = \frac{1 \cdot (1 - \beta_1^t)}{1 - \beta_1}$；另外一个地方是第一个等式，即$\mathbb{E}[g_i] = \mathbb{E}[g_t] + \zeta_i$，这里主要考虑使用等式右边近似表示等式左边的数值然后加上一个误差项，如果$g_i, g_t$属于独立同分布则这个误差项接近于0；论文里有提到，如果$\mathbb{E}[g_i]$比较稳定的时候，这里的误差项接近于0，或者当$\beta_1$比较小的时候，那么对于很久以前的梯度$g_i$赋予很小的权重。</p>
<p>当不使用这些矫正项的时候，Adam退化为RMSProp + Momentum的优化算法，实验表明，随着$\beta_2 \rightarrow 1$ 时，训练越来越不稳定。下图左侧开始的地方给出了一个示意图，其中绿色为 ground truth，紫色为预测的曲线，可以看到紫色部分在开始的地方偏小。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adam1.png" alt="图-2 偏置项矫正的作用">
    <figcaption>图-2 偏置项矫正的作用</figcaption>
    </center>
</figure></p>
<p>缺点是，像 Adam 这些 adaptive gradient optimization methods 在图像分类等任务上泛化性能不够高。常见的 adaptive gradient methods 包括：AdaGrad，RMSProp，Adam，AMSGrad等。可能的原因包括陡峭的局部最优解的出现或其他自身存在的缺点。</p>
<h2 id="adamw">AdamW</h2>
<p><a href="https://arxiv.org/pdf/1711.05101.pdf">Decoupled Weight Decay Regularization</a>作者发现，通过解偶weight decay以及基于loss的反向传播两个过程，可以让学习率、weight decay 两个超参的选取解偶，并且极大提高 adam 优化器在分类任务上的泛化性能，与 SGD + Momentum 取得类似的效果。</p>
<p>既然泛化性能不够，那说明正则化强度不够，所以作者就想到了对 L2 / weight decay 在 Adam 中的使用进行了研究。论文研究了 L2 正则项与 weight decay 在 SGD / Adam 中作用的异同，包括下面几点。</p>
<ul>
<li>L2 正则化项与 weight decay 的实现是不同的</li>
<li>Adam中L2正则项作用不明显</li>
<li>SGD中 L2 与 weight decay 效果类似</li>
<li>关于Adam中 Weight decay的选取，一般来说如果训练需要的迭代次数(iteration)越多，这个数值应该越小</li>
<li>Adam配合全局的学习率调整可以进一步提高性能，比如 cosine annealing等</li>
</ul>
<p>考虑<a href="#l2%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8Eweight-decay%E7%9A%84%E5%8C%BA%E5%88%AB">L2与WD区别</a>，论文提出了 SGDW 优化算法，用于解耦weight decay实现中依赖于学习率来计算l2参数。主要改动在于：惩罚项从计算 Momentum 计算之前移动到之后了；在不考虑momentum的实现时，图-3中的算法那才时 weight decay 的真正实现，至于所说的解耦的好处，这也是 weight decay 本身自带的优势。(当红色起作用时，绿色不起作用；或者相反)</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/sgdw0.png" alt="图-3 SGDW的实现">
    <figcaption>图-3 SGDW的实现</figcaption>
    </center>
</figure></p>
<p>但是对于Adam的实现来说，带有weight decay的实现与 L2 Loss 计算梯度无法等价，这一点可以通过对 L2 loss 求导替换$\nabla f_t(\theta)$看出来，若需要两者等价，则L2的系数需要满足:</p>
<p>$$\lambda' = \frac{\lambda}{\alpha \mathbf{M}_t}$$</p>
<p>才行。在 SGD 中，L2 与 Weight Decay 的作用都是让权重接近于0，但是在 Adaptive gradient algorithms中，L2 却不会这样，下个式子给出了带有 L2 正则项的Loss函数求导然后更新权重的过程。</p>
<p>$$\theta_{t+1} \leftarrow \theta_t - \alpha \mathbf{M}_t \nabla f_t(\theta_t) - \alpha \mathbf{M}_t\lambda' \theta_t$$</p>
<p>也就是说，L2 惩罚项也会被用于计算梯度，然后这个计算的梯度被用于 Adam 更新算法中，惩罚项并不会直接作用于权重上，而是参与到自适应一阶、二阶矩$\mathbf{M}_t$的调整中。而在 weight decay 的实现中，没有这个问题，因为$\mathbf{M}_t$只依赖于梯度，而weight decay起作用的方式在于$(1 - \lambda)g_t$，具体公式见论文的Proposition2部分以及对应的附录部分。</p>
<p>为什么 L2 惩罚项这种使用方式不明显呢？首先明确一点是，weight decay / L2 都是为了获得更小的权重值，这样模型泛化会更好一些，为了实现这一点，就需要让权重较大的元素下降的更快一些，权重已经比较小的元素更新小一些。来看 L2 在 Adam 中的具体作用。</p>
<p><a href="https://towardsdatascience.com/why-adamw-matters-736223f31b5d">Why AdamW matters</a> 博客里最后一个公式，相当于对权重的exponential moving更新过程是：</p>
<p>$$（1 - \frac{\alpha ( 1 - \beta_1)w}{\sqrt{v_t} + \epsilon})x_{t-1}$$</p>
<p>所以当梯度较大的时候，$\sqrt{v_t}$也比较大，导致该权重的更新（exponential moving）比较小，且还不如那些梯度很小的权重变化大，所以L2作用被打折扣，这也是论文里提到的，拥有大梯度的权重x被惩罚的力度还不如其它权重。最后论文的prosition3 给出了weight decay 与 L2 作用等价时候的关系，但是实际不会在 adaptive gradient algorithms 中实现，而且按照公式关系就可以实现对大的权重进行惩罚，尤其是那些历史梯度都比较大的权重。</p>
<p>那么可不可以理论证明 Weight Decay 的效果确实优于 L2 呢？见<a href="#%E4%BB%8E-bayesian-filtering-%E7%9A%84%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85weight-decay">bayesian filtering</a></p>
<p>图-4给出了准确的 adam with weight decay 的实现。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adamw1.png" alt="图-4 AdamW的实现">
    <figcaption>图-4 AdamW的实现</figcaption>
    </center>
</figure></p>
<p>很明显的看出来，现在 weight decay 已经跟一阶、二阶矩没有关系了。论文中，作者实验证明，即使 Adam 会自适应的调整学习率，但是加上一个全局的乘积因子，如 Cosine Annealing 等，效果会更好。</p>
<h3 id="从-bayesian-filtering-的角度看待weight-decay">从 Bayesian Filtering 的角度看待Weight Decay</h3>
<p>论文引用<a href="https://openreview.net/forum?id=BygREjC9YQ">Aitchison 2018</a>论文的观点，将模型参数的优化过程看做是求解一系列权重参数最优分布的过程。并将训练过程看做是一系列根据训练数据求解模型权重最大似然的过程$P(\theta_t | y_{1..t})$，状态转移函数是一个训练数据无关的过程，即从$P(\theta_{t+1} | \theta_t)$；$\theta$被认为是state，$y_{1..t}$被认为是观测结果。这里还需要更详细的整理。</p>
<p>对于Bayesian Filtering的一个发展是大名鼎鼎的卡尔曼滤波，此外还有其它更复杂的非线性情况下的扩展等实现，这方面一个比较好的教材是：<a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf">State Estimation for Robotics</a>，上面提到的状态的预测、更新在这本书里都会有比较详细的支撑内容。</p>
<p>补充书上式3.3推导过程中第一个等式（下图）的来源，参考<a href="https://math.stackexchange.com/questions/408774/bayes-rule-with-multiple-conditions">Bayes rule with multiple conditions</a>。</p>
<p><figure>
    <center>
    <img src="/imgs/adam-adamw/adamw0.png" alt="图-5 Bayes推导公式">
    <figcaption>图-5 Bayes推导公式</figcaption>
    </center>
</figure></p>
<p>这可以基于chain rule 来思考：</p>
<p>$$P(a, z, b) = P(a, z | b)P(b) = P(a | z, b) P(z, b) = P(a | z, b)P(z|b)P(b)$$</p>
<p>所以就有了：</p>
<p>$$P(x | v, y) = \frac{P(y, x, v)}{P(v, y)} = \frac{P(y|x, v)P(x|v)P(v)}{P(y|v)P(v)}$$</p>
<p>多说一句，<a href="http://www.r-5.org/files/books/computers/algo-list/image-processing/vision/Richard_Hartley_Andrew_Zisserman-Multiple_View_Geometry_in_Computer_Vision-EN.pdf">Multiple View Gemometry in Computer Vision</a>与上面这本书是视觉SLAM领域两个非常重要的参考读物，一本讲解了从视觉图像上得到三维空间信息，另一本讲解了三维空间信息的使用、变换、误差优化等信息，完美相互配合。</p>
<h3 id="其它的一些改进">其它的一些改进</h3>
<p>论文里，作者用实验表明训练不同的迭代次数时，最优的 weight decay 也是不同的。作者提出了“Normalized Weight Decay”，计算如下：</p>
<p>$$\lambda = \lambda_{norm}\frac{b}{BT}$$</p>
<p>其中，$B$是 training point，T是total epochs数，b是batch size。training point 是单个 epoch 内迭代更新的次数（我的理解）。</p>
<p>其它的改进包括 AdamW with Warm Restart and Cosine Annealing等，一个例子如下。</p>
<p>$$\eta_t = 0.5 + 0.5\cos (\pi T_{cur} / T_i)$$</p>
<p>可以配合上面的 normalized weight decay 对$\lambda$进行调整。Warm Restart 的好处是可以保留前几次的信息，形成一个 momentum，具体可以参考论文。此外，附录B, C部分给出了一个例子，用来说明每次 Restart 时 T 增大两倍，以及 $\eta$ 的变化过程。</p>
<h2 id="l2正则化与weight-decay的区别">L2正则化与weight decay的区别</h2>
<p>注意，L2 正则化项与 Weight Decay 在 SGD 中的使用是两个不同的公式，虽然两者之间存在一个基于学习率的系数差别。结论就是，在 Adaptive Gradient methods 中 L2 的正则化能力弱于weight decay。</p>
<p>关于 weight decay，一般来说 CNN 模型会选取一个比较小的数值，如 10-4 / 10-5 这种量级，但是对于 transformer 模型来说，这个数值一般都会在10-2量级，直观的反映了模型容量对正则化强度的需求变化。</p>
<p>下面来看具体的区别。首先是定义，L2正则化项的定义是修改 Loss 函数，即：</p>
<p>$$f_t^{reg}(\mathbf{\theta}) = f_t(\theta) + \frac{\lambda'}{2}\parallel \theta \parallel^2$$</p>
<p>其中$f_t(\theta)$为 Loss 函数，$\theta$为模型的权重。而SGD中 weight decay 对应的实现是在参数更新的时候起作用，即：</p>
<p>$$\theta_{t+1} = (1 - \lambda) \theta_t + \alpha \nabla f_t(\theta_t) $$</p>
<p>可以看出来是在原来的权重基础上做了一个 expontional decay 的类似计算，$\alpha$为学习率。对 L2 正则项的Loss函数求导并更新权重的公式如下：</p>
<p>$$\theta_{t+1} = \theta - \alpha \nabla f_t(\theta_t) - \alpha \lambda' \theta$$</p>
<p>结合上面两个参数更新过程的式子，可以得出，当L2正则项的参数满足$\lambda' = \frac{\lambda}{\alpha}$条件时，这两个正则方式是等价的。另一方面，在 SGD 中，L2 与 weight decay 的参数（$\lambda'$与$\lambda$）是紧密关联的，前者是后者除以学习率得到。</p>
<h2 id="其它">其它</h2>
<ol>
<li>
<p>为什么weight decay正则化项不会包含 bias 项？</p>
<p>参考下面的<a href="https://stats.stackexchange.com/questions/153605/no-regularisation-term-for-bias-unit-in-neural-network">答案</a>。</p>
<blockquote>
<p>Overfitting usually requires the output of the model to be sensitive to small changes in the input data (i.e. to exactly interpolate the target values, you tend to need a lot of curvature in the fitted function). The bias parameters don&rsquo;t contribute to the curvature of the model, so there is usually little point in regularising them as well.</p>
</blockquote>
<p>事实上， bias项一般会用 mean = 1 的随机数进行初始化，而不是 mean = 0。</p>
</li>
<li>
<p>另有Adam的一个改进时：<a href="https://openreview.net/pdf?id=ryQu7f-RZ">AMSGrad</a>，用于改进Adam很多时候仅能收敛到局部最优点的问题。</p>
</li>
</ol>
        
            <p>Residual Connection以及后续发展。</p>
<p>主要是为了自己梳理一下，总不能最基础的残差网络也忘了吧。更多的信息可以参考：<a href="https://zhuanlan.zhihu.com/p/353185272">ResNet系列网络演绎过程</a></p>
<h2 id="基础">基础</h2>
<p>残差网络(ResNet)是2015年何凯明在<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">Deep Residual Learning for Image Recognition</a>提出的，旁路连接方便了梯度回传，可以帮助模型更好的训练。基础结构如下图1。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual0.png" alt="图1 - 残差块">
    <figcaption>图1 - 残差块</figcaption>
    </center>
</figure></p>
<p>我们知道，VGG / ResNet / Mobilenet 等论文里已经说明现在网络结构设计可以通过简单的 Block 堆叠来构建，并且Blocks可以分组为若干个 Stage，每个 Stage 包含若干层 Block。为了提高计算性能以及提高感受野等，不同 Stage 之间会下采样降低空间分辨率同时提高 channel 个数（神经元）来保证模型容量。对于每个 Stage 的第一层 Block 需要完成下采样、channel翻倍的任务，为了保证输入数据与这两步处理后的输出数据尺寸相同，需要修改旁路，不再是 Indentity，而需要通过卷积完成映射。论文里在每个Block的第一层卷积里使用<code>stride=2</code>来完成下采样。 有论文表明，使用 avg pooling 进行下采样会更好，避免丢失很多的信息。</p>
<p>另外，一般配合 BN 时，CNN 的 bias 作用不明显可去掉。对于 bias 的作用可简单参考：<a href="https://www.pico.net/kb/the-role-of-bias-in-neural-networks/">The role of bias in Neural Networks</a>，猜测是在 BN 之前用于修正 <code>W * x</code> 的偏置，<strong>防止方差过大导致训练困难</strong>，即学习一个参数来降低输出值的方差，类似于降噪。</p>
<p>上述过程的示例代码如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">torch</span> <span style="color:#000;font-weight:bold">import</span> nn

<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">ResBasicBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    expansion <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, in_c, out_c, stride, downsample<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, reduce_first<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, ks<span style="color:#000;font-weight:bold">=</span><span style="color:#099">3</span>, padding<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        first_planes <span style="color:#000;font-weight:bold">=</span> out_c <span style="color:#000;font-weight:bold">//</span> reduce_first

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, first_planes, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(first_planes)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(first_planes, out_c, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#998;font-style:italic">## for downsample &amp; double channel number</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">None</span>
        <span style="color:#000;font-weight:bold">if</span> downsample:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential(
                nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_c, ks, stride, padding, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>),        <span style="color:#998;font-style:italic"># 通常这里的 ks = 1, senet 里 ks = 3</span>
                nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
            )
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1(x)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2(feat)
        <span style="color:#998;font-style:italic">## add se block here</span>
        <span style="color:#998;font-style:italic"># feat = self.se(feat)</span>

        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample(x)
        feat <span style="color:#000;font-weight:bold">+=</span> x
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2(feat)

        <span style="color:#000;font-weight:bold">return</span> feat
</code></pre></td></tr></table>
</div>
</div><p>注意在Stem模块中，不使用残差模块，并且通过 <code>stride=2</code> 以及一个 <code>MaxPooling(stride=2)</code> 来将输入图片下采样4倍。</p>
<h2 id="bottleneck-结构">Bottleneck 结构</h2>
<p><a href="#%E5%9F%BA%E7%A1%80">第一小节</a>里提到的结构更多的是用于 resnet-18/34等浅层网络，为了构建深层网络（resnet-50/101-152）等，作者提出了 Bottleneck 模块。Bottleneck 模块包含三层卷机，分别是 <code>conv1x1, conv3x3, conv1x1</code>，并且第一个 <code>conv1x1</code>将输入数据的channel根据一个因子（通常是4）进行缩小，最后一个<code>conv1x1</code>在缩放回原来大小，这样既可以完成残差计算，也降低了中间<code>conv3x3</code>的计算。实验表明，这里即使不降低 channel 个数也不会影响性能，所以Bottleneck 完全为了实际中提高计算效率，至于 Mobilenetv2 里提到的 Inverted Residual Block，不会展开。网络结构示意图如图2右边部分。</p>
<p>实现的时候需要注意的是，每个 Stage 里Block内的Channel变化过程，最后一个<code>conv1x1</code>的是第一个<code>conv1x1</code><strong>输入</strong>的expansion倍。下图展示的其实是Stage内非第一个Block的结构，相较于输入，第一个<code>conv1x1</code>将channel数降低了4倍；而第一个Block的输入channel数时上一Stage输出的channel数，配合channel double的过程，第一个<code>conv1x1</code>只是将channel下降了2；此外，下采样部分是在 <code>conv3x3, stride=2</code> 部分完成的，如果放在第一个<code>conv1x1</code>里，会导致3/4的信息丢失。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual1.png" alt="图-2 Bottleneck 结构">
    <figcaption>图-2 Bottleneck 结构</figcaption>
    </center>
</figure></p>
<p>具体实现代码如下。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">40
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">41
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">42
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">43
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">44
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">45
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">46
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#000;font-weight:bold">from</span> <span style="color:#555">torch</span> <span style="color:#000;font-weight:bold">import</span> nn
<span style="color:#000;font-weight:bold">class</span> <span style="color:#458;font-weight:bold">BottleneckBlock</span>(nn<span style="color:#000;font-weight:bold">.</span>Module):
    <span style="color:#000;font-weight:bold">def</span> __init__(<span style="color:#999">self</span>, in_c<span style="color:#000;font-weight:bold">=</span><span style="color:#099">256</span>, out_c<span style="color:#000;font-weight:bold">=</span><span style="color:#099">64</span>, expansion<span style="color:#000;font-weight:bold">=</span><span style="color:#099">4</span>, stride<span style="color:#000;font-weight:bold">=</span><span style="color:#099">1</span>, downsample<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>, <span style="color:#000;font-weight:bold">**</span>kwargs):
        <span style="color:#0086b3">super</span>()<span style="color:#000;font-weight:bold">.</span>__init__(<span style="color:#000;font-weight:bold">**</span>kwargs)
        out_planes <span style="color:#000;font-weight:bold">=</span> out_c <span style="color:#000;font-weight:bold">*</span> expansion

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_c, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(out_c, out_c, <span style="color:#099">3</span>, stride, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)      <span style="color:#998;font-style:italic"># stride = 2 时进行下采样</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Conv2d(out_c, out_planes, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>)         <span style="color:#998;font-style:italic"># 注意输出 channel 的个数</span>
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_c)
        <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act3 <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>ReLU()

        <span style="color:#000;font-weight:bold">if</span> downsample:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> nn<span style="color:#000;font-weight:bold">.</span>Sequential(
                nn<span style="color:#000;font-weight:bold">.</span>Conv2d(in_c, out_planes, <span style="color:#099">1</span>, <span style="color:#099">1</span>, bias<span style="color:#000;font-weight:bold">=</span><span style="color:#999">False</span>),
                nn<span style="color:#000;font-weight:bold">.</span>BatchNorm2d(out_planes)
            )
        <span style="color:#000;font-weight:bold">else</span>:
            <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">False</span>
    
    <span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">forward</span>(<span style="color:#999">self</span>, x):
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv1(x)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn1(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act1(feat)

        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn2(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act2(feat)
        <span style="color:#998;font-style:italic">## use avg pooling to downsample here</span>

        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>conv3(feat)
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>bn3(feat)
        <span style="color:#998;font-style:italic">## add se here</span>
        <span style="color:#998;font-style:italic"># feat = self.se(feat)</span>
        <span style="color:#998;font-style:italic">## drop path here, i.e. random drop some samples along batch axis</span>
        <span style="color:#998;font-style:italic">## downsample projection path here</span>
        <span style="color:#000;font-weight:bold">if</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample <span style="color:#000;font-weight:bold">is</span> <span style="color:#000;font-weight:bold">not</span> <span style="color:#999">None</span>:
            x <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>downsample(x)
        feat <span style="color:#000;font-weight:bold">+=</span> x
        feat <span style="color:#000;font-weight:bold">=</span> <span style="color:#999">self</span><span style="color:#000;font-weight:bold">.</span>act3(feat)
        <span style="color:#000;font-weight:bold">return</span> feat
</code></pre></td></tr></table>
</div>
</div><p>总结一下SE的位置，SE Block 均是在每一个 Block 最后一层卷积 BN 之后的特征上进行。</p>
<h2 id="resnet-v2">ResNet v2</h2>
<p>论文地址：<a href="https://arxiv.org/pdf/1603.05027.pdf">Identity Mappings in Deep Residual Networks</a></p>
<p>论文里其实是对 BN / ReLU 两个函数的位置进行了挪动。作者测试了下面几种排列组合，发现最后一种实现效果最好。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual2.png" alt="图-3 ResNet v2改进">
    <figcaption>图-3 ResNet v2改进</figcaption>
    </center>
</figure></p>
<p>分析一下，(b)里 BN 在 Identity (左侧)分支里，会改变Identity分支的分布，影响信息传递，在训练开始的时候会阻碍Loss的下降。这一点可以通过论文里的梯度反向传播推导过程看出来。</p>
<p>(c)里residual(右侧)分支是 ReLU 的输出，导致这个分支对结果只有正向影响，毕竟非负，但我们希望有两个方向的影响，所以非最优。关于(d, e)，实验表明都不如(f)，毕竟 BN 在Residual分支上可以对输入就起到正则化的作用。</p>
<h2 id="resnext">ResNeXt</h2>
<p>网络结构如图4。</p>
<p><figure>
    <center>
    <img src="/imgs/resnet-series/residual3.png" alt="图-4 ResNeXt网络结构">
    <figcaption>图-4 ResNeXt网络结构</figcaption>
    </center>
</figure></p>
<p>(a)为最开始的思想，(c)为等价形式。也就是说，中间的<code>conv3x3</code>替换为分组卷积计算。
主要改动就是将普通残差结构中的 Residual 分支用 Inception 思想进行修改，用多路并行卷积代替原来的一支卷积，与Inception论文不同的是，这里每个分支采用相同的参数配置，如kernel size等。</p>
<h2 id="其它">其它</h2>
<ul>
<li>
<p>ResNeSt</p>
<p>与<a href="https://arxiv.org/abs/1903.06586">SKNet</a>类似。</p>
</li>
<li>
<p>Res2Net</p>
<p>在单个残差块内引入Inception思想，感受野逐步增大，最后concatenate 起来送入 <code>conv1x1</code> 计算。</p>
</li>
<li>
<p>SKNet</p>
</li>
</ul>
        
            <p>二叉搜索树相关笔记</p>
<h2 id="定义">定义</h2>
<p>每个节点有两个子节点：左节点、右节点，即是一个二叉树；同时有顺序关系：左子树小于父节点，右子树大于父节点。</p>
<p><figure>
    <center>
    <img src="/imgs/binary-search-tree/BSTSearch01.png" alt="binary-search-image structure example">
    <figcaption>binary-search-image structure example</figcaption>
    </center>
</figure></p>
<p>查找复杂度：$\sim 2\ln(N)$，其中 N 是节点个数。查找未命中也是这个复杂度。</p>
<p>作为一种数据结构，主要功能就是：增删查改。floor() 也可以认为是一种“查”操作。</p>
<p>目前来看，相关应用可以分为两大类：遍历，排序。遍历就是包括前序、中序、后序进行遍历然后处理；排序一般就是按照中序进行处理。</p>
<h2 id="floor-函数">floor() 函数</h2>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">Key <span style="color:#900;font-weight:bold">floor</span>(Key key)
{
    Node x <span style="color:#000;font-weight:bold">=</span> floor(root, key);
    <span style="color:#000;font-weight:bold">if</span> (x <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    <span style="color:#000;font-weight:bold">else</span>
        <span style="color:#000;font-weight:bold">return</span> x;
}

Node <span style="color:#900;font-weight:bold">floor</span>(Node root, Key key)
{
    <span style="color:#000;font-weight:bold">if</span> (root <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    
    <span style="color:#000;font-weight:bold">if</span> (root<span style="color:#000;font-weight:bold">-&gt;</span>key <span style="color:#000;font-weight:bold">==</span> key)
        <span style="color:#000;font-weight:bold">return</span> root;
    <span style="color:#000;font-weight:bold">if</span> (root<span style="color:#000;font-weight:bold">-&gt;</span>key <span style="color:#000;font-weight:bold">&gt;</span> key)    <span style="color:#998;font-style:italic">// left branch
</span><span style="color:#998;font-style:italic"></span>        <span style="color:#000;font-weight:bold">return</span> floor(root<span style="color:#000;font-weight:bold">-&gt;</span>left, key);
    <span style="color:#998;font-style:italic">// right branch: key &gt; root-&gt;key
</span><span style="color:#998;font-style:italic"></span>    Node t <span style="color:#000;font-weight:bold">=</span> floor(root<span style="color:#000;font-weight:bold">-&gt;</span>right, key);
    <span style="color:#000;font-weight:bold">if</span> (t <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
        <span style="color:#000;font-weight:bold">return</span> root;
    <span style="color:#000;font-weight:bold">else</span>
        <span style="color:#000;font-weight:bold">return</span> t;
}
</code></pre></td></tr></table>
</div>
</div><p>为什么这里相当于是一个前序排序，其实本质上需要的是 <code>root-&gt;left</code> 分支的递归要在 <code>root-&gt;right</code> 的前面，这样才能保证走右子树的时候，可以最先判断这个子树上最小的数值。</p>
<p>这个函数关键是分清楚什么时候向左走，什么时候向右走，然后才能明白应该返回什么数值。向左走的时候，说明当前的 <code>root-&gt;key</code> 大于目标值，向右走的时候，说明当前的 <code>root-&gt;key</code> 小于目标值，而小于目标值则说明结果必定存在，因为大不了返回当前的 root 即可。因此，在递归过程向上走的时候，如果当前属于右子路的下一层(代码第19行)并且返回的是 nullptr，那么就返回当前的节点就行了(第20行)，否则返回递归返回的结果即可。总而言之，如果走上右分支，就必定有解；走上左分支，则原样返回即可。</p>
<p><strong>递归过程可以看成两个步骤：首先是沿着左子树或者右子树向下走，然后就是沿着树向上爬。</strong> 向上爬的时候，需要注意返回结果，对树的状态进行更新，比如链接、具体成员变量等。另一方面，递归过程其实就是一个先进后出的过程，所以对应的非递归实现可以借助Stack来实现。向上走的时候基于向下走的时候的判断来保证正确。</p>
<h2 id="delete函数">Delete()函数</h2>
<p>首先是删除最小、最大值节点。</p>
<p>主要在于递归返回结果，<strong>只需要将返回的链接赋给作为参数的链接</strong>。</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp"><span style="color:#998;font-style:italic">// ...
</span><span style="color:#998;font-style:italic"></span>x.left <span style="color:#000;font-weight:bold">=</span> deleteMin(x.left);     <span style="color:#998;font-style:italic">// delete minimum
</span><span style="color:#998;font-style:italic">// ...
</span></code></pre></td></tr></table>
</div>
</div><p>其次是删除中间某个节点，与删除最小、最大节点的区别在于，这个节点包含两个字节点，而且还需要保持二叉搜索树之间的顺序。这个问题常用的解法是：<code>Hibbard算法</code>。该算法表示在删除节点 x 之后，用它的后继节点填充它的位置，并且这个后继节点就是右子树中的最小节点。</p>
<p>对应的4个步骤如下：</p>
<ol>
<li>找到需要被删除的节点，用 t 表示</li>
<li>然后找到被删除节点右（左，随机选择）分支的最小（大）值，表示为 x，即 <code>x = min(t.right)</code></li>
<li>然后更新 x 节点的左右分支，<code>x.right</code> 更新为 <code>deleteMin(t.right)</code>，保证右子树都大于 <code>x</code></li>
<li><code>x.left</code> 更新为 <code>t.left</code>，最后返回 x 作为原来 t 的父节点的子节点，即代替 t</li>
</ol>
<p>实现示例如下：</p>
<div class="highlight"><div style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">
<table style="border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 0
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-cpp" data-lang="cpp">Node <span style="color:#900;font-weight:bold">delete</span>(Key key)
{
    <span style="color:#000;font-weight:bold">delete</span>(root, key);
}

Node <span style="color:#900;font-weight:bold">delete</span>(Node t, key key)
{
    <span style="color:#000;font-weight:bold">if</span> (t <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)       <span style="color:#998;font-style:italic">// not exists
</span><span style="color:#998;font-style:italic"></span>        <span style="color:#000;font-weight:bold">return</span> <span style="color:#000;font-weight:bold">nullptr</span>;
    <span style="color:#000;font-weight:bold">if</span> (key <span style="color:#000;font-weight:bold">&lt;</span> t<span style="color:#000;font-weight:bold">-&gt;</span>key)       <span style="color:#998;font-style:italic">// go left
</span><span style="color:#998;font-style:italic"></span>    {
        t<span style="color:#000;font-weight:bold">-&gt;</span>left <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">delete</span>(t<span style="color:#000;font-weight:bold">-&gt;</span>left, key);
        <span style="color:#000;font-weight:bold">return</span> t;
    }
    elif (key <span style="color:#000;font-weight:bold">&gt;</span> t<span style="color:#000;font-weight:bold">-&gt;</span>key)
    {
        t<span style="color:#000;font-weight:bold">-&gt;</span>right <span style="color:#000;font-weight:bold">=</span> <span style="color:#000;font-weight:bold">delete</span>(t<span style="color:#000;font-weight:bold">-&gt;</span>right, key);
        <span style="color:#000;font-weight:bold">return</span> t;
    }
    <span style="color:#000;font-weight:bold">else</span>                    <span style="color:#998;font-style:italic">// equal
</span><span style="color:#998;font-style:italic"></span>    {
        <span style="color:#000;font-weight:bold">if</span> (t.right <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
            <span style="color:#000;font-weight:bold">return</span> t.left;
        <span style="color:#000;font-weight:bold">if</span> (t.left <span style="color:#000;font-weight:bold">==</span> <span style="color:#000;font-weight:bold">nullptr</span>)
            <span style="color:#000;font-weight:bold">return</span> t.right;
        Node x <span style="color:#000;font-weight:bold">=</span> min(t.right);
        x.right <span style="color:#000;font-weight:bold">=</span> deleteMin(t.right);
        x.left <span style="color:#000;font-weight:bold">=</span> t.left;
        <span style="color:#000;font-weight:bold">delete</span> t;
        <span style="color:#000;font-weight:bold">return</span> x;
    }
    <span style="color:#998;font-style:italic">// other update
</span><span style="color:#998;font-style:italic"></span>}
</code></pre></td></tr></table>
</div>
</div><h2 id="其他支持的函数">其他支持的函数</h2>
<ul>
<li>
<p>Select()</p>
<p>即返回第k小的节点。</p>
<p>可以通过中序进行遍历，然后每次递归返回的时候，通过引用更新计数，当计数为0的时侯返回该节点即可。</p>
</li>
<li>
<p>Rank()</p>
<p>即给定一个key，返回该key在树中的位置。</p>
<p>如果给定 key 小于当前节点，则正确的位置必定在右分支中，则最终的排序就是左分支的节点个数 + 1（当前节点） + 在右子树中的排序，而在右子树中的排序直接递归即可。</p>
</li>
<li>
<p>范围查找</p>
<p>典型的中序遍历思路，即判断当前节点是否在制定的范围内，如果在范围内，则保存当前节点到一个队列即可。</p>
</li>
</ul>
        
            <p>经典自监督模型，包括MoCo / SimCLR / SwAV / BYOL / SimSiam 等。</p>
<p>主要关注无监督策略的研究，模型结构不是本文重点，所以主要包括 MoCo 系列、SimCLR 系列、SwAV、BYOL 等几篇论文。无监督训练模型的一点在于要避免模型坍塌，通过 Contrastive Loss, Clustering Constraints, Predictor(Stop Gradient), Batch Normalization等。</p>
<p>整体来说，图像自监督学习方法按照自监督实现思想可以分为下面几类。</p>
<ul>
<li>基于contrastive loss</li>
<li>基于蒸馏的方式，一般设计 momentum</li>
<li>基于聚类的方法</li>
</ul>
<p>关于预训练任务也存在多种选择。</p>
<ul>
<li>预测图像选装方向。图像经过 0/90/180/270 等几个角度的随机旋转，然后训练模型进行4分类</li>
<li>预测图片不定位置相对关系。图像被分割成 3 * 3 的表格，然后选取中心小图与另外8个子图中的随机一个进行位置分类，分类类别为8（两个子图的输出拼接起来送入分类层），一些技巧是图像分割成子图时可以增加缝隙或者抖动等</li>
<li>补丁拼图。将图片分割成 3 * 3 的子图，然后随机打乱，将子图的所有输出特征拼接起来送入分类层，正常来说，类别说应该是 9!，但是作者对这些排列类别做了合并，因为很多排列比较相似，合并过程基于汉明距离进行</li>
<li>图片上色。灰度图片输入 Encoder，然后Decoder输出彩色图片，可以使用 L2 Loss，或者 LAB 颜色空间等</li>
<li>自编码器系列。</li>
<li>GAN系列。</li>
<li>对比学习。需要构造丰富的负样本，比如大的 Batch Size 或者借助 Memory Bank等</li>
</ul>
<h2 id="moco-系列">MoCo 系列</h2>
<h2 id="simclr-系列">SimCLR 系列</h2>
<h2 id="swav">SwAV</h2>
<h2 id="byol">BYOL</h2>
<p>分析了怎么防止模型坍塌（也就是所有的输入的模型输出都是相同的），关键是要让模型的输出部分层学习到新的知识。在这里，一方面是借助 Mean Teacher，一方面是在 Student Network 上面增加了一层 Predictor，这两个因素可以让 Prediction 层不断学习新的知识，从而避免模型坍塌。BYOL包含两个模型，一个称为 Online，一个称为 Target。</p>
<p>按理来说，没有负样本，那么优化损失函数的梯度$\nabla_{\theta}(\mathcal{L}_{\theta, \epsilon}^{\mathrm{BYOL}})$应该很快导致模型坍塌啊，也就是损失降为0，但实际没有发生，作者认为这是因为这个损失的梯度下降方向与 Target 模型参数的变化方向是不一致的，也就是梯度下降方向 与 Target 模型让 Online 模型参数更新的放向不一样，所以避免了模型坍塌。另一方面，这也就意味着不存在一个Loss可以同时优化 Target / Online 模型的权重，类似于 GAN 模型的G / D的参数无法同时优化一样。作者也用消融实验表明，保持 prediction 足够好貌似是防止坍塌的关键。</p>
<p>为啥 SimCLR 依赖于 color jitter 这个变换，因为如果去掉这个变换的话，两次 crop 的图像的颜色直方图分布其实是非常接近的，导致模型非常容易学习。</p>
<p>发现去掉 Weight Decay 后，模型发散，说明 WD 对自监督模型的重要性，但是增加模型初始化时的初始值范围对模型性能影响不大。</p>
<h2 id="simsiam">SimSiam</h2>
<p>SimSiam 的 Prediction Head 需要固定 Learning Rate，也就是不随 Scheduler 变化。</p>
<h2 id="消融实验">消融实验</h2>
<h2 id="一些-tricks">一些 Tricks</h2>
<ul>
<li>
<p>Rethinking Image Mixture for Unsupervised Visual Representation Learning</p>
<p>在无监督训练中引入了Image Mixture &amp; Label Smooth。</p>
</li>
<li>
<p>Whitening for Self-Supervised Representation Learning</p>
</li>
<li>
<p>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</p>
</li>
<li>
<p>Contrastive Multiview Coding</p>
</li>
</ul>
        
            <p>一些以提高 Transformer 计算性能为目的的 Xformer 方案整理。</p>
<p>主要包含以下几篇论文：</p>
<ul>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a></li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer</a></li>
<li><a href="https://arxiv.org/abs/2006.04768">Linformer</a></li>
<li><a href="https://arxiv.org/abs/2009.14794">Performers</a></li>
<li><a href="https://arxiv.org/abs/2004.05150">Longformer</a></li>
</ul>
<h2 id="reformer">Reformer</h2>
<h2 id="linformer">Linformer</h2>
<h2 id="performers">Performers</h2>
<h2 id="longformer">Longformer</h2>
<h2 id="a-survey">A Survey</h2>
        
	
		<span>2</span>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2022-01-16 21:38:57.286895 &#43;0800 CST m=&#43;0.116228475">2022</time> triloon. Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
